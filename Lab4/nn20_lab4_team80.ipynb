{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 4o Lab: Παίζοντας Atari με βαθιά ενισχυτική μάθηση\n"},{"metadata":{},"cell_type":"markdown","source":"# Αρχικοποιήσεις"},{"metadata":{},"cell_type":"markdown","source":"## Tensorboard και ngrok\n\nΑντικαταστήστε το authentication token σας που παίρνετε από το [ngrok](https://ngrok.com/) στη γραμμή που υποδεικνύεται. Όταν εκτελεστεί το κελί θα σας δώσει το URL όπου μπορείτε να βλέπετε το TensorBoard. Σημειώστε ότι σε περίπτωση επανεκκίνησης του πυρήνα θα πρέπει να ξανατρέξετε το κελί. Η διεύθυνση θα είναι διαφορετική, αλλά τα προηγούμενα στατιστικά σας δεν χάνονται (μέχρι να ανακυκλωθεί ο πυρήνας)."},{"metadata":{"execution":{"iopub.execute_input":"2021-02-01T12:38:17.874329Z","iopub.status.busy":"2021-02-01T12:38:17.862336Z","iopub.status.idle":"2021-02-01T12:38:25.172607Z","shell.execute_reply":"2021-02-01T12:38:25.171689Z"},"papermill":{"duration":7.332493,"end_time":"2021-02-01T12:38:25.172898","exception":false,"start_time":"2021-02-01T12:38:17.840405","status":"completed"},"tags":[],"trusted":true,"scrolled":false},"cell_type":"code","source":"!apt install apt-utils psmisc -y\n!killall -9 ngrok\n!rm -rf *ngrok*\n!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n!unzip ngrok-stable-linux-amd64.zip\n!mkdir -p ngrok-dir\n!mv ngrok ngrok-dir/\n# Στην επόμενη γραμμή βάλτε το δικό σας authentication token από το ngrok\n!ngrok-dir/ngrok authtoken 1pcN37Yi5sx8yMipxMQcFDBRURT_2zt9dVjiaw8KXisyRNyE9\n\nLOG_DIR = 'tb_log/'\n!mkdir -p LOG_DIR\n\nget_ipython().system_raw(\n    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n    .format(LOG_DIR)\n)\nget_ipython().system_raw('ngrok-dir/ngrok http 6006 &')\n! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\nprint(\"Wait 10-15 seconds and then click the URL above to open TensorBoard\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Εγκατάσταση βιβλιοθήκης και gym"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip\n!pip install --upgrade stable_baselines3[extra]\n# we need a specific version of gym because of this issue: https://github.com/DLR-RM/stable-baselines3/issues/294\n!pip install gym==0.17.3","execution_count":2,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (21.0)\nCollecting pip\n  Downloading pip-21.0.1-py3-none-any.whl (1.5 MB)\n\u001b[K     |████████████████████████████████| 1.5 MB 2.8 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 21.0\n    Uninstalling pip-21.0:\n      Successfully uninstalled pip-21.0\nSuccessfully installed pip-21.0.1\nCollecting stable_baselines3[extra]\n  Downloading stable_baselines3-0.11.1-py3-none-any.whl (152 kB)\n\u001b[K     |████████████████████████████████| 152 kB 2.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from stable_baselines3[extra]) (1.19.5)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from stable_baselines3[extra]) (1.7.0)\nRequirement already satisfied: gym>=0.17 in /opt/conda/lib/python3.7/site-packages (from stable_baselines3[extra]) (0.18.0)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from stable_baselines3[extra]) (1.6.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from stable_baselines3[extra]) (3.3.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from stable_baselines3[extra]) (1.1.5)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from stable_baselines3[extra]) (7.2.0)\nRequirement already satisfied: tensorboard>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from stable_baselines3[extra]) (2.4.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from stable_baselines3[extra]) (5.8.0)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from stable_baselines3[extra]) (4.5.1.48)\nCollecting atari-py~=0.2.0\n  Downloading atari_py-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (2.8 MB)\n\u001b[K     |████████████████████████████████| 2.8 MB 15.1 MB/s eta 0:00:01     |████████                        | 696 kB 15.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from atari-py~=0.2.0->stable_baselines3[extra]) (1.15.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym>=0.17->stable_baselines3[extra]) (1.4.1)\nRequirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym>=0.17->stable_baselines3[extra]) (1.5.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable_baselines3[extra]) (0.18.2)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->stable_baselines3[extra]) (0.4.2)\nRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->stable_baselines3[extra]) (3.14.0)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->stable_baselines3[extra]) (0.36.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->stable_baselines3[extra]) (3.3.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->stable_baselines3[extra]) (49.6.0.post20201009)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->stable_baselines3[extra]) (1.0.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->stable_baselines3[extra]) (2.25.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->stable_baselines3[extra]) (1.24.0)\nRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->stable_baselines3[extra]) (1.32.0)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->stable_baselines3[extra]) (0.10.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->stable_baselines3[extra]) (1.8.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->stable_baselines3[extra]) (4.6)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->stable_baselines3[extra]) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->stable_baselines3[extra]) (4.1.1)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable_baselines3[extra]) (1.3.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->stable_baselines3[extra]) (3.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->stable_baselines3[extra]) (0.4.8)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable_baselines3[extra]) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable_baselines3[extra]) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable_baselines3[extra]) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable_baselines3[extra]) (1.26.2)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable_baselines3[extra]) (3.0.1)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->stable_baselines3[extra]) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->stable_baselines3[extra]) (0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->stable_baselines3[extra]) (3.4.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable_baselines3[extra]) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable_baselines3[extra]) (0.10.0)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable_baselines3[extra]) (2.8.1)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable_baselines3[extra]) (2.4.7)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->stable_baselines3[extra]) (2019.3)\nInstalling collected packages: stable-baselines3, atari-py\nSuccessfully installed atari-py-0.2.6 stable-baselines3-0.11.1\nCollecting gym==0.17.3\n  Downloading gym-0.17.3.tar.gz (1.6 MB)\n\u001b[K     |████████████████████████████████| 1.6 MB 2.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym==0.17.3) (1.4.1)\nRequirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.7/site-packages (from gym==0.17.3) (1.19.5)\nRequirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym==0.17.3) (1.5.0)\nRequirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym==0.17.3) (1.6.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.18.2)\nBuilding wheels for collected packages: gym\n","name":"stdout"},{"output_type":"stream","text":"  Building wheel for gym (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654653 sha256=e0a2d205abd1d0485499b6be574a6f5a7630d9879c9d74f28ad5a421d669a8aa\n  Stored in directory: /root/.cache/pip/wheels/d1/81/4b/dd9c029691022cb957398d1f015e66b75e37637dda61abdf58\nSuccessfully built gym\nInstalling collected packages: gym\n  Attempting uninstall: gym\n    Found existing installation: gym 0.18.0\n    Uninstalling gym-0.18.0:\n      Successfully uninstalled gym-0.18.0\nSuccessfully installed gym-0.17.3\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install sb3-contrib","execution_count":3,"outputs":[{"output_type":"stream","text":"Collecting sb3-contrib\n  Downloading sb3_contrib-0.11.1-py3-none-any.whl (21 kB)\nRequirement already satisfied: stable-baselines3[docs,tests]>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from sb3-contrib) (0.11.1)\nRequirement already satisfied: gym>=0.17 in /opt/conda/lib/python3.7/site-packages (from stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (0.17.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (3.3.3)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (1.6.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (1.1.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (1.19.5)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (1.7.0)\nCollecting sphinx\n  Downloading Sphinx-3.5.2-py3-none-any.whl (2.8 MB)\n\u001b[K     |████████████████████████████████| 2.8 MB 2.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: sphinx-rtd-theme in /opt/conda/lib/python3.7/site-packages (from stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (0.2.4)\nCollecting sphinx-autobuild\n  Downloading sphinx_autobuild-2021.3.14-py3-none-any.whl (9.9 kB)\nCollecting sphinx-autodoc-typehints\n  Downloading sphinx_autodoc_typehints-1.11.1-py3-none-any.whl (8.7 kB)\nCollecting sphinxcontrib.spelling\n  Downloading sphinxcontrib_spelling-7.1.0-py3-none-any.whl (14 kB)\nRequirement already satisfied: pytest in /opt/conda/lib/python3.7/site-packages (from stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (6.2.2)\nCollecting pytest-env\n  Downloading pytest-env-0.6.2.tar.gz (1.7 kB)\nCollecting isort>=5.0\n  Downloading isort-5.7.0-py3-none-any.whl (104 kB)\n\u001b[K     |████████████████████████████████| 104 kB 12.7 MB/s eta 0:00:01\n\u001b[?25hCollecting pytest-xdist\n  Downloading pytest_xdist-2.2.1-py3-none-any.whl (37 kB)\nRequirement already satisfied: flake8>=3.8 in /opt/conda/lib/python3.7/site-packages (from stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (3.8.4)\nRequirement already satisfied: black in /opt/conda/lib/python3.7/site-packages (from stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (20.8b1)\nCollecting pytype\n  Downloading pytype-2021.3.10-cp37-cp37m-manylinux2014_x86_64.whl (1.9 MB)\n\u001b[K     |████████████████████████████████| 1.9 MB 10.4 MB/s eta 0:00:01\n\u001b[?25hCollecting pytest-cov\n  Downloading pytest_cov-2.11.1-py2.py3-none-any.whl (20 kB)\nRequirement already satisfied: pyflakes<2.3.0,>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from flake8>=3.8->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (2.2.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from flake8>=3.8->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (3.3.0)\nRequirement already satisfied: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/lib/python3.7/site-packages (from flake8>=3.8->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (2.6.0)\nRequirement already satisfied: mccabe<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from flake8>=3.8->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (0.6.1)\nRequirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym>=0.17->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (1.5.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym>=0.17->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (1.4.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (0.6)\nRequirement already satisfied: regex>=2020.1.8 in /opt/conda/lib/python3.7/site-packages (from black->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (2020.11.13)\nRequirement already satisfied: pathspec<1,>=0.6 in /opt/conda/lib/python3.7/site-packages (from black->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (0.8.1)\nRequirement already satisfied: toml>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from black->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (0.10.2)\nRequirement already satisfied: mypy-extensions>=0.4.3 in /opt/conda/lib/python3.7/site-packages (from black->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (0.4.3)\nRequirement already satisfied: click>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from black->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (7.1.2)\nRequirement already satisfied: typed-ast>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from black->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (1.4.2)\nRequirement already satisfied: appdirs in /opt/conda/lib/python3.7/site-packages (from black->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (1.4.4)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->flake8>=3.8->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (3.4.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (1.3.1)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (7.2.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (0.10.0)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (2.8.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (1.15.0)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (2019.3)\nRequirement already satisfied: iniconfig in /opt/conda/lib/python3.7/site-packages (from pytest->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (1.1.1)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (20.3.0)\nRequirement already satisfied: pluggy<1.0.0a1,>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (0.13.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from pytest->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (20.8)\nRequirement already satisfied: py>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from pytest->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (1.10.0)\nCollecting coverage>=5.2.1\n  Downloading coverage-5.5-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n\u001b[K     |████████████████████████████████| 242 kB 17.1 MB/s eta 0:00:01\n\u001b[?25hCollecting execnet>=1.1\n  Downloading execnet-1.8.0-py2.py3-none-any.whl (39 kB)\nCollecting pytest-forked\n  Downloading pytest_forked-1.3.0-py2.py3-none-any.whl (4.7 kB)\nCollecting apipkg>=1.4\n  Downloading apipkg-1.5-py2.py3-none-any.whl (4.9 kB)\nCollecting importlab>=0.6.1\n  Downloading importlab-0.6.1.tar.gz (26 kB)\nRequirement already satisfied: pyyaml>=3.11 in /opt/conda/lib/python3.7/site-packages (from pytype->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (5.3.1)\n","name":"stdout"},{"output_type":"stream","text":"Collecting ninja>=1.10.0.post2\n  Downloading ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107 kB)\n\u001b[K     |████████████████████████████████| 107 kB 19.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: networkx>=2 in /opt/conda/lib/python3.7/site-packages (from importlab>=0.6.1->pytype->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (2.5)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2->importlab>=0.6.1->pytype->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (4.4.2)\nCollecting snowballstemmer>=1.1\n  Downloading snowballstemmer-2.1.0-py2.py3-none-any.whl (93 kB)\n\u001b[K     |████████████████████████████████| 93 kB 1.6 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from sphinx->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (2.25.1)\nCollecting alabaster<0.8,>=0.7\n  Downloading alabaster-0.7.12-py2.py3-none-any.whl (14 kB)\nCollecting sphinxcontrib-qthelp\n  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n\u001b[K     |████████████████████████████████| 90 kB 5.6 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: Pygments>=2.0 in /opt/conda/lib/python3.7/site-packages (from sphinx->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (2.7.3)\nRequirement already satisfied: docutils>=0.12 in /opt/conda/lib/python3.7/site-packages (from sphinx->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (0.16)\nRequirement already satisfied: babel>=1.3 in /opt/conda/lib/python3.7/site-packages (from sphinx->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (2.9.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from sphinx->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (49.6.0.post20201009)\nCollecting sphinxcontrib-htmlhelp\n  Downloading sphinxcontrib_htmlhelp-1.0.3-py2.py3-none-any.whl (96 kB)\n\u001b[K     |████████████████████████████████| 96 kB 3.5 MB/s  eta 0:00:01\n\u001b[?25hCollecting sphinxcontrib-devhelp\n  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n\u001b[K     |████████████████████████████████| 84 kB 2.6 MB/s  eta 0:00:01\n\u001b[?25hCollecting sphinxcontrib-applehelp\n  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n\u001b[K     |████████████████████████████████| 121 kB 5.6 MB/s eta 0:00:01\n\u001b[?25hCollecting imagesize\n  Downloading imagesize-1.2.0-py2.py3-none-any.whl (4.8 kB)\nCollecting sphinxcontrib-jsmath\n  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\nCollecting sphinxcontrib-serializinghtml\n  Downloading sphinxcontrib_serializinghtml-1.1.4-py2.py3-none-any.whl (89 kB)\n\u001b[K     |████████████████████████████████| 89 kB 559 kB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: Jinja2>=2.3 in /opt/conda/lib/python3.7/site-packages (from sphinx->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (2.11.2)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=2.3->sphinx->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (1.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.5.0->sphinx->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (1.26.2)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.5.0->sphinx->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.5.0->sphinx->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.5.0->sphinx->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (2020.12.5)\nCollecting livereload\n  Downloading livereload-2.6.3.tar.gz (25 kB)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sphinx-autobuild->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (0.4.4)\nRequirement already satisfied: tornado in /opt/conda/lib/python3.7/site-packages (from livereload->sphinx-autobuild->stable-baselines3[docs,tests]>=0.11.1->sb3-contrib) (5.0.2)\nCollecting PyEnchant>=3.1.1\n  Downloading pyenchant-3.2.0-py3-none-any.whl (55 kB)\n\u001b[K     |████████████████████████████████| 55 kB 2.5 MB/s  eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pytest-env, importlab, livereload\n  Building wheel for pytest-env (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytest-env: filename=pytest_env-0.6.2-py3-none-any.whl size=2370 sha256=fbebd4ae45c574e8beefef8e3cb378abaf027cbc3f13772c0cc62f28be0db56e\n  Stored in directory: /root/.cache/pip/wheels/00/78/6f/8413b85149939cd491ef2c5d3c5422e49cb2f898c8402f81f7\n  Building wheel for importlab (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for importlab: filename=importlab-0.6.1-py2.py3-none-any.whl size=21288 sha256=4794b05b43d2d3a70e3f2be1663e71e8b6253189b8098bd5e4234f95feee83f6\n  Stored in directory: /root/.cache/pip/wheels/b0/fa/22/7dc08c0a692b64347b964b10bbacbee247e9d8a3d00a2c0945\n  Building wheel for livereload (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for livereload: filename=livereload-2.6.3-py2.py3-none-any.whl size=24713 sha256=e8f887b204d80c69d9d31c62bf56a6d4e56eca0fab88684f255701cc5efd20be\n  Stored in directory: /root/.cache/pip/wheels/d4/f2/03/55f37093eb8cb0c89d7efb206f792dba55cd5bd67b1c5b1ce1\nSuccessfully built pytest-env importlab livereload\nInstalling collected packages: sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, snowballstemmer, imagesize, apipkg, alabaster, sphinx, pytest-forked, PyEnchant, ninja, livereload, importlab, execnet, coverage, sphinxcontrib.spelling, sphinx-autodoc-typehints, sphinx-autobuild, pytype, pytest-xdist, pytest-env, pytest-cov, isort, sb3-contrib\nSuccessfully installed PyEnchant-3.2.0 alabaster-0.7.12 apipkg-1.5 coverage-5.5 execnet-1.8.0 imagesize-1.2.0 importlab-0.6.1 isort-5.7.0 livereload-2.6.3 ninja-1.10.0.post2 pytest-cov-2.11.1 pytest-env-0.6.2 pytest-forked-1.3.0 pytest-xdist-2.2.1 pytype-2021.3.10 sb3-contrib-0.11.1 snowballstemmer-2.1.0 sphinx-3.5.2 sphinx-autobuild-2021.3.14 sphinx-autodoc-typehints-1.11.1 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-1.0.3 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 sphinxcontrib-serializinghtml-1.1.4 sphinxcontrib.spelling\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from stable_baselines3.common.env_util import make_atari_env\nfrom stable_baselines3.common.vec_env import VecFrameStack\nimport datetime\nfrom stable_baselines3 import DQN\nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport base64\nfrom pathlib import Path\nfrom IPython import display as ipythondisplay\nimport numpy as np\nfrom stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\nfrom stable_baselines3 import A2C\nfrom sb3_contrib import QRDQN\nfrom stable_baselines3 import PPO","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Καταγραφή video του actual gameplay του πράκτορα\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!apt-get install ffmpeg freeglut3-dev xvfb  -y # For visualization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up fake display; otherwise rendering will fail\nimport os\nos.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\nos.environ['DISPLAY'] = ':1'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"video_folder = '/kaggle/working/videos'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_videos(video_path='', prefix=''):\n  \"\"\"\n  Taken from https://github.com/eleurent/highway-env\n\n  :param video_path: (str) Path to the folder containing videos\n  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n  \"\"\"\n  html = []\n  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n      video_b64 = base64.b64encode(mp4.read_bytes())\n      html.append('''<video alt=\"{}\" autoplay \n                    loop controls style=\"height: 400px;\">\n                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n                </video>'''.format(mp4, video_b64.decode('ascii')))\n  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def record_video(eval_env, model, video_length=500, prefix='', video_folder=video_folder):\n  \"\"\"\n  :param env_id: (str)\n  :param model: (RL model)\n  :param video_length: (int)\n  :param prefix: (str)\n  :param video_folder: (str)\n  \"\"\"\n  # Start the video at step=0 and record video_length steps\n  env = VecVideoRecorder(eval_env, video_folder=video_folder,\n                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n                              name_prefix=prefix)\n\n  obs = env.reset()\n  for _ in range(video_length):\n    #action, _ = model.predict(obs)\n    #θα επιλέξουμε την ενέργεια με τις μεγαλύτερες πιθανότητες\n    actions, _ = model.predict(obs, deterministic=True)\n    obs, _, _, _ = env.step(actions)\n\n  # Close the video recorder\n  env.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Agent"},{"metadata":{},"cell_type":"markdown","source":"Αρχικά ορίζω ένα τυχαίο περιβάλλον για να τρέξει ο random agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"atari_env_name='AsterixDeterministic-v4'\n\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN δεν επιτρέπει multi envs\nenv = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\n# με ένα frame έχουμε τη θέση, με δύο την ταχύτητα, με τρία την επιτάχυνση και με τέσσερα τον ρυθμό μεταβολής της επιτάχυνσης (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ορίζω ένα τυχαίο πράκτορα"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_agent = DQN('CnnPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Και τώρα τον τεστάρω:"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(random_agent, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Πειράματα Αρχικοποίησης Μοντέλου"},{"metadata":{},"cell_type":"markdown","source":"Αρχικά ορίζω ένα περιβάλλον"},{"metadata":{"trusted":true},"cell_type":"code","source":"atari_env_name='AsterixDeterministic-v4'\n\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN δεν επιτρέπει multi envs\nenv = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\n# με ένα frame έχουμε τη θέση, με δύο την ταχύτητα, με τρία την επιτάχυνση και με τέσσερα τον ρυθμό μεταβολής της επιτάχυνσης (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Πειραμα CnnPolicy και 1000000 timesteps"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='dqn-CnnpPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model = DQN('CnnPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\ndqn_model.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(dqn_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dqn_model.save(\"dqn_AsterixDeterministic-v4_CnnPolicy\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Πειραμα CnnPolicy και 2000000 timesteps"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='dqn-CnnpPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model = DQN('CnnPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=2000000\ndqn_model.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(dqn_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dqn_model.save(\"dqn_AsterixDeterministic-v4_CnnPolicy_big\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy( test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Πειραμα ΜlpPolicy και 1000000 timesteps"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='dqn-MlpPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\ndqn_model.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(dqn_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dqn_model.save(\"dqn_AsterixDeterministic-v4_version2\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Πειραμα ΜlpPolicy και 2000000 timesteps"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='dqn-MlpPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=2000000\ndqn_model.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(dqn_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dqn_model.save(\"dqn_AsterixDeterministic-v4_version3\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Deterministic-v4"},{"metadata":{},"cell_type":"markdown","source":"### Πείραμα στο Deterministic-v4 με a2c"},{"metadata":{"trusted":true},"cell_type":"code","source":"atari_env_name='AsterixDeterministic-v4'\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN δεν επιτρέπει multi envs\nenv = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\n# με ένα frame έχουμε τη θέση, με δύο την ταχύτητα, με τρία την επιτάχυνση και με τέσσερα τον ρυθμό μεταβολής της επιτάχυνσης (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='a2c-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\na2c_model = A2C('CnnPolicy', env, verbose=1, tensorboard_log=model_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\na2c_model.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(a2c_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a2c_model.save(\"a2c_AsterixDeterministic-v4\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Πείραμα στο Deterministic-v4 με QRDQN"},{"metadata":{"trusted":true},"cell_type":"code","source":"atari_env_name='AsterixDeterministic-v4'\n\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN δεν επιτρέπει multi envs\nenv = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\n# με ένα frame έχουμε τη θέση, με δύο την ταχύτητα, με τρία την επιτάχυνση και με τέσσερα τον ρυθμό μεταβολής της επιτάχυνσης (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='QRDQN-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nQRDQN_model = QRDQN('CnnPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\nQRDQN_model.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(QRDQN_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"QRDQN_model.save(\"QRDQN_AsterixDeterministic-v4\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Πείραμα στο Deterministic-v4 με PPO"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='PPO-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nPPO_model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=model_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\nPPO_model.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(PPO_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# AsterixNoFrameskip-v4"},{"metadata":{},"cell_type":"markdown","source":"### dqn"},{"metadata":{"trusted":true},"cell_type":"code","source":"atari_env_name='AsterixNoFrameskip-v4'\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN δεν επιτρέπει multi envs\nenv = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\n# με ένα frame έχουμε τη θέση, με δύο την ταχύτητα, με τρία την επιτάχυνση και με τέσσερα τον ρυθμό μεταβολής της επιτάχυνσης (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='dqn-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model2 = DQN('CnnPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\ndqn_model2.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(dqn_model2, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dqn_model2.save(\"dqn_model_AsterixNoFrameskip-v4\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### a2c"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='a2c-MlpPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\na2c_model2 = A2C('CnnPolicy', env, verbose=1, tensorboard_log=model_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\na2c_model2.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(a2c_model2, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a2c_model2.save(\"a2c_AsterixNoFrameskip-v4\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### QRDQN"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='QRDQN-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nQRDQN_model = QRDQN('CnnPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\nQRDQN_model.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(QRDQN_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"QRDQN_model.save(\"QRDQN_AsterixNoFrameskip-v4\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PPO"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='PPO-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nPPO_model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=model_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\nPPO_model.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(PPO_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Asterix-v4 "},{"metadata":{"trusted":true},"cell_type":"code","source":"atari_env_name='Asterix-v4'\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN δεν επιτρέπει multi envs\nenv = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\n# με ένα frame έχουμε τη θέση, με δύο την ταχύτητα, με τρία την επιτάχυνση και με τέσσερα τον ρυθμό μεταβολής της επιτάχυνσης (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### dqn"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='dqn-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model2 = DQN('CnnPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\ndqn_model2.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(dqn_model2, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dqn_model2.save(\"dqn_model_Asterix-v4\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### a2c"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='a2c-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\na2c_model2 = A2C('CnnPolicy', env, verbose=1, tensorboard_log=model_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\na2c_model2.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(a2c_model2, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a2c_model2.save(\"a2c_model_Asterix-v4\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### QRDQN"},{"metadata":{"trusted":true},"cell_type":"code","source":"atari_env_name='Asterix-v4'\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN δεν επιτρέπει multi envs\nenv = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\n# με ένα frame έχουμε τη θέση, με δύο την ταχύτητα, με τρία την επιτάχυνση και με τέσσερα τον ρυθμό μεταβολής της επιτάχυνσης (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='QRDQN-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nQRDQN_model = QRDQN('CnnPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\nQRDQN_model.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(QRDQN_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"QRDQN_model.save(\"QRDQN_Asterix-v4\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PPO"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='PPO-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nPPO_model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=model_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\nPPO_model.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(PPO_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parametre Tunning"},{"metadata":{},"cell_type":"markdown","source":"### gamma=0.95"},{"metadata":{"trusted":true},"cell_type":"code","source":"atari_env_name='Asterix-v4'\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN δεν επιτρέπει multi envs\nenv = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\n# με ένα frame έχουμε τη θέση, με δύο την ταχύτητα, με τρία την επιτάχυνση και με τέσσερα τον ρυθμό μεταβολής της επιτάχυνσης (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='dqn-TUN-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model2 = DQN('CnnPolicy', env, verbose=1,gamma=0.95, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\ndqn_model2.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(dqn_model2, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dqn_model2.save(\"dqn_model_Asterix-v4_g_0.95\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"QRDQN"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='QRDQN-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nQRDQN_model = QRDQN('CnnPolicy', env, verbose=1, gamma=0.95, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\nQRDQN_model.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(QRDQN_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"batch_size=16"},{"metadata":{"trusted":true},"cell_type":"code","source":"atari_env_name='Asterix-v4'\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN δεν επιτρέπει multi envs\nenv = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\n# με ένα frame έχουμε τη θέση, με δύο την ταχύτητα, με τρία την επιτάχυνση και με τέσσερα τον ρυθμό μεταβολής της επιτάχυνσης (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='dqn-Batch-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model2 = DQN('CnnPolicy', env, verbose=1,gamma=0.95,batch_size=16, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\ndqn_model2.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(dqn_model2, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"QRDQN testing batch size =16"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='QRDQN-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nQRDQN_model = QRDQN('CnnPolicy', env, verbose=1, gamma=0.95, batch_size=16, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\nQRDQN_model.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(QRDQN_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### tau"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='dqn-Batch-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model2 = DQN('CnnPolicy', env, verbose=1,gamma=0.95, batch_size=16, tau=0.9, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\ndqn_model2.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(dqn_model2, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='QRDQN-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nQRDQN_model = QRDQN('CnnPolicy', env, verbose=1, gamma=0.95, batch_size=16,tau=0.9, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\nQRDQN_model.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(QRDQN_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"study learning rate"},{"metadata":{},"cell_type":"markdown","source":"dqn"},{"metadata":{"trusted":true},"cell_type":"code","source":"atari_env_name='Asterix-v4'\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN δεν επιτρέπει multi envs\nenv = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\n# με ένα frame έχουμε τη θέση, με δύο την ταχύτητα, με τρία την επιτάχυνση και με τέσσερα τον ρυθμό μεταβολής της επιτάχυνσης (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='dqn-Batch-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model2 = DQN('CnnPolicy', env, verbose=1,gamma=0.95, batch_size=16,learning_rate=0.00005, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\ndqn_model2.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(dqn_model2, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"atari_env_name='Asterix-v4'\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN δεν επιτρέπει multi envs\nenv = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\n# με ένα frame έχουμε τη θέση, με δύο την ταχύτητα, με τρία την επιτάχυνση και με τέσσερα τον ρυθμό μεταβολής της επιτάχυνσης (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='QRDQN-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nQRDQN_model = QRDQN('CnnPolicy', env, verbose=1, gamma=0.95, batch_size=32,learning_rate=0.0005, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=1000000\nQRDQN_model.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(QRDQN_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Τελικό Πείραμα"},{"metadata":{},"cell_type":"markdown","source":"### Πρώτο"},{"metadata":{"trusted":true},"cell_type":"code","source":"atari_env_name='Asterix-v0'\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN δεν επιτρέπει multi envs\nenv = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\n# με ένα frame έχουμε τη θέση, με δύο την ταχύτητα, με τρία την επιτάχυνση και με τέσσερα τον ρυθμό μεταβολής της επιτάχυνσης (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='dqn-Batch-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model2 = DQN('CnnPolicy', env, verbose=1,gamma=0.95, batch_size=16, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=3000000\ndqn_model2.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(dqn_model2, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dqn_model2.save(\"final-v0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"record_video(test_env,dqn_model2, video_length=5000, prefix='v0',video_folder = '/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Δεύτερο"},{"metadata":{"trusted":true},"cell_type":"code","source":"atari_env_name='Asterix-v4'\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN δεν επιτρέπει multi envs\nenv = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\n# με ένα frame έχουμε τη θέση, με δύο την ταχύτητα, με τρία την επιτάχυνση και με τέσσερα τον ρυθμό μεταβολής της επιτάχυνσης (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='dqn-Batch-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model2 = DQN('CnnPolicy', env, verbose=1,gamma=0.95, batch_size=16, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=3000000\ndqn_model2.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(dqn_model2, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dqn_model2.save(\"final-v4\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"record_video(test_env,dqn_model2, video_length=5000, prefix='v4',video_folder = '/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Τρίτο Πείραμα"},{"metadata":{"trusted":true},"cell_type":"code","source":"atari_env_name='Asterix-v0'\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN δεν επιτρέπει multi envs\nenv = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\n# με ένα frame έχουμε τη θέση, με δύο την ταχύτητα, με τρία την επιτάχυνση και με τέσσερα τον ρυθμό μεταβολής της επιτάχυνσης (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1, seed=0)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name='QRDQN-CnnPolicy'\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nQRDQN_model = QRDQN('CnnPolicy', env, verbose=1, gamma=0.95, batch_size=32, tensorboard_log=model_log, buffer_size=100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_steps=3000000\nQRDQN_model.learn(total_timesteps=max_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_reward, std_reward = evaluate_policy(QRDQN_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+/-{std_reward})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"QRDQN_model.save(\"qrdqn-v0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"record_video(test_env,QRDQN_model, video_length=5000, prefix='qrdqn-v0',video_folder = '/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Φόρτωση εκπαιδευμένου μοντέλου \n"},{"metadata":{},"cell_type":"markdown","source":"### Φόρτωση του final-v4"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget --no-check-certificate 'https://www.dropbox.com/s/yvhx5qlvbohwurf/final-v4.zip?dl=1' -O final-v4.zip","execution_count":5,"outputs":[{"output_type":"stream","text":"--2021-03-16 01:50:39--  https://www.dropbox.com/s/yvhx5qlvbohwurf/final-v4.zip?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.9.18, 2620:100:601f:18::a27d:912\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.9.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/yvhx5qlvbohwurf/final-v4.zip [following]\n--2021-03-16 01:50:39--  https://www.dropbox.com/s/dl/yvhx5qlvbohwurf/final-v4.zip\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://ucb37140f0b37e32c08e91887c86.dl.dropboxusercontent.com/cd/0/get/BKxo0I6x7sJRBc2ARjpzmNaIB_fj5O58uQchEtMys16J3GuTv5OYLCoLbI3doLT7nFRyuQ1eMSaXCRTdSB1ucMtUwY5S9i1npziOU-Vvi9rOCgIX3V4nWs7oxjCZyfgtGTDG97r_2KRv2hzaEUrsUhlY/file?dl=1# [following]\n--2021-03-16 01:50:39--  https://ucb37140f0b37e32c08e91887c86.dl.dropboxusercontent.com/cd/0/get/BKxo0I6x7sJRBc2ARjpzmNaIB_fj5O58uQchEtMys16J3GuTv5OYLCoLbI3doLT7nFRyuQ1eMSaXCRTdSB1ucMtUwY5S9i1npziOU-Vvi9rOCgIX3V4nWs7oxjCZyfgtGTDG97r_2KRv2hzaEUrsUhlY/file?dl=1\nResolving ucb37140f0b37e32c08e91887c86.dl.dropboxusercontent.com (ucb37140f0b37e32c08e91887c86.dl.dropboxusercontent.com)... 162.125.9.15, 2620:100:601f:15::a27d:90f\nConnecting to ucb37140f0b37e32c08e91887c86.dl.dropboxusercontent.com (ucb37140f0b37e32c08e91887c86.dl.dropboxusercontent.com)|162.125.9.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 27283871 (26M) [application/binary]\nSaving to: ‘final-v4.zip’\n\nfinal-v4.zip        100%[===================>]  26.02M  45.3MB/s    in 0.6s    \n\n2021-03-16 01:50:40 (45.3 MB/s) - ‘final-v4.zip’ saved [27283871/27283871]\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_v4 = DQN.load(\"final-v4\", verbose=1)","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Φόρτωση του final-v0"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget --no-check-certificate 'https://www.dropbox.com/s/94nonly13wr2zjd/final-v0.zip?dl=1' -O final-v0.zip","execution_count":9,"outputs":[{"output_type":"stream","text":"--2021-03-16 01:54:37--  https://www.dropbox.com/s/94nonly13wr2zjd/final-v0.zip?dl=1\nResolving www.dropbox.com (www.dropbox.com)... 162.125.9.18, 2620:100:601f:18::a27d:912\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.9.18|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/dl/94nonly13wr2zjd/final-v0.zip [following]\n--2021-03-16 01:54:37--  https://www.dropbox.com/s/dl/94nonly13wr2zjd/final-v0.zip\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc90c9fd0be3b8d2f19896e1bdb2.dl.dropboxusercontent.com/cd/0/get/BKxJRyjVtyI1OuJ4JB4b_jh7pv_qQQJkU2V21HWdWr3Fk64Rr9oASkQs7MUgzdgsqrQNbL4b-MThiZ1yeB8p_K_YjpncVzA0LE5aQ4OSM3OTA8vBtrAANO70KGcAeuLx5PpptA7Ewt__-aAibvw9H2Q_/file?dl=1# [following]\n--2021-03-16 01:54:38--  https://uc90c9fd0be3b8d2f19896e1bdb2.dl.dropboxusercontent.com/cd/0/get/BKxJRyjVtyI1OuJ4JB4b_jh7pv_qQQJkU2V21HWdWr3Fk64Rr9oASkQs7MUgzdgsqrQNbL4b-MThiZ1yeB8p_K_YjpncVzA0LE5aQ4OSM3OTA8vBtrAANO70KGcAeuLx5PpptA7Ewt__-aAibvw9H2Q_/file?dl=1\nResolving uc90c9fd0be3b8d2f19896e1bdb2.dl.dropboxusercontent.com (uc90c9fd0be3b8d2f19896e1bdb2.dl.dropboxusercontent.com)... 162.125.9.15, 2620:100:601f:15::a27d:90f\nConnecting to uc90c9fd0be3b8d2f19896e1bdb2.dl.dropboxusercontent.com (uc90c9fd0be3b8d2f19896e1bdb2.dl.dropboxusercontent.com)|162.125.9.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 27283870 (26M) [application/binary]\nSaving to: ‘final-v0.zip’\n\nfinal-v0.zip        100%[===================>]  26.02M  50.9MB/s    in 0.5s    \n\n2021-03-16 01:54:39 (50.9 MB/s) - ‘final-v0.zip’ saved [27283870/27283870]\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_v0 = DQN.load(\"final-v0\", verbose=1)","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Φόρτωση του qrdqn-v0"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget --no-check-certificate 'https://www.dropbox.com/s/r33h3aiv5v1telf/qrdqn-v0.zip?dl=1' -O qrdqn-v0.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qrdqn_v0 = QRDQN.load(\"qrdqn-v0\", verbose=1)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}