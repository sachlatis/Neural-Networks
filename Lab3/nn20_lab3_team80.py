# -*- coding: utf-8 -*-
"""nn20_lab3_team80.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-7WTT2ljzGZk2WTBQad3oEfGD8n-ko_n

# Ομάδα 80
*   Στέφανος-Σταμάτης Αχλάτης, 03116149, sachlatis@gmail.com
*   Νικολέτα-Μαρκέλα, Ηλιακοπούλου, 03116111, nmiliakopoulou@gmail.com
*   Γεωργία Σταυροπούλου, 03116162, stavrgeorgia@gmail.com


**Επισυναπτεται PDF αρχείο με την αναφορά**

# Βαθιά μάθηση στο CIFAR-100

## Εισαγωγή και επισκόπηση του συνόλου δεδομένων
"""

from __future__ import absolute_import, division, print_function, unicode_literals # legacy compatibility

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
import datetime

# Basic imports
import os
import gc
import time
import numpy as np
import pandas as pd
import librosa
import librosa.display
import re
import keras
import random
import zipfile
from imgaug import augmenters as iaa
from math import sqrt, pi, exp
import pickle
import random
import collections
import csv
import ntpath
import functools
import matplotlib
import math
import itertools
import seaborn as sns
from IPython.display import display, HTML
from importlib.machinery import SourceFileLoader

# Sklearn related imports
from sklearn.model_selection import StratifiedKFold, KFold, learning_curve, ShuffleSplit, GridSearchCV
from sklearn.naive_bayes import GaussianNB
from sklearn import metrics
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error , accuracy_score, confusion_matrix
from sklearn.datasets import load_digits
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.decomposition import PCA
from sklearn.preprocessing import scale, StandardScaler
from sklearn.model_selection import learning_curve
from sklearn.metrics import classification_report, precision_recall_fscore_support, f1_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.ensemble import VotingClassifier, BaggingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, f1_score
from sklearn.metrics import roc_auc_score

# Scipy related imports
from scipy.spatial import Voronoi, voronoi_plot_2d
from scipy.stats import norm, multivariate_normal
from scipy.signal import butter, deconvolve
from scipy import signal
from scipy.linalg import svd

# Torch related imports
from torch.utils.data import DataLoader, TensorDataset
from torchvision.utils import make_grid
from torch.nn.utils.rnn import pack_padded_sequence
from torch.utils.data.sampler import SubsetRandomSampler
from torch.nn.utils.rnn import pad_packed_sequence
import torch
from torchvision import transforms, models
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import Dataset

import pywt 
from statsmodels.robust import mad

# Plot related imports 
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
from plotly.subplots import make_subplots
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from numba import jit
from math import log, floor
from sklearn.neighbors import KDTree
from pathlib import Path
from sklearn.utils import shuffle
import seaborn as sns
from matplotlib import colors
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize

# Keras elated imports
from keras import Input, metrics
from keras.callbacks import TensorBoard, ModelCheckpoint
from keras.layers.normalization import BatchNormalization
from keras.engine import Model
from keras.layers import Conv1D, MaxPooling1D, UpSampling1D
from keras.optimizers import Adadelta, SGD
from keras import backend as K
from keras import optimizers, losses, activations, models
from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau
from keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, concatenate
from keras.models import Sequential
from keras.utils.np_utils import to_categorical
from keras.layers import Dropout, Flatten, Dense
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from keras.metrics import Precision
from keras.metrics import Recall

# keras tuner
! pip install keras-tuner
from kerastuner import RandomSearch
from kerastuner.engine.hyperparameters import HyperParameters

# Future statement definitions
from __future__ import absolute_import, division, print_function, unicode_literals # legacy compatibility
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from math import ceil

drive.mount('/content/drive')
sns.set()

# helper functions

# select from from_list elements with index in index_list
def select_from_list(from_list, index_list):
  filtered_list= [from_list[i] for i in index_list]
  return(filtered_list)

# append in filtered_list the index of each element of unfilterd_list if it exists in in target_list
def get_ds_index(unfiliterd_list, target_list):
  index = 0
  filtered_list=[]
  for i_ in unfiliterd_list:
    if i_[0] in target_list:
      filtered_list.append(index)
    index += 1
  return(filtered_list)

# select a url for a unique subset of CIFAR-100 with 20, 40, 60, or 80 classes
def select_classes_number(classes_number = 20):
  cifar100_20_classes_url = "https://pastebin.com/raw/nzE1n98V"
  cifar100_40_classes_url = "https://pastebin.com/raw/zGX4mCNP"
  cifar100_60_classes_url = "https://pastebin.com/raw/nsDTd3Qn"
  cifar100_80_classes_url = "https://pastebin.com/raw/SNbXz700"
  if classes_number == 20:
    return cifar100_20_classes_url
  elif classes_number == 40:
    return cifar100_40_classes_url
  elif classes_number == 60:
    return cifar100_60_classes_url
  elif classes_number == 80:
    return cifar100_80_classes_url
  else:
    return -1

# load the entire dataset
(x_train_all, y_train_all), (x_test_all, y_test_all) = tf.keras.datasets.cifar100.load_data(label_mode='fine')

print(x_train_all.shape)

"""Η κάθε ομάδα θα δουλέψει με ένα μοναδικό ξεχωριστό υποσύνολο του CIFAR-100
Στο επόμενο κελί, αντικαταστήστε την τιμή της μεταβλητής `team_seed` με τον αριθμό της ομάδας σας.
"""

# REPLACE WITH YOUR TEAM NUMBER
team_seed = 80

"""Στο επόμενο κελί μπορείτε να διαλέξετε το πλήθος των κατηγορίων σας: 20 (default), 40, 60 ή 80."""

# select the number of classes
cifar100_classes_url = select_classes_number()

"""Δημιουργούμε το μοναδικό dataset της ομάδας μας:"""

team_classes = pd.read_csv(cifar100_classes_url, sep=',', header=None)
CIFAR100_LABELS_LIST = pd.read_csv('https://pastebin.com/raw/qgDaNggt', sep=',', header=None).astype(str).values.tolist()[0]

our_index = team_classes.iloc[team_seed,:].values.tolist()
our_classes = select_from_list(CIFAR100_LABELS_LIST, our_index)
train_index = get_ds_index(y_train_all, our_index)
test_index = get_ds_index(y_test_all, our_index)

x_train_ds = np.asarray(select_from_list(x_train_all, train_index))
y_train_ds = np.asarray(select_from_list(y_train_all, train_index))
x_test_ds = np.asarray(select_from_list(x_test_all, test_index))
y_test_ds = np.asarray(select_from_list(y_test_all, test_index))

# print our classes
print(our_classes)

CLASSES_NUM=len(our_classes)

print(x_train_ds[1].shape)

# get (train) dataset dimensions
data_size, img_rows, img_cols, img_channels = x_train_ds.shape

# set validation set percentage (wrt the training set size)
validation_percentage = 0.15
val_size = round(validation_percentage * data_size)

# Reserve val_size samples for validation and normalize all values
x_val = x_train_ds[-val_size:]/255
y_val = y_train_ds[-val_size:]
x_train = x_train_ds[:-val_size]/255
y_train = y_train_ds[:-val_size]
x_test = x_test_ds/255
y_test = y_test_ds

print(len(x_val))

# summarize loaded dataset
print('Train: X=%s, y=%s' % (x_train.shape, y_train.shape))
print('Validation: X=%s, y=%s' % (x_val.shape, y_val.shape))
print('Test: X=%s, y=%s' % (x_test.shape, y_test.shape))

# get class label from class index
def class_label_from_index(fine_category):
  return(CIFAR100_LABELS_LIST[fine_category.item(0)])

# plot first few images
plt.figure(figsize=(6, 6))
for i in range(9):
	# define subplot
  plt.subplot(330 + 1 + i).set_title(class_label_from_index(y_train[i]))
	# plot raw pixel data
  plt.imshow(x_train[i], cmap=plt.get_cmap('gray'))
  #show the figure
plt.show()

y_labeled_df = pd.DataFrame({"Name" : [class_label_from_index(y) for y in list(y_train[:,0]) + list(y_test[:,0])]})
fig, ax = plt.subplots(figsize=(25, 5))
ax = sns.countplot( x = "Name", data = y_labeled_df,palette="Set3" )

"""#Δημιουγία εικόνων μεγαλύτερων Διαστάσεων"""

def _bytes_feature(value):
    # If the value is an eager tensor BytesList won't unpack a string from an EagerTensor.
    if isinstance(value, type(tf.constant(0))):
        value = value.numpy() 
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def _int64_feature(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

def serialize_example(image, label, image_shape):
    feature = {
        "image": _bytes_feature(image),
        "label": _int64_feature(label),
        'height': _int64_feature(image_shape[0]),
        'width': _int64_feature(image_shape[1]),
        'depth': _int64_feature(image_shape[2]),
    }

    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))
    return example_proto.SerializeToString()

def write_TFR(tfrecord_dir, x, y):
  with tf.io.TFRecordWriter(tfrecord_dir) as writer:
      for img_array, label in zip(x, y):
                
          img_array = tf.image.resize(img_array, (TARGET_SIZE[0], TARGET_SIZE[1])) 
          img_bytes = tf.io.serialize_tensor(img_array)
          image_shape = img_array.shape
          example = serialize_example(img_bytes, label, image_shape)
          writer.write(example)
  return None

def _parse_function(example_proto):
    feature_description = {
        'image': tf.io.FixedLenFeature([], tf.string),
        'label': tf.io.FixedLenFeature([], tf.int64),
        'height': tf.io.FixedLenFeature([], tf.int64),
        'width': tf.io.FixedLenFeature([], tf.int64),
        'depth': tf.io.FixedLenFeature([], tf.int64)
    }
    example = tf.io.parse_single_example(example_proto, feature_description)

    label = example["label"]
    image_shape = TARGET_SIZE

    image = tf.io.parse_tensor(example["image"], float)
    image = tf.reshape(image, image_shape)


    return  image , [label]

def read_dataset(file):
    dataset = tf.data.TFRecordDataset(file)
    return dataset.map(_parse_function)

"""## 74 dim img"""

TARGET_SIZE = (74,74,3)

dir_74dim_img = "/content/drive/My Drive/Neural_Networks/lab3/dir_74dim_img/"

write_TFR(dir_74dim_img + "train_74dim", x_train, y_train)
write_TFR(dir_74dim_img + "val_74dim", x_val, y_val)
write_TFR(dir_74dim_img + "test_74dim", x_test, y_test)

train_ds_74 = read_dataset(dir_74dim_img + "train_74dim")
validation_ds_74 = read_dataset(dir_74dim_img + "val_74dim")
test_ds_74 = read_dataset(dir_74dim_img + "test_74dim")

plt.figure(figsize=(10,10))
for i, data in enumerate(train_ds_74.take(9)):
    img = tf.keras.preprocessing.image.array_to_img(data[0])
    plt.subplot(3,3,i+1)
    plt.imshow(img)
plt.show()

"""# Συναρτήσεις εκπαίδευσης

Θα χρησιμοποιήσουμε την ιδιότητα data prefetch του tf2:
"""

# we user prefetch https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch 
# see also AUTOTUNE
# the dataset is now "infinite"

BATCH_SIZE = 128
AUTOTUNE = tf.data.experimental.AUTOTUNE # https://www.tensorflow.org/guide/data_performance
LABEL  = sorted(list(set([y for y in list(y_train[:,0]) + list(y_test[:,0])])))
LABEL_DICT = {LABEL[i] : i for i in range(len(LABEL))}

def _input_fn(x,y, BATCH_SIZE):
  ds = tf.data.Dataset.from_tensor_slices((x,y))
  ds = ds.shuffle(buffer_size=data_size)
  ds = ds.repeat()
  ds = ds.batch(BATCH_SIZE)
  ds = ds.prefetch(buffer_size=AUTOTUNE)
  return ds

def _input_fn_for_bigger_images(ds, BATCH_SIZE):
  ds = ds.shuffle(buffer_size=data_size)
  ds = ds.repeat()
  #ds = ds.cache()
  ds = ds.batch(BATCH_SIZE)
  ds = ds.prefetch(buffer_size=AUTOTUNE)
  return ds

def map_label(l):
  l = [ [ LABEL_DICT[i[0]] ]  for i in l]
  return np.array(l)

train_ds =_input_fn(x_train,y_train, BATCH_SIZE) #PrefetchDataset object
validation_ds =_input_fn(x_val,y_val, BATCH_SIZE) #PrefetchDataset object
test_ds =_input_fn(x_test,y_test, BATCH_SIZE) #PrefetchDataset object
augment = False #Data augmentation
# steps_per_epoch and validation_steps for training and validation: https://www.tensorflow.org/guide/keras/train_and_evaluate

def train_model(model,name,early_stopping, epochs = 10, steps_per_epoch = 2, verbose=2 , validation_steps = 1):
  
  dir = os.path.join('/content/drive/My Drive/Neural_Networks/lab3/models', name)
  
  if (early_stopping==1):
    tensorboard_callback = tf.keras.callbacks.TensorBoard(dir, histogram_freq=1)
    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10, restore_best_weights=True) 
    history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps, callbacks=[tensorboard_callback, early_stopping_callback])
  else:
    history = model.fit(train_ds, epochs=epochs, verbose=2, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps)
  return(history)

"""## Γραφικές παραστάσεις εκπαίδευσης και απόδοση στο σύνολο ελέγχου"""

def precision(y_true, y_pred): #taken from old keras source code
     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
     precision = true_positives / (predicted_positives + K.epsilon())
     return precision
def recall(y_true, y_pred): #taken from old keras source code
     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
     recall = true_positives / (possible_positives + K.epsilon())
     return recall

# plot diagnostic learning curves
def summarize_diagnostics(history):
  fig, axs = plt.subplots(2, 1, figsize=(8, 8))
  axs[0].set_title('Cross Entropy Loss')
  axs[0].plot(history.history['loss'], color='blue', label='train')
  axs[0].plot(history.history['val_loss'], color='orange', label='val')
  axs[0].legend(loc='best')
  axs[1].set_title('Classification Accuracy')
  axs[1].plot(history.history['accuracy'], color='blue', label='train')
  axs[1].plot(history.history['val_accuracy'], color='orange', label='val')
  axs[1].legend(loc='best')
  #axs[2].set_title('Classification Precision')
  #axs[2].plot(history.history['precision'], color='blue', label='train')
  #axs[2].plot(history.history['val_precision'], color='orange', label='val')
  #axs[2].legend(loc='best')
  #axs[3].set_title('Classification Recall')
  #axs[3].plot(history.history['recall'], color='blue', label='train')
  #axs[3].plot(history.history['recall'], color='orange', label='val')
  #axs[3].legend(loc='best')
  plt.tight_layout()
  return plt
 
# print test set evaluation metrics
def model_evaluation(model, evaluation_steps=10):
  print('\nTest set evaluation metrics')
  #loss0,accuracy0,precision0,recall0 = model.evaluate(test_ds, steps = evaluation_steps)
  loss0,accuracy0 = model.evaluate(test_ds, steps = evaluation_steps)
  print("Loss: {:.2f}".format(loss0))
  print("Accuracy: {:.2f}".format(accuracy0))
  #print("Precision: {:.2f}".format(precision0))
  #print("Recall: {:.2f}".format(recall0))

def model_report(model, history, evaluation_steps = 10):
	plt = summarize_diagnostics(history)
	plt.show()
	model_evaluation(model, evaluation_steps)

"""## Περισσότερες Λειτουγικότητες"""

def color(x,y):
    """Color augmentation
    Args:
        x: Image

    Returns:
        Augmented image
    """
    x = tf.image.random_hue(x, 0.08)
    x = tf.image.random_saturation(x, 0.6, 1.6)
    x = tf.image.random_brightness(x, 0.05)
    x = tf.image.random_contrast(x, 0.7, 1.3)
    return tf.clip_by_value(x, 0, 1),y

def crop(x,y,dim_input=32):
    """Crop augmentation
    Args:
        x: Image

    Returns:
        Augmented image
    """
    x = tf.image.resize_with_crop_or_pad(
        x, dim_input + 8,dim_input+ 8)
    x = tf.image.random_crop(x, [dim_input, dim_input, 3])
    return x,y


def flip(x,y):
    """Flip augmentation

    Args:
        x: Image

    Returns:
        Augmented image
    """
    x = tf.image.random_flip_left_right(x)
    return tf.clip_by_value(x, 0, 1),y
augment=True

"""## Μοντέλα δικτύων

# Ένα μικρό συνελικτικό δίκτυο "from scratch"
"""

train_ds =_input_fn(x_train,y_train, BATCH_SIZE) #PrefetchDataset object
validation_ds =_input_fn(x_val,y_val, BATCH_SIZE) #PrefetchDataset object
test_ds =_input_fn(x_test,y_test, BATCH_SIZE) #PrefetchDataset object
augment = False #Data augmentation

# a simple CNN https://www.tensorflow.org/tutorials/images/cnn
def init_zero_model(summary,input,optimizer):
  model = models.Sequential()
  model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(input,input,3)))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(64, (3, 3), activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(64, (3, 3), activation='relu'))
  model.add(layers.Flatten())
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dense(100, activation='softmax'))

  model.compile(optimizer=optimizer, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

  init_zero_model(True,32,tf.optimizers.Adam(learning_rate=0.0001))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# zero_model = init_zero_model(True,32,tf.optimizers.Adam(learning_rate=0.0001))
# zero_model_history = train_model(zero_model,"zero_model_1",False, 60, 40, 10)

model_report(zero_model,zero_model_history)
zero_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/zero_model_32.h5')

"""# Δικά Μας μοντέλα

## Πρώτο Μοντέλο
"""

train_ds =_input_fn(x_train,y_train, BATCH_SIZE) #PrefetchDataset object
validation_ds =_input_fn(x_val,y_val, BATCH_SIZE) #PrefetchDataset object
test_ds =_input_fn(x_test,y_test, BATCH_SIZE) #PrefetchDataset object
augment = False #Data augmentation

def init_first_model(summary,input,optimizer):
  
  model = models.Sequential()
  model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(input,input,3)))
  model.add(layers.MaxPooling2D(pool_size=(2, 2)))
  model.add(layers.BatchNormalization(axis=-1,scale=False))
  model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))
  model.add(layers.MaxPooling2D(pool_size=(2, 2)))
  model.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))
  model.add(layers.MaxPooling2D(pool_size=(2, 2)))
  model.add(layers.Flatten())
  model.add(layers.Dense(256, activation='relu'))
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dense(100, activation='softmax'))
  
  model.compile(optimizer=optimizer, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.optimizers.Adam(learning_rate=0.0001))
# first_model_history = train_model(first_model,"first_model_1",False, 60, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.keras.optimizers.Nadam( learning_rate=0.001 ))
# first_model_history = train_model(first_model,"first_model_1",False, 60, 40, 10)
#

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

"""## Second Mine"""

train_ds =_input_fn(x_train,y_train, BATCH_SIZE) #PrefetchDataset object
validation_ds =_input_fn(x_val,y_val, BATCH_SIZE) #PrefetchDataset object
test_ds =_input_fn(x_test,y_test, BATCH_SIZE) #PrefetchDataset object
augment = False #Data augmentation

def init_second_model(summary,input,optimizer):
  
  model = models.Sequential()
  model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(input,input,3)))
  model.add(layers.MaxPooling2D(pool_size=(2, 2)))
  model.add(layers.BatchNormalization(axis=-1,scale=False))
  model.add(layers.Conv2D(128, kernel_size=(2, 2), activation='relu'))
  model.add(layers.MaxPooling2D(pool_size=(2, 2)))
  model.add(layers.Dropout(rate=0.4))
  model.add(layers.Conv2D(256, kernel_size=(2, 2), activation='relu'))
  model.add(layers.MaxPooling2D(pool_size=(2, 2)))
  model.add(layers.Dropout(rate=0.4))
  model.add(layers.Conv2D(512, kernel_size=(2, 2), activation='relu'))
  model.add(layers.MaxPooling2D(pool_size=(2, 2)))
  model.add(layers.Dropout(rate=0.4))
  model.add(layers.Flatten())
  model.add(layers.Dense(1024, activation='relu'))
  model.add(layers.Dense(512, activation='relu'))
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dense(100, activation='softmax'))
  
  model.compile(optimizer=optimizer, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# second_model = init_second_model(True,32,tf.optimizers.Adam(learning_rate=0.0001))
# second_model_history = train_model(second_model,"second_model_1",False, 60, 40, 10)

model_report(second_model,second_model_history)
second_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/second_model_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# second_model = init_second_model(True,32,tf.keras.optimizers.Nadam( learning_rate=0.001 ))
# second_model_history = train_model(second_model,"second_model_1",False, 60, 40, 10)

model_report(second_model,second_model_history)
second_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/second_model_32.h5')

"""## Third Model"""

train_ds =_input_fn(x_train,y_train, BATCH_SIZE) #PrefetchDataset object
validation_ds =_input_fn(x_val,y_val, BATCH_SIZE) #PrefetchDataset object
test_ds =_input_fn(x_test,y_test, BATCH_SIZE) #PrefetchDataset object
augment = False #Data augmentation

def init_third_model(summary,input,optimizer):
  
  model = models.Sequential()
  model.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(input,input,3)))
  model.add(layers.MaxPooling2D(pool_size=(2, 2)))
  model.add(layers.Dropout(rate=0.4))
  model.add(layers.BatchNormalization(axis=-1,scale=False))

  model.add(layers.Conv2D(512, kernel_size=(2, 2), activation='relu'))
  model.add(layers.MaxPooling2D(pool_size=(2, 2)))
  model.add(layers.Dropout(rate=0.4))
  model.add(layers.BatchNormalization(axis=-1,scale=False))


  model.add(layers.Conv2D(1024, kernel_size=(2, 2), activation='relu'))
  model.add(layers.MaxPooling2D(pool_size=(2, 2)))
  model.add(layers.Dropout(rate=0.4))
  model.add(layers.BatchNormalization(axis=-1,scale=False))

  model.add(layers.Conv2D(512, kernel_size=(2, 2), activation='relu'))
  model.add(layers.MaxPooling2D(pool_size=(2, 2)))
  model.add(layers.Dropout(rate=0.4))
  model.add(layers.BatchNormalization(axis=-1,scale=False))
  
  model.add(layers.Flatten())
  model.add(layers.Dense(2048, activation='relu'))
  model.add(layers.Dense(512, activation='relu'))
  model.add(layers.Dense(1024, activation='relu'))
  model.add(layers.Dense(512, activation='relu'))
  model.add(layers.Dense(100, activation='softmax'))
  
  model.compile(optimizer=optimizer, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# third_model = init_third_model(True,32,tf.optimizers.Adam(learning_rate=0.0001))
# third_model_history = train_model(third_model,"third_model_1",False, 100, 40, 10)

model_report(third_model,third_model_history)
third_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/third_model_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# third_model = init_third_model(True,32,tf.keras.optimizers.Nadam( learning_rate=0.001 ))
# third_model_history = train_model(third_model,"third_model_1",False, 100, 40, 10)

model_report(third_model,third_model_history)
third_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/third_model_32.h5')

"""## Forth"""

train_ds =_input_fn(x_train,y_train, BATCH_SIZE) #PrefetchDataset object
validation_ds =_input_fn(x_val,y_val, BATCH_SIZE) #PrefetchDataset object
test_ds =_input_fn(x_test,y_test, BATCH_SIZE) #PrefetchDataset object
augment = False #Data augmentation

def init_fourth_model(summary,input,optimizer):
  model = models.Sequential()
  model.add(layers.Conv2D(32, (5, 5),padding='valid',activation='relu', input_shape=(input, input, 3)))
  model.add(layers.Conv2D(64, (5, 5),padding='valid',activation='relu'))
  model.add(layers.MaxPooling2D(pool_size=(2, 2)))
  model.add(layers.Dropout(rate=0.25))
  model.add(layers.Conv2D(64, (3, 3),padding='valid',activation='relu'))
  model.add(layers.Conv2D(64, (3, 3),padding='valid',activation='relu'))
  model.add(layers.MaxPooling2D(pool_size=(2, 2)))
  model.add(layers.Dropout(rate=0.4))  
  model.add(layers.Flatten())
  model.add(layers.Dense(1024, activation='relu'))
  model.add(layers.Dropout(rate=0.4))
  model.add(layers.Dense(512, activation='relu'))
  model.add(layers.Dropout(rate=0.4))
  model.add(layers.Dense(100, activation='softmax'))
  
  model.compile(optimizer=optimizer, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# forth_model = init_fourth_model(True,32,tf.keras.optimizers.Nadam( learning_rate=0.001 ))
# forth_model_history = train_model(forth_model,"forth_model_1",False, 60, 40, 10)

model_report(forth_model,forth_model_history)
forth_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/forth_model_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# forth_model = init_fourth_model(True,32,tf.optimizers.Adam(learning_rate=0.0001))
# forth_model_history = train_model(forth_model,"forth_model_1",False, 60, 40, 10)

model_report(forth_model,forth_model_history)
forth_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/forth_model_32.h5')

"""#Βελτιστοποίηση Του καλύτερου μοντέλου (First)

## Μελέτη Πλήθους κλάσεων
"""

cifar100_classes_url = select_classes_number(80)

team_classes = pd.read_csv(cifar100_classes_url, sep=',', header=None)
CIFAR100_LABELS_LIST = pd.read_csv('https://pastebin.com/raw/qgDaNggt', sep=',', header=None).astype(str).values.tolist()[0]

our_index = team_classes.iloc[team_seed,:].values.tolist()
our_classes = select_from_list(CIFAR100_LABELS_LIST, our_index)
train_index = get_ds_index(y_train_all, our_index)
test_index = get_ds_index(y_test_all, our_index)

x_train_ds = np.asarray(select_from_list(x_train_all, train_index))
y_train_ds = np.asarray(select_from_list(y_train_all, train_index))
x_test_ds = np.asarray(select_from_list(x_test_all, test_index))
y_test_ds = np.asarray(select_from_list(y_test_all, test_index))

print(our_classes)

# get (train) dataset dimensions
data_size, img_rows, img_cols, img_channels = x_train_ds.shape

# set validation set percentage (wrt the training set size)
validation_percentage = 0.15
val_size = round(validation_percentage * data_size)

# Reserve val_size samples for validation and normalize all values
x_val = x_train_ds[-val_size:]/255
y_val = y_train_ds[-val_size:]
x_train = x_train_ds[:-val_size]/255
y_train = y_train_ds[:-val_size]
x_test = x_test_ds/255
y_test = y_test_ds

print(len(x_val))

# summarize loaded dataset
print('Train: X=%s, y=%s' % (x_train.shape, y_train.shape))
print('Validation: X=%s, y=%s' % (x_val.shape, y_val.shape))
print('Test: X=%s, y=%s' % (x_test.shape, y_test.shape))

# get class label from class index
def class_label_from_index(fine_category):
  return(CIFAR100_LABELS_LIST[fine_category.item(0)])

# plot first few images
plt.figure(figsize=(6, 6))
for i in range(9):
	# define subplot
  plt.subplot(330 + 1 + i).set_title(class_label_from_index(y_train[i]))
	# plot raw pixel data
  plt.imshow(x_train[i], cmap=plt.get_cmap('gray'))
  #show the figure
plt.show()

BATCH_SIZE =128
train_ds =_input_fn(x_train,y_train, BATCH_SIZE) #PrefetchDataset object
validation_ds =_input_fn(x_val,y_val, BATCH_SIZE) #PrefetchDataset object
test_ds =_input_fn(x_test,y_test, BATCH_SIZE) #PrefetchDataset object


def train_model(model,name,early_stopping, epochs = 10, steps_per_epoch = 2, verbose=2 , validation_steps = 1):
  
  dir = os.path.join('/content/drive/My Drive/Neural_Networks/lab3/models', name)
  
  if (early_stopping==1):
    tensorboard_callback = tf.keras.callbacks.TensorBoard(dir, histogram_freq=1)
    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10, restore_best_weights=True) 
    history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps, callbacks=[tensorboard_callback, early_stopping_callback])
  else:
    history = model.fit(train_ds, epochs=epochs, verbose=2, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps)
  return(history)
  
# print test set evaluation metrics
def model_evaluation(model, evaluation_steps=10):
  print('\nTest set evaluation metrics')
  #loss0,accuracy0,precision0,recall0 = model.evaluate(test_ds, steps = evaluation_steps)
  loss0,accuracy0 = model.evaluate(test_ds, steps = evaluation_steps)
  print("Loss: {:.2f}".format(loss0))
  print("Accuracy: {:.2f}".format(accuracy0))
  #print("Precision: {:.2f}".format(precision0))
  #print("Recall: {:.2f}".format(recall0))

def model_report(model, history, evaluation_steps = 10):
	plt = summarize_diagnostics(history)
	plt.show()
	model_evaluation(model, evaluation_steps)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.keras.optimizers.Nadam( learning_rate=0.001 ))
# first_model_history = train_model(first_model,"first_model_1",True, 200, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

"""## Μελέτη Batch Size"""

BATCH_SIZE =32
train_ds =_input_fn(x_train,y_train, BATCH_SIZE) #PrefetchDataset object
validation_ds =_input_fn(x_val,y_val, BATCH_SIZE) #PrefetchDataset object
test_ds =_input_fn(x_test,y_test, BATCH_SIZE) #PrefetchDataset object


def train_model(model,name,early_stopping, epochs = 10, steps_per_epoch = 2, verbose=2 , validation_steps = 1):
  
  dir = os.path.join('/content/drive/My Drive/Neural_Networks/lab3/models', name)
  
  if (early_stopping==1):
    tensorboard_callback = tf.keras.callbacks.TensorBoard(dir, histogram_freq=1)
    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10, restore_best_weights=True) 
    history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps, callbacks=[tensorboard_callback, early_stopping_callback])
  else:
    history = model.fit(train_ds, epochs=epochs, verbose=2, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps)
  return(history)
  
# print test set evaluation metrics
def model_evaluation(model, evaluation_steps=10):
  print('\nTest set evaluation metrics')
  #loss0,accuracy0,precision0,recall0 = model.evaluate(test_ds, steps = evaluation_steps)
  loss0,accuracy0 = model.evaluate(test_ds, steps = evaluation_steps)
  print("Loss: {:.2f}".format(loss0))
  print("Accuracy: {:.2f}".format(accuracy0))
  #print("Precision: {:.2f}".format(precision0))
  #print("Recall: {:.2f}".format(recall0))

def model_report(model, history, evaluation_steps = 10):
	plt = summarize_diagnostics(history)
	plt.show()
	model_evaluation(model, evaluation_steps)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.optimizers.Adam(learning_rate=0.0001))
# first_model_history = train_model(first_model,"first_model_1",True, 200, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

BATCH_SIZE = 64
train_ds =_input_fn(x_train,y_train, BATCH_SIZE) #PrefetchDataset object
validation_ds =_input_fn(x_val,y_val, BATCH_SIZE) #PrefetchDataset object
test_ds =_input_fn(x_test,y_test, BATCH_SIZE) #PrefetchDataset object


def train_model(model,name,early_stopping, epochs = 10, steps_per_epoch = 2, verbose=2 , validation_steps = 1):
  
  dir = os.path.join('/content/drive/My Drive/Neural_Networks/lab3/models', name)
  
  if (early_stopping==1):
    tensorboard_callback = tf.keras.callbacks.TensorBoard(dir, histogram_freq=1)
    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10, restore_best_weights=True) 
    history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps, callbacks=[tensorboard_callback, early_stopping_callback])
  else:
    history = model.fit(train_ds, epochs=epochs, verbose=2, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps)
  return(history)

# print test set evaluation metrics
def model_evaluation(model, evaluation_steps=10):
  print('\nTest set evaluation metrics')
  loss0,accuracy0 = model.evaluate(test_ds, steps = evaluation_steps)
  print("Loss: {:.2f}".format(loss0))
  print("Accuracy: {:.2f}".format(accuracy0))

def model_report(model, history, evaluation_steps = 10):
	plt = summarize_diagnostics(history)
	plt.show()
	model_evaluation(model, evaluation_steps)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.optimizers.Adam(learning_rate=0.0001))
# first_model_history = train_model(first_model,"first_model_1",True, 200, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

BATCH_SIZE = 128
train_ds =_input_fn(x_train,y_train, BATCH_SIZE) #PrefetchDataset object
validation_ds =_input_fn(x_val,y_val, BATCH_SIZE) #PrefetchDataset object
test_ds =_input_fn(x_test,y_test, BATCH_SIZE) #PrefetchDataset object


def train_model(model,name,early_stopping, epochs = 10, steps_per_epoch = 2, verbose=2 , validation_steps = 1):
  
  dir = os.path.join('/content/drive/My Drive/Neural_Networks/lab3/models', name)
  
  if (early_stopping==1):
    tensorboard_callback = tf.keras.callbacks.TensorBoard(dir, histogram_freq=1)
    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10, restore_best_weights=True) 
    history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps, callbacks=[tensorboard_callback, early_stopping_callback])
  else:
    history = model.fit(train_ds, epochs=epochs, verbose=2, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps)
  return(history)

# print test set evaluation metrics
def model_evaluation(model, evaluation_steps=10):
  print('\nTest set evaluation metrics')
  loss0,accuracy0 = model.evaluate(test_ds, steps = evaluation_steps)
  print("Loss: {:.2f}".format(loss0))
  print("Accuracy: {:.2f}".format(accuracy0))

def model_report(model, history, evaluation_steps = 10):
	plt = summarize_diagnostics(history)
	plt.show()
	model_evaluation(model, evaluation_steps)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.optimizers.Adam(learning_rate=0.0001))
# first_model_history = train_model(first_model,"first_model_1",True, 200, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

BATCH_SIZE = 256
train_ds =_input_fn(x_train,y_train, BATCH_SIZE) #PrefetchDataset object
validation_ds =_input_fn(x_val,y_val, BATCH_SIZE) #PrefetchDataset object
test_ds =_input_fn(x_test,y_test, BATCH_SIZE) #PrefetchDataset object


def train_model(model,name,early_stopping, epochs = 10, steps_per_epoch = 2, verbose=2 , validation_steps = 1):
  
  dir = os.path.join('/content/drive/My Drive/Neural_Networks/lab3/models', name)
  
  if (early_stopping==1):
    tensorboard_callback = tf.keras.callbacks.TensorBoard(dir, histogram_freq=1)
    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10, restore_best_weights=True) 
    history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps, callbacks=[tensorboard_callback, early_stopping_callback])
  else:
    history = model.fit(train_ds, epochs=epochs, verbose=2, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps)
  return(history)

# print test set evaluation metrics
def model_evaluation(model, evaluation_steps=10):
  print('\nTest set evaluation metrics')
  loss0,accuracy0 = model.evaluate(test_ds, steps = evaluation_steps)
  print("Loss: {:.2f}".format(loss0))
  print("Accuracy: {:.2f}".format(accuracy0))

def model_report(model, history, evaluation_steps = 10):
	plt = summarize_diagnostics(history)
	plt.show()
	model_evaluation(model, evaluation_steps)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.optimizers.Adam(learning_rate=0.0001))
# first_model_history = train_model(first_model,"first_model_1",True, 200, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

"""## Μελέτη learning rate"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.optimizers.Adam(learning_rate=0.01))
# first_model_history = train_model(first_model,"first_model_1",True, 200, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.optimizers.Adam(learning_rate=0.001))
# first_model_history = train_model(first_model,"first_model_1",True, 200, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.optimizers.Adam(learning_rate=0.0001))
# first_model_history = train_model(first_model,"first_model_1",True, 200, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.optimizers.Adam(learning_rate=0.00001))
# first_model_history = train_model(first_model,"first_model_1",True, 200, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

"""## Μελέτη optimizer"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.keras.optimizers.SGD( learning_rate=0.001 ))
# first_model_history = train_model(first_model,"first_model_1",True, 200, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.keras.optimizers.Adadelta( learning_rate=0.001))
# first_model_history = train_model(first_model,"first_model_1",True, 200, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.keras.optimizers.Adagrad( learning_rate=0.001 ))
# first_model_history = train_model(first_model,"first_model_1",True, 200, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.keras.optimizers.Nadam( learning_rate=0.001 ))
# first_model_history = train_model(first_model,"first_model_1",True, 200, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

"""## epochs"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.keras.optimizers.Nadam( learning_rate=0.001 ))
# first_model_history = train_model(first_model,"first_model_1",0, 50, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.keras.optimizers.Nadam( learning_rate=0.001 ))
# first_model_history = train_model(first_model,"first_model_1",0, 100, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.keras.optimizers.Nadam( learning_rate=0.001 ))
# first_model_history = train_model(first_model,"first_model_1",0, 150, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.keras.optimizers.Nadam( learning_rate=0.001 ))
# first_model_history = train_model(first_model,"first_model_1",0, 200, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.keras.optimizers.Nadam( learning_rate=0.001 ))
# first_model_history = train_model(first_model,"first_model_1",1, 150, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

"""## data broken"""

BATCH_SIZE = 128

def random_crop(img, random_crop_size):
    height, width = img.shape[0], img.shape[1]
    dy, dx = random_crop_size
    x = np.random.randint(0, width - dx + 1)
    y = np.random.randint(0, height - dy + 1)
    return img[y:(y+dy), x:(x+dx),:]

def split_input_batches(x,y, BATCH_SIZE):
  ds = tf.data.Dataset.from_tensor_slices((x,y))
  ds = ds.shuffle(buffer_size = data_size)
  ds = ds.batch(BATCH_SIZE)
  ds = ds.repeat()
  ds = ds.prefetch(buffer_size = AUTOTUNE)
  return ds

def crop_generator(batches, crop_length, num_channel = 3):
    while True:
        batch_x, batch_y = next(batches)
        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, num_channel))
        for i in range(batch_x.shape[0]):
            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))
        yield (batch_crops, batch_y)

TRAINING_DATAGEN =  tf.keras.preprocessing.image.ImageDataGenerator(
    
    width_shift_range = 0.001,
    height_shift_range = 0.001,
    zoom_range = 0.001,
    horizontal_flip = False)

VALIDATION_DATAGEN =  tf.keras.preprocessing.image.ImageDataGenerator()

def resize_and_augment(pretrained_model, training_datagen = '', validation_datagen = ''):
  if pretrained_model == "Xception" :
    X_train_used, X_val_used, X_test_used = tf.image.resize_images(X_train, (71, 71)), tf.image.resize_images(X_val, (71, 71)), tf.image.resize_images(X_test, (71, 71))
    with tf.Session() as sess:
      X_train_used = x_train_used.eval()
      X_val_used   = x_val_used.eval()
      X_test_used  = x_test_used.eval()
  else:
    X_train_used, X_val_used, X_test_used = x_train, x_val, x_test
  
  if training_datagen != "":
    train_tf = training_datagen.flow(X_train_used, map_label(y_train), batch_size = BATCH_SIZE)
    validation_tf = validation_datagen.flow(X_val_used, map_label(y_val), batch_size = BATCH_SIZE)  
  else:
    train_tf = split_input_batches(X_train_used, map_label(y_train), BATCH_SIZE)
    validation_tf = split_input_batches(X_val_used, map_label(y_val), BATCH_SIZE)  

  test_tf = split_input_batches(X_test_used,map_label(y_test), BATCH_SIZE) 

  return train_tf, validation_tf, test_tf

train_ds,validation_ds,test_ds = resize_and_augment("lalal",TRAINING_DATAGEN,VALIDATION_DATAGEN )

def train_model(model,name,early_stopping, epochs = 10, steps_per_epoch = 2, verbose=2 , validation_steps = 1):
  
  dir = os.path.join('/content/drive/My Drive/Neural_Networks/lab3/models', name)
  
  if (early_stopping==1):
    tensorboard_callback = tf.keras.callbacks.TensorBoard(dir, histogram_freq=1)
    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10, restore_best_weights=True) 
    history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps, callbacks=[tensorboard_callback, early_stopping_callback])
  else:
    history = model.fit(train_ds, epochs=epochs, verbose=2, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps)
  return(history)
  
# print test set evaluation metrics
def model_evaluation(model, evaluation_steps=10):
  print('\nTest set evaluation metrics')
  #loss0,accuracy0,precision0,recall0 = model.evaluate(test_ds, steps = evaluation_steps)
  loss0,accuracy0 = model.evaluate(test_ds, steps = evaluation_steps)
  print("Loss: {:.2f}".format(loss0))
  print("Accuracy: {:.2f}".format(accuracy0))
  #print("Precision: {:.2f}".format(precision0))
  #print("Recall: {:.2f}".format(recall0))

def model_report(model, history, evaluation_steps = 10):
	plt = summarize_diagnostics(history)
	plt.show()
	model_evaluation(model, evaluation_steps)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# first_model = init_first_model(True,32,tf.keras.optimizers.Nadam( learning_rate=0.001 ))
# first_model_history = train_model(first_model,"first_model_1",0, 200, 40, 10)

model_report(first_model,first_model_history)
first_model.save('/content/drive/My Drive/Neural_Networks/lab3/models/first_model_32.h5')

"""# Μεταφορά μάθησης

#VGG16
"""

def init_VGG16_model(summary, trainable):
  VGG16_MODEL=tf.keras.applications.VGG16(input_shape=(32,32,3), include_top=False, weights='imagenet')

  if (trainable == "all"):
    VGG16_MODEL.trainable=True
  elif (trainable == "top"):
    VGG16_MODEL.trainable=False
  else:
    percentage = float(trainable)/100
    nlayers =  len(VGG16_MODEL.layers)
    nfreeze = ceil(nlayers*percentage)

    for layer in VGG16_MODEL.layers[:nfreeze]:
	    layer.trainable = False


  dropout_layer = tf.keras.layers.Dropout(rate = 0.5)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

  # add top layer for CIFAR100 classification
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  model = tf.keras.Sequential([VGG16_MODEL, dropout_layer, global_average_layer, prediction_layer])
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# VGG16_MODEL_all = init_VGG16_model(True, "all")
# VGG16_MODEL_all_history = train_model(VGG16_MODEL_all,"VGG16_all_1",True, 200, 40, 10)

model_report(VGG16_MODEL_all,VGG16_MODEL_all_history)
VGG16_MODEL_all.save('/content/drive/My Drive/Neural_Networks/lab3/models/VGG16_all_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# VGG16_MODEL_5 = init_VGG16_model(summary = True, trainable = "5")
# VGG16_MODEL_5_history = train_model(VGG16_MODEL_5,"VGG16_5%_1",True, 200, 40, 10)

model_report(VGG16_MODEL_5,VGG16_MODEL_5_history)
VGG16_MODEL_5.save('/content/drive/My Drive/Neural_Networks/lab3/models/VGG16_5%_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# VGG16_MODEL_20 = init_VGG16_model(summary = True, trainable = "20")
# VGG16_MODEL_20_history = train_model(VGG16_MODEL_20,"VGG16_20%_1",True, 200, 40, 10)

model_report(VGG16_MODEL_20,VGG16_MODEL_20_history)
VGG16_MODEL_20.save('/content/drive/My Drive/Neural_Networks/lab3/models/VGG16_20%_32.h5')

"""#VGG19"""

def init_VGG19_model(summary, trainable):
  VGG16_MODEL=tf.keras.applications.VGG19(input_shape=(32,32,3), include_top=False, weights='imagenet')

  if (trainable == "all"):
    # unfreeze conv layers
    VGG16_MODEL.trainable=True
  elif (trainable == "top"):
    VGG16_MODEL.trainable=False
  else:
    percentage = float(trainable)/100
    nlayers =  len(VGG16_MODEL.layers)
    nfreeze = ceil(nlayers*percentage)

    for layer in VGG16_MODEL.layers[:nfreeze]:
	    layer.trainable = False


  dropout_layer = tf.keras.layers.Dropout(rate = 0.5)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

  # add top layer for CIFAR100 classification
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  model = tf.keras.Sequential([VGG16_MODEL, dropout_layer, global_average_layer, prediction_layer])
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# VGG19_MODEL_all = init_VGG19_model(True, "all")
# VGG19_MODEL_all_history = train_model(VGG19_MODEL_all,"VGG19_all_1",True, 400, 40, 10)

model_report(VGG19_MODEL_all,VGG19_MODEL_all_history)
VGG19_MODEL_all.save('/content/drive/My Drive/Neural_Networks/lab3/models/VGG19_all_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# VGG19_MODEL_5 = init_VGG19_model(summary = True, trainable = "5")
# VGG19_MODEL_5_history = train_model(VGG19_MODEL_5,"VGG19_5%_1",True, 200, 40, 10)

model_report(VGG19_MODEL_5,VGG19_MODEL_5_history)
VGG19_MODEL_5.save('/content/drive/My Drive/Neural_Networks/lab3/models/VGG19_5%_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# VGG19_MODEL_20 = init_VGG19_model(summary = True, trainable = "20")
# VGG19_MODEL_20_history = train_model(VGG19_MODEL_20,"VGG19_20%_1",True, 200, 40, 10)

model_report(VGG19_MODEL_20,VGG19_MODEL_20_history)
VGG19_MODEL_20.save('/content/drive/My Drive/Neural_Networks/lab3/models/VGG19_20%_32.h5')

"""#DenseNet"""

def init_DenseNet_model(summary, trainable):
  ResNet50=tf.keras.applications.DenseNet121(input_shape=(32,32,3), include_top=False, weights='imagenet')

  if (trainable == "all"):
    # unfreeze conv layers
    ResNet50.trainable=True
  elif (trainable == "top"):
    ResNet50.trainable=False
  else:
    percentage = float(trainable)/100
    nlayers =  len(ResNet50.layers)
    nfreeze = ceil(nlayers*percentage)

    for layer in ResNet50.layers[:nfreeze]:
	    layer.trainable = False


  dropout_layer = tf.keras.layers.Dropout(rate = 0.5)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

  # add top layer for CIFAR100 classification
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  model = tf.keras.Sequential([ResNet50, dropout_layer, global_average_layer, prediction_layer])
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# DenseNet_MODEL_all = init_DenseNet_model(True, "all")
# DenseNet_MODEL_all_history = train_model(DenseNet_MODEL_all,"DenseNet_all_1",True, 400, 40, 10)

model_report(DenseNet_MODEL_all,DenseNet_MODEL_all_history)
DenseNet_MODEL_all.save('/content/drive/My Drive/Neural_Networks/lab3/models/DenseNet_all_32.h5')

"""# ResNet50"""

def init_ResNet50_model(summary, trainable):
  ResNet50=tf.keras.applications.ResNet50(input_shape=(32,32,3), include_top=False, weights='imagenet')

  if (trainable == "all"):
    # unfreeze conv layers
    ResNet50.trainable=True
  elif (trainable == "top"):
    ResNet50.trainable=False
  else:
    percentage = float(trainable)/100
    nlayers =  len(ResNet50.layers)
    nfreeze = ceil(nlayers*percentage)

    for layer in ResNet50.layers[:nfreeze]:
	    layer.trainable = False


  dropout_layer = tf.keras.layers.Dropout(rate = 0.5)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

  # add top layer for CIFAR100 classification
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  model = tf.keras.Sequential([ResNet50, dropout_layer, global_average_layer, prediction_layer])
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ResNet50_MODEL_all = init_ResNet50_model(True, "all")
# ResNet50_MODEL_all_history = train_model(ResNet50_MODEL_all,"ResNet50_all_1",True, 400, 40, 10)

model_report(ResNet50_MODEL_all,ResNet50_MODEL_all_history)
ResNet50_MODEL_all.save('/content/drive/My Drive/Neural_Networks/lab3/models/ResNet50_all_32.h5')

"""#Xception"""

train_ds = read_dataset(dir_74dim_img + "train_74dim")
validation_ds  = read_dataset(dir_74dim_img + "val_74dim")
test_ds  = read_dataset(dir_74dim_img + "test_74dim")

train_ds =_input_fn_for_bigger_images(train_ds, 128) 
validation_ds =_input_fn_for_bigger_images(validation_ds, 128)
test_ds =_input_fn_for_bigger_images(test_ds, 128)


def train_model(model,name,early_stopping, epochs = 10, steps_per_epoch = 2, verbose=2 , validation_steps = 1):
  
  dir = os.path.join('/content/drive/My Drive/Neural_Networks/lab3/models', name)
  
  if (early_stopping==1):
    tensorboard_callback = tf.keras.callbacks.TensorBoard(dir, histogram_freq=1)
    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10, restore_best_weights=True) 
    history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps, callbacks=[tensorboard_callback, early_stopping_callback])
  else:
    history = model.fit(train_ds, epochs=epochs, verbose=2, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps)
  return(history)
  
# print test set evaluation metrics
def model_evaluation(model, evaluation_steps=10):
  print('\nTest set evaluation metrics')
  #loss0,accuracy0,precision0,recall0 = model.evaluate(test_ds, steps = evaluation_steps)
  loss0,accuracy0 = model.evaluate(test_ds, steps = evaluation_steps)
  print("Loss: {:.2f}".format(loss0))
  print("Accuracy: {:.2f}".format(accuracy0))
  #print("Precision: {:.2f}".format(precision0))
  #print("Recall: {:.2f}".format(recall0))

def model_report(model, history, evaluation_steps = 10):
	plt = summarize_diagnostics(history)
	plt.show()
	model_evaluation(model, evaluation_steps)

def train_model(model,name,early_stopping, epochs = 10, steps_per_epoch = 2, verbose=2 , validation_steps = 1):
  
  dir = os.path.join('/content/drive/My Drive/Neural_Networks/lab3/models', name)
  
  if (early_stopping==1):
    tensorboard_callback = tf.keras.callbacks.TensorBoard(dir, histogram_freq=1)
    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10, restore_best_weights=True) 
    history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps, callbacks=[tensorboard_callback, early_stopping_callback])
  else:
    history = model.fit(train_ds, epochs=epochs, verbose=2, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps)
  return(history)

def init_Xception_model(summary, trainable):
  Xception=tf.keras.applications.Xception(input_shape=(74,74,3), include_top=False, weights='imagenet')

  if (trainable == "all"):
    # unfreeze conv layers
    Xception.trainable=True
  elif (trainable == "top"):
    Xception.trainable=False
  else:
    percentage = float(trainable)/100
    nlayers =  len(Xception.layers)
    nfreeze = ceil(nlayers*percentage)

    for layer in Xception.layers[:nfreeze]:
	    layer.trainable = False


  dropout_layer = tf.keras.layers.Dropout(rate = 0.5)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

  # add top layer for CIFAR100 classification
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  model = tf.keras.Sequential([Xception, dropout_layer, global_average_layer, prediction_layer])
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# Xception_MODEL_all = init_Xception_model(True, "all")
# Xception_MODEL_all_history = train_model(Xception_MODEL_all,"Xception_all_1",True, 400, 40, 10)

model_report(Xception_MODEL_all,Xception_MODEL_all_history)
Xception_MODEL_all.save('/content/drive/My Drive/Neural_Networks/lab3/models/Xception_all_32.h5')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# Xception_MODEL_all = init_Xception_model(True, "20")
# Xception_MODEL_all_history = train_model(Xception_MODEL_all,"Xception_20_1",True, 400, 40, 10)

model_report(Xception_MODEL_all,Xception_MODEL_all_history)
Xception_MODEL_all.save('/content/drive/My Drive/Neural_Networks/lab3/models/Xception_all_32.h5')

"""#Βελτιστοποίηση καλύτερου Μοντελου tl"""

cifar100_classes_url = select_classes_number(80)

team_classes = pd.read_csv(cifar100_classes_url, sep=',', header=None)
CIFAR100_LABELS_LIST = pd.read_csv('https://pastebin.com/raw/qgDaNggt', sep=',', header=None).astype(str).values.tolist()[0]

our_index = team_classes.iloc[team_seed,:].values.tolist()
our_classes = select_from_list(CIFAR100_LABELS_LIST, our_index)
train_index = get_ds_index(y_train_all, our_index)
test_index = get_ds_index(y_test_all, our_index)

x_train_ds = np.asarray(select_from_list(x_train_all, train_index))
y_train_ds = np.asarray(select_from_list(y_train_all, train_index))
x_test_ds = np.asarray(select_from_list(x_test_all, test_index))
y_test_ds = np.asarray(select_from_list(y_test_all, test_index))

print(our_classes)

# get (train) dataset dimensions
data_size, img_rows, img_cols, img_channels = x_train_ds.shape

# set validation set percentage (wrt the training set size)
validation_percentage = 0.15
val_size = round(validation_percentage * data_size)

# Reserve val_size samples for validation and normalize all values
x_val = x_train_ds[-val_size:]/255
y_val = y_train_ds[-val_size:]
x_train = x_train_ds[:-val_size]/255
y_train = y_train_ds[:-val_size]
x_test = x_test_ds/255
y_test = y_test_ds

print(len(x_val))

# summarize loaded dataset
print('Train: X=%s, y=%s' % (x_train.shape, y_train.shape))
print('Validation: X=%s, y=%s' % (x_val.shape, y_val.shape))
print('Test: X=%s, y=%s' % (x_test.shape, y_test.shape))

# get class label from class index
def class_label_from_index(fine_category):
  return(CIFAR100_LABELS_LIST[fine_category.item(0)])

# plot first few images
plt.figure(figsize=(6, 6))
for i in range(9):
	# define subplot
  plt.subplot(330 + 1 + i).set_title(class_label_from_index(y_train[i]))
	# plot raw pixel data
  plt.imshow(x_train[i], cmap=plt.get_cmap('gray'))
  #show the figure
plt.show()

BATCH_SIZE =128
train_ds =_input_fn(x_train,y_train, BATCH_SIZE) #PrefetchDataset object
validation_ds =_input_fn(x_val,y_val, BATCH_SIZE) #PrefetchDataset object
test_ds =_input_fn(x_test,y_test, BATCH_SIZE) #PrefetchDataset object


def train_model(model,name,early_stopping, epochs = 10, steps_per_epoch = 2, verbose=2 , validation_steps = 1):
  
  dir = os.path.join('/content/drive/My Drive/Neural_Networks/lab3/models', name)
  
  if (early_stopping==1):
    tensorboard_callback = tf.keras.callbacks.TensorBoard(dir, histogram_freq=1)
    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10, restore_best_weights=True) 
    history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps, callbacks=[tensorboard_callback, early_stopping_callback])
  else:
    history = model.fit(train_ds, epochs=epochs, verbose=2, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps)
  return(history)
  
# print test set evaluation metrics
def model_evaluation(model, evaluation_steps=10):
  print('\nTest set evaluation metrics')
  #loss0,accuracy0,precision0,recall0 = model.evaluate(test_ds, steps = evaluation_steps)
  loss0,accuracy0 = model.evaluate(test_ds, steps = evaluation_steps)
  print("Loss: {:.2f}".format(loss0))
  print("Accuracy: {:.2f}".format(accuracy0))
  #print("Precision: {:.2f}".format(precision0))
  #print("Recall: {:.2f}".format(recall0))

def model_report(model, history, evaluation_steps = 10):
	plt = summarize_diagnostics(history)
	plt.show()
	model_evaluation(model, evaluation_steps)

def init_VGG19_model(summary, trainable):
  VGG16_MODEL=tf.keras.applications.VGG19(input_shape=(32,32,3), include_top=False, weights='imagenet')

  if (trainable == "all"):
    # unfreeze conv layers
    VGG16_MODEL.trainable=True
  elif (trainable == "top"):
    VGG16_MODEL.trainable=False
  else:
    percentage = float(trainable)/100
    nlayers =  len(VGG16_MODEL.layers)
    nfreeze = ceil(nlayers*percentage)

    for layer in VGG16_MODEL.layers[:nfreeze]:
	    layer.trainable = False


  dropout_layer = tf.keras.layers.Dropout(rate = 0.5)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

  # add top layer for CIFAR100 classification
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  model = tf.keras.Sequential([VGG16_MODEL, dropout_layer, global_average_layer, prediction_layer])
  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# VGG19_MODEL_all = init_VGG19_model(True, "all")
# VGG19_MODEL_all_history = train_model(VGG19_MODEL_all,"VGG19_all_1",True, 400, 40, 10)

model_report(VGG19_MODEL_all,VGG19_MODEL_all_history)
VGG19_MODEL_all.save('/content/drive/My Drive/Neural_Networks/lab3/models/VGG19_all_32.h5')

def init_Xception_model(summary, trainable):
  Xception=tf.keras.applications.Xception(input_shape=(74,74,3), include_top=False, weights='imagenet')

  if (trainable == "all"):
    # unfreeze conv layers
    Xception.trainable=True
  elif (trainable == "top"):
    Xception.trainable=False
  else:
    percentage = float(trainable)/100
    nlayers =  len(Xception.layers)
    nfreeze = ceil(nlayers*percentage)

    for layer in Xception.layers[:nfreeze]:
	    layer.trainable = False


  dropout_layer = tf.keras.layers.Dropout(rate = 0.5)
  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

  # add top layer for CIFAR100 classification
  prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')
  model = tf.keras.Sequential([Xception, dropout_layer, global_average_layer, prediction_layer])
  model.compile(optimizer=tf.keras.optimizers.Nadam( learning_rate=0.001 ), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
  if summary: 
    model.summary()
  return model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# Xception_MODEL_all = init_Xception_model(True, "all")
# Xception_MODEL_all_history = train_model(Xception_MODEL_all,"Xception_all_1",True, 400, 40, 10)

model_report(Xception_MODEL_all,Xception_MODEL_all_history)
Xception_MODEL_all.save('/content/drive/My Drive/Neural_Networks/lab3/models/Xception_all_32.h5')