# -*- coding: utf-8 -*-
"""nn2020_lab1_team_80.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AeEy52LZe7mhCFJ8SqiumJfTJMbnNp5d

#Ομάδα 80



*   Αχλάτης Στέφανος Σταμάτιος (03116149)
*   Ηλιακοπούλου Νικολέτα Μαρκέλα (03116111)
*   Σταυροπούλου Γεωργία (03116162)

#Mount Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""#Imports"""

!pip install -U imbalanced-learn

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt 
import random
from sklearn.base import BaseEstimator, ClassifierMixin
import random
from sklearn.decomposition import PCA
from google.colab import files
from sklearn.model_selection import learning_curve
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import validation_curve
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import GaussianNB
import sklearn.model_selection
from sklearn.ensemble import VotingClassifier,BaggingClassifier
from sklearn.base import BaseEstimator, ClassifierMixin
from scipy.stats import multivariate_normal
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.dummy import DummyClassifier
from sklearn.metrics import confusion_matrix
import itertools
from yellowbrick.classifier import ClassificationReport
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import precision_recall_fscore_support
from sklearn.model_selection import GridSearchCV
from sklearn import preprocessing
from sklearn.neural_network import MLPClassifier
from sklearn.svm import LinearSVC
from imblearn.pipeline import Pipeline

"""#Μέρος 1ο

##Πληροφορίες Ionosphere DataSet

>Tο εν λόγω data set δημιουργήκε από μετρήσεις που συλλέχθηκαν από ένα σύστημα στο Goose Bay, Labrador του Καναδά. Το σύστημα αυτό αποτελείται από ένα phased array 16 κεραιών υψηλής συχνότητας, με συνολική εκπεμπόμενη ισχύ της τάξης των 6.4 kW. Οι στόχοι της εκπομπής ήταν ελεύθερα ηλεκτρόνια στην ιονόσφαιρα. "Καλές"("good") αποκρίσεις θεωρούνται αυτές που υποδεικνύουν ύπαρξη κάποιου είδους δομής στην ιονόσφαιρα, ενώ "κακές", θεωρούνται αυτές που δεν φανερώνουν κάποια δομή.

>Τα λαμβανόμενα σήματα επεξεργάστηκαν με τη χρήση μιας συνάρτησης αυτοσυσχέτισης της οποίας τα ορίσματα είναι ο χρόνος του παλμού και ο αριθμός του (συνολικά βρέθηκαν 17 αριθμοί). Τα στιγμιότυπα (δείγματα) αυτού του συνόλου δεδομένων περιγράφονται από 2 ορίσματα ανά αριθμό παλμού που αντιστοιχούν στις μιγαδικές τιμές που επιστρέφονται από τη συνάρτηση, ως αποτέλεσμα του μιγαδικού ηλεκτρομαγνητικού σήματος.(**ερώτημα 1**)

>Ο αριθμός των δειγμάτων είναι 351 και κάθε δείγμα περιγράφεται από 34 συνεχή χαρακτηριστικά(17 παλμοί που περιγράφονται από 2 ορίσματα ο καθένας) συν την κλάση στην οποία ανήκει το δείγμα(good(g)/bad(b)). Απουσιάζουσες τιμές αναφέρεται ότι δεν υπάρχουν.

>*Τα παραπάνω στοιχεία αντλήθηκαν από το ionosphere.names αρχείο το οποίο περιέχει πληροφορίες του data set*

#Βήμα 1: Επεξεργασία δεδομένων

>Θα εισάγουμε τα δεδομένα σε dataframes απευθείας από τα αρχεία που βρίσκονται αποθηκευμένα στο drive μας.
"""

df1 = pd.read_csv("/content/drive/My Drive/Neural_Networks/lab1/small/ionosphere.data", sep=" ", header=None)

"""> Στην συνέχεια συνεχίζουμε με μια γενική επσικόπηση του training set για να δούμε την μορφή των πληροφοριών που περιέχει"""

df1

"""> Επομένως βλέπουμε ότι τα δεδομένα μας δεν είναι χωρισμένα σε κολόνες, όπως θα περιμέναμε, για να γίνεται πιο κατανοητή η αναπαράσταση του διανύσματος, οπότε θα κάνουμε κατάλληλη μετατροπή των δεδομένων διαβάζοντάς τα σε μορφή csv και διαχωρίζοντας τα χαρακτηριστικά σε στήλες (**ερώτημα 5**). Επίσης, τόσο από την παραπάνω μορφή, όσο και από την απευθείας παρατήρηση των δεδομένων από το άνοιγμα του αρχείου ionosphere.data στον υπολογιστή μας, παρατηρούμε ότι δεν υπάρχει αρίθμηση γραμμών αλλά ούτε και επικεφαλίδες στα δείγματα (**ερώτημα 3**).

"""

mylist=[]
for i in range(0,351):
  mylist.append(df1.values.tolist()[i][0].split(','))

labels = []
for i in range(0,34):
  labels.append("Feature_"+str(i))
labels.append("Label")

df = pd.DataFrame(mylist, columns = labels)
df

""">Έτσι παρατηρούμε ότι τα features είναι συνεχή αριθμητικά και τα labels των class που ανήκουν τα δεδομένα είναι μη διατεταγμένα(αν θεωρήσουμε το είδος της κλάσης ως χαρακτηριστικό). Με την προηγούμενη ανάλυση βλέπουμε ότι έχουμε 351 δείγματα και οι κολώνες 0-33 περιέχουν τα features (34 features)(**ερώτημα 2**) ενώ η τελευταία κολόνα δηλαδή η 34 περιέχει το όνομα της κλάσης στην οποία ανήκει το κάθε δεδομένο(έχουμε 2 κλάσεις: g για τα good και b για τα bad)(**ερώτημα 4**).<br/>
Παρατηρούμε ότι το προηγούμετο dataframe έχει όλα τα features σε floats τιμές και τα ονόματα των κλάσεων σε string τιμές. Αξίζει να ειπωθεί ότι για τεχνικούς λόγους, λόγω της καθορισμένης διαδικασίας διαβάσματος, τα δεδομένα στο προηγούμενο dataframe είναι τυπωμένα ως strings.
"""

df.isna().any().any()

""">Από το παραπάνω output παρατηρούμε ότι δεν υπάρχουν απουσιάζουσες τιμές, όπως άλλωστε αναφερόταν και στις πληροφορίες του data set (**ερώτημα 6**).

## Στατιστική μελέτη Δεδομένων

### Δεδομένα y
"""

class_list=[]
for i in range(0,351):
  class_list.append(df1.values.tolist()[i][0].split(',')[34])
  #etsi kratame mono ta labels

y_df = pd.DataFrame(class_list,columns = ["Class"])
y_df

num_of_goods, num_of_bads = y_df["Class"].value_counts()

print("Percentage of data labeled as good: {:.3f}".format(100*num_of_goods/351))
print("Percentage of data labeled as good: {:.3f}".format(100*num_of_bads/351))

"""> Από την παραπάνω ανάλυση βλέπουμε ότι ο αριθμός των κλάσεων είναι 2 όπως προαναφέραμε (binary dataset). Το ποσοστό των "καλών" δειγμάτων είναι 64.103% και των "κακών" είναι 35.897%. Θεωρώντας ως μη ισορροπημένο ένα dataset στο οποίο η μια κλάση είναι 1.5 φορά πιο συχνή από την άλλη (δηλαδή 60-40 αναλογία) προκύπτει ότι το εν λόγω data set είναι μη ισορροπημένο. Παρακάτω οπτικοποιούμε τον αριθμό των δειγμάτων ανά κλάση και το ποσοστό των δειγμάτων που αντιστοιχούν σε κάθε μία από αυτές(**ερώτημα 7**).


"""

priors = [num_of_goods, num_of_bads]

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
x = ['good', 'bad']
plt.bar(x, priors)
plt.xticks(x,x)
plt.title('Data classified in each group')
plt.xlabel('Class')
plt.ylabel('Number')
plt.grid()
plt.show()

priors = [100*num_of_goods/351, 100*num_of_bads/351]

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
x = ['good', 'bad']
plt.bar(x, priors)
plt.xticks(x,x)
plt.title('Proportion of data classified in each group')
plt.xlabel('Class')
plt.ylabel('Proportion')
plt.grid()
plt.show()

"""###Δεδομένα Χ"""

feature_list=[]
for i in range(0,351):
  feature_list.append([ float(x) for x in df1.values.tolist()[i][0].split(',') if x!='g' and x!='b'])

feature_names_list = []
for i in range(0,34):
  feature_names_list .append("Feature_"+str(i))

X_df = pd.DataFrame(feature_list,columns = feature_names_list)
X_df

"""Παρακάτω κάνουμε στατιστική ανάλυση των features μέσω της describe() μεθόδου, έχοντας πρώτα αποκλείσει την κλάση από την αναπαράστασή τους."""

X_df.describe()

""">Επομένως παρατηρούμε ότι το feature_1 έχει μέση τιμή και διασπορά ίση με το 0, δηλαδή όλα τα δεδομένα του είναι ίσα με το 0. Επομένως μπορούμε να το απομακρύνουμε καθώς δεν προσθέτει καμία πληροφορία.<br/>
Σε γενικές γραμμές αν η διακύμανση ενός χαρακτηριστικού εισόδου είναι πολύ χαμηλή, δεν μπορεί να προσφέρει σημαντικά στη διαχωριστική ικανότητα του ταξινομητή. Ειδικά στην περίπτωση που η διακύμανση είναι 0, δηλαδή το χαρακτηριστικό έχει σταθερή τιμή για όλα τα δείγματα εκπαίδευσης, δεν χρησιμεύει καθόλου στον ταξινομητή για να αποφασίσει αν ένα δείγμα ανήκει σε μία κλάση ή σε μια άλλη και επιπλέον μπορεί να δυσκολέψει άλλες διαδικασίες της προεπεξεργασίας όπως η κανονικοποίηση των χαρακτηριστικών.<br/>
Γενικά η μεγάλη διαστατικότητα δυσκολεύει τους αλγορίθμους μηχανικής μάθησης, (curse of dimentionality) επομένως τέτοιες "απομακρύνσεις" είναι αρκετά βοηθητικές για το σύστημα.
"""

X_df.drop(labels="Feature_1",axis=1,inplace=True)
X_df

""">Και στην συνέχεια αλλάζουμε τα ονόματα των κολόνων με ένα λεξικό το οποίο αντιστοιχίζει την παλιά ονομασία της κολόνας με την καινούρια"""

new_lables_dic = dict()
new_lables_dic.update({"Feature_0": "Feature_0"})
for i in range(2,34):
  old_name = "Feature_"+str(i)
  new_name = "Feature_"+str(i-1)
  new_lables_dic.update({old_name: new_name})

X_df.rename(new_lables_dic,axis=1,inplace=True)
X_df

""">Παρατηρούμε, επίσης, από το min και max των δεδομένων μας, ότι τα δεδομένα είναι ***κανονικοποιημένα*** επομένως δεν χρειάζεται να κάνουμε κάποια κανονικοποίηση.
Γενικά χαρακτηριστικά με πολύ μεγάλες διαφορές στις απόλυτες τιμές τους μπορούν να προκαλέσουν προβλήματα στην εκπαίδευση και να δώσουν ταξινομητές με μη βέλτιστη απόδοση. Για παράδειγμα, ένα χαρακτηριστικό με πολύ μεγάλες τιμές θα έχει μεγαλύτερη επίδραση στον υπολογισμό της απόστασης στον kNN από ότι ένα με μικρές τιμές, χωρίς αυτό να σημαίνει απαραίτητα ότι είναι περισσότερο καθοριστικό για το διαχωρισμό των κλάσεων.

### split

Στην συνέχεια μετατρέπουμε τα δεδομένα σε μορφή συμβατή ως προς την scikit learn δηλαδή σε τύπο: <class 'numpy.ndarray'>
"""

X = X_df.values
y = y_df.values.ravel()

"""Και στην συνέχεια διαχωρίζουμε training και testing (**ερώτημα 8**) δεδομένα σε ποσοστό 80%-20%, όπως μας ζητείται, άρα έχουμε 280 δείγματα στο train set και 71 στο test set."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=17)

print("Training feature size {} with type {} and training labels size {} with type {} ".format(X_train.shape, type(X_train), y_train.shape,type(y_train)))

print("Testing feature size {} with type {} and testing labels size {} with type {} ".format(X_test.shape, type(X_test), y_test.shape,type(y_test)))

""">Τώρα για να έχουμε μία καλύτερη εικόνα των δεδομένων μας θα τα τυπώσουμε σε ένα 2D επίπεδο. Έτσι θα χρειαστεί να κάνουμε μείωση διαστατικότητας από 33 σε 2. Αυτή η διαδικασία θα γίνει με PC Analysis και θα μας δώσει μια εικόνα για την γεωμετρική κατανομή των δεδομένων.<br/>
Αξίζει να σημειωθεί ότι κανουμε projection  μόνο τα Training δεδομένα, και από δώ και πέρα για να μην εισάγουμε bias στα μοντέλα μας θα αγνοούμε την ύπαρξη των testing δεδομένων. Τα testing δεδομένα θα λαμβάνονται υπόψην μόνο για το evaluation.
"""

pca = PCA(n_components=2)
pca_test  = pca.fit_transform(X_train)
print("The pca shape is: {}".format(pca_test.shape))
print()
print("The first component contains {:.2f}% of the total information and the second one contains {:.2f}%".format( (100*pca.explained_variance_ratio_[0]),(100*pca.explained_variance_ratio_[1]) ) )
print("In total the 2 components contains {:.2f}% of the total information".format((100*pca.explained_variance_ratio_[0])+(100*pca.explained_variance_ratio_[1])) )
print()

fig = plt.figure(figsize = (5,5))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel("PCA componenct 1")
ax.set_ylabel("PCA componenct 2")
ax.set_title("PCA Analysis")

classes = ['g','b']
colors = ['pink', 'blue']

for i, color in zip(classes, colors):
  index = np.where(y_train == i)
  first_component = []
  second_component = []
  for i in index[0]:
    first_component.append(pca_test [i][0])
    second_component.append(pca_test [i][1])
  ax.scatter(first_component, second_component, c = color, s = 50,alpha=0.7)
ax.legend(classes)
ax.grid()
fig.savefig('part13b.png')

"""Μπορούμε να δούμε ότι το PCA μας κρατάει συνολικά περίπου το 44% της συνολικής πληροφορίας και η υπόλοιπη "χανεται" λόγο της μείωσης διαστατικότητας.
Έτσι, μπορούμε να παρατηρήσουμε ότι τα δεδομένα που έχουν κατηγοριοποίηση στην κλάση 'bad' βρίσκονται κυρίως στη μέση.<br/>
Έτσι με αυτή την πολύ απλή ιδέα θα μπορούσαμε να φτιάξουμε έναν πολύ naive classifier ο οποίος θα θεωρεί τα δεδομένα που είναι κοντά στο κέντρο ότι ανήκουν στην κλάση bad ενώ τα υπόλοιπα στην κλάση good. 
Προφανώς ένα τόσο απλοικό μοντέλο δεν θα λειτουγούσε αλλά δείχνει μια απλή προσέγγιση machine learning

Με τον όρο **μη ισορροπημένο** dataset εννοούμε ένα dataset στο οποίο τα πλήθη των δειγμάτων της κάθε κλάσης διαφέρουν σημαντικά μεταξύ τους. Χωρίς να υπάρχει κάποια συνολική απάντηση, όταν ο λόγος μεταξύ του αριθμού των δειγμάτων δύο κλάσεων αρχίζει να είναι μεγαλύτερος από 2:3, μπορούμε να αρχίζουμε να θεωρούμε το dataset μη ισορροπημένο (imbalanced). Στα πραγματικά datasets αυτό είναι κάτι πολύ κοινό. Οι περισσότεροι ταξινομητές ωστόσο εκπαιδεύονται καλύτερα όταν τα δείγματα όλων των κλάσεων είναι σχετικά ισάριθμα.</br>


Έχουμε δύο βασικούς τρόπους για να εξισσοροπούμε ένα dataset, την υποδειγματοληψία (undersampling) και την υπερδειγματοληψία (oversampling). Εν ολίγοις, στο undersampling απλά αφαιρούμε τυχαία δείγματα από όλες τις κατηγορίες που έχουν μεγαλύτερο πλήθος από τη μικρότερη, ενώ στο oversampling επιλέγουμε τυχαία ορισμένα παραδείγματα από τις λιγότερο συχνές κατηγορίες και τα επαναλαμβάνουμε. Στην πρώτη δηλαδή αφαιρούμε δεδομένα ενώ στην άλλη προσθέτουμε. 

Γενικά το oversampling ενδείκνυται περισσότερο, αφού δεν χάνουμε δεδομένα εκπαίδευσης. Επίσης, σε κάποιους αλγορίθμους όπως πχ. random forests, έχει παρατηρηθεί ότι τα αποτελέσματα βελτιώνονται ακόμα και με oversampling με παράγοντες άνω του 2, δηλαδή αντιγράφοντας τα ίδια δεδομένα μπορεί να βοηθάμε τη σύγκλιση. Τα προηγούμενα βέβαια δεν παρατηρούνται σε όλες τις περιπτώσεις.

Ο λόγος αυτός για το dataset μας είναι ο εξής:
"""

num_of_good= len(np.where(y_train == 'g')[0])
num_of_bad= len(np.where(y_train == 'b')[0])

print("Num of goods in training set = {}".format(num_of_good))
print("Num of bads in training set = {}".format(num_of_bad))
print("Num_of_bad/Num_of_good = {}".format(num_of_bad/num_of_good))

""">Προσοχή! Την ενέργεια αυτή την επιτελούμε μόνο στα δεδομένα εκπαίδευσης και όχι στα δεδομένα testing, για να έχουμε ένα αντικειμενικό μέτρο της επίδοσης του μοντέλου μας.

Έχοντας αναλύσει τα ερωτήματα και παραπάνω, τα συγκεντρώνουμε περιληπτικά εδώ:


1.   Οι πληροφορίες για το data set δίνονται στην παράγραφο **Πληροφορίες Ionoshere DataSet**
2.  Το πλήθος των δειγμάτων είναι 351 και ο αριθμός των χαρακτηριστικών 34. Είναι συνεχή αριθμητικά χαρακτηριστικά και το μόνο μη διατεταγμένο στοιχείο είναι ο τύπος της κλάσης.
3.  Δεν υπάρχει ούτε επικεφαλίδα, ούτε αρίθμηση γραμμών στα χαρακτηριστικά.
4.  Οι ετικέτες των κλάσεων είναι b και g (για τα bad και good δεδομένα) και βρίσκονται στην 35 κολόνα(το index της, έχοντας αρίθμηση από το 0, είναι το 34).
5.  Διαβάσαμε το αρχείο σε μορφή csv, χωρίσαμε τα χαρακτηριστικά σε κολόνες, ονοματοδοτώντας την κάθε κολόνα που αντιστοιχεί σε κάθε χαρακτηριστικό.
6.  Από την παραπάνω ανάλυση παρατηρήσαμε ότι δεν υπάρχουν απουσιάζουσες τιμές στο συγκεκριμένο data set
7.  Ο αριμός των κλάσεων είναι 2 και το ποσοτό των good είναι 64.103 και των bad 35.897.Θεωρώντας ως μη ισορροπημένο ένα dataset στο οποίο η μια κλάση είναι 1.5 φορά πιο συχνή από την άλλη (δηλαδή 60-40 αναλογία) προκύπτει ότι το εν λόγω data set είναι μη ισορροπημένο.
8.  Στο κομμάτι **split**, χωρίσαμε το data set σε training και testing δεδομένα, με αναλογία 80-20%, δηλαδή 280 δεδομένα για training και 71 για testing.

#Βημα 2:Ταξινόμηση

Στο βήμα αυτό, θα εξετάσουμε τους ταξιμομητές dummy(uniform, constant, stratified, most_frequent), Gaussian Naive Bayes και KNN και θα αξιολογήσουμε την απόδοσή τους με τη βοήθεια των μετρικών f1_micro και f1_macro, με απλή αρχικοποίηση, χωρίς να κάνουμε βελτιστοποίηση.

>**Confusion Matrix**<br> H βάση για τις μετρικές απόδοσης των ταξινομητών είναι ο πίνακας σύγχυσης (confusion matrix) και δείχνει αν ο εκάστοτε ταξινομητής "συγχέει" τις κλάσεις του dataset(εδώ ο πίνακας αυτός είναι τετραγωνικός αφού έχουμε binary classification). Κάθε στήλη του πίνακα σύγχυσης αναπαριστά τα δείγματα που προβλέφθηκε ότι ανήκουν στη συγκεκριμένη κλάση, ενώ κάθε γραμμή, αυτά που όντως ανήκουν στην κλάση αυτή. Επομένως, το στοιχείο $C_{i,j}$ είναι ίσο με τα δείγματα που ανήκουν στην κατηγορία i, ενώ προβλέφθηκε ότι ανήκουν στην κατηγορία j. Αν το $i \neq j$, τα πλήθος αυτών των στοιχείων έχει ταξινομηθεί λανθασμένα, ενώ αν $i = j$ (κύρια διαγώνιος) τα δείγματα αυτά έχουν ταξινομηθεί σωστά (ταξινομήθηκαν στην κλάση i και όντως ανήκουν σε αυτή). <i>

>Αξίζει να αναφέρουμε ότι ο ορισμός του confusion matrix της wikipedia με αυτόν του scikit-learn δεν συμπίπτει, γεγονός που καθιστά εύκολη την πραγματοποίηση σοβαρού λάθους<i/>.

>Η παρακάτω συνάρτηση είναι υπεύθυνη για την γραφική απεικόνισή του και οι ταξινομητές που εξετάζουμε την καλούν για να αξιολογήσουν το κατά πόσο επετεύχθηκε η ορθή ταξινόμηση του test set χρησιμοποιώντας τον εκάστοτε ταξινομητή.
"""

def plot_confusion_matrix(cm, classes,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('Predicted label')
    plt.xlabel('True label')
    plt.tight_layout()  
    plt.show()

"""Παράλληλα αναφέρουμε ότι το f1 score, σύμφωνα με το οποίο μας ζητείται να αξιολογήσουμε τους ταξινομητές, ορίζεται ως ο αρμονικός μέσος των γνωστών μας μετρικών Precision(P) και Recall(R) μεταξύ των οποίων υπάρχει συχνά trade-off:$$F1 = 2\frac{P \times R}{P+R}$$
To f1 score επιτυγχάνει να σταθμίζει κατά μία έννοια το precision και το recall μεταξύ των οποίων υπάρχει συνήθως ένα trade-off. Αναφέρουμε για πληρότητα, ότι εδώ, το precision, σύμφωνα με τον τρόπο που έχουμε κατασκευάσει το confusion matrix, μέτράει πόσα από τα δείγματα που ταξινομήσαμε ως bad, άνηκαν όντως σε αυτή την κλάση και το recall πόσα από τα δείγματα που ανήκουν στην κλάση bad, ταξινομήθηκαν ως bad από τον classifier(θεωρώντας ως Positive την κλάση bad).

##Dummy Classifiers

Αρχικά εξετάζουμε τους dummy classifiers με χρήση της κλάσης DummyClassifier, η οποία δέχεται μια παράμετρο που καθορίζει την τακτική της ταξινόμησης ως εξής:

 *   "uniform”: προβλέπει τυχαία και ομοιόμορφα
 *   “constant”: προβλέπει πάντα μία κατηγορία που τη διαλέγει ο χρήστης
 *   “most_frequent”: προβλέπει πάντα την πιο συχνή κατηγορία στο training set
 *   “stratified”: κάνει προβλέψεις διατηρώντας την κατανομή των κλάσεων στο training set

Για τους ταξινομητές αυτούς, δε θα κάνουμε εκτενή ανάλυση των βημάτων και των αριθμητικών και γραφικών αποτελεσμάτων που παράγουν, καθώς σε κάθε run τα outputs των uniform και stratified αλλάζουν.

###Dummy Uniform

Εξετάζουμε, αρχικά, τον dummy stratified. Αρχικά, "εκπαιδεύουμε" τον ταξινομητή στο σύνολο εκπαίδευσης μέσω της fit και με τη μέθοδο predict παράγουμε προβλέψεις για τα δεδομένα ελέγχου(test set) και τις εκτυπώνουμε.
"""

clf = DummyClassifier(strategy="uniform")
clf.fit(X_train, y_train)
dc_uniform_predict = clf.predict(X_test)
print("Predictions for Dummy uniform Classifier:"+"\n"+" {}".format(dc_uniform_predict))

"""Δημιουργούμε τον πίνακα σύγχησης και καλούμε τη συνάρτηση που δημιουργήσαμε για τη γραφική απεικόνισή του.

"""

cm = confusion_matrix(y_test, dc_uniform_predict)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of Dummy uniform Classifier')

clf.predict(X_test)
cnt=0
for i,val in enumerate(dc_uniform_predict):
  if (val=='g'):
    cnt+=1
print(cnt)

"""Παρακάτω, εφαρμόζουμε τις μετρικές απόδοσης του ταξινομητή."""

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, dc_uniform_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_dc_uniform = list(precision_recall_fscore_support(y_test, dc_uniform_predict, average='micro'))
micro_dc_uniform.pop(3) # tou support
precision_micro_dc_uniform = micro_dc_uniform[0]
recall_micro_dc_uniform = micro_dc_uniform[1]
f1_micro_dc_uniform = micro_dc_uniform[2]
print(micro_dc_uniform)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_dc_uniform = list(precision_recall_fscore_support(y_test, dc_uniform_predict, average='macro'))
macro_dc_uniform.pop(3) # tou support
precision_macro_dc_uniform = macro_dc_uniform[0]
recall_macro_dc_uniform = macro_dc_uniform[1]
f1_macro_dc_uniform = macro_dc_uniform[2]
print(macro_dc_uniform)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,dc_uniform_predict, target_names=['bad', 'good'] ,output_dict=True)
print(pd.DataFrame(class_report).T)

"""Η αξιολόγηση του ταξινομητή γίνεται μέσω του f1_micro και f1_macro"""

print("Dummy uniform Classifier f1_micro score = {}".format(f1_micro_dc_uniform))
print("Dummy uniform Classifier  f1_macro score = {}".format(f1_macro_dc_uniform))

"""Παρακάτω φαίνεται η οπτικοποίηση όλων των μετρικών που τυπώσαμε προηγουμένως, σε heatmap."""

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""###Dummy Constant b

Dummy Constant b: Αρχικά, "εκπαιδεύουμε" τον ταξινομητή στο σύνολο εκπαίδευσης μέσω της fit και με τη μέθοδο predict παράγουμε προβλέψεις για τα δεδομένα ελέγχου(test set) και τις εκτυπώνουμε.
"""

clf = DummyClassifier(strategy="constant", constant = 'b')
clf.fit(X_train, y_train)
dc_constant_b_predict = clf.predict(X_test)
print("Predictions for Dummy constant_b Classifier:"+"\n"+" {}".format(dc_constant_b_predict))

"""Δημιουργούμε τον πίνακα σύγχησης και καλούμε τη συνάρτηση που δημιουργήσαμε για τη γραφική απεικόνισή του. """

cm = confusion_matrix(y_test, dc_constant_b_predict)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of Dummy constant_b Classifier')

clf.predict(X_test)
cnt=0
for i,val in enumerate(dc_constant_b_predict):
  if (val=='g'):
    cnt+=1
print(cnt)

"""Παρακάτω, εφαρμόζουμε τις μετρικές απόδοσης του ταξινομητή."""

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, dc_constant_b_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_dc_constant_b = list(precision_recall_fscore_support(y_test, dc_constant_b_predict, average='micro'))
micro_dc_constant_b.pop(3) # tou support
precision_micro_dc_constant_b = micro_dc_constant_b[0]
recall_micro_dc_constant_b = micro_dc_constant_b[1]
f1_micro_dc_constant_b = micro_dc_constant_b[2]
print(micro_dc_constant_b)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_dc_constant_b = list(precision_recall_fscore_support(y_test, dc_constant_b_predict, average='macro'))
macro_dc_constant_b.pop(3) # tou support
precision_macro_dc_constant_b = macro_dc_constant_b[0]
recall_macro_dc_constant_b = macro_dc_constant_b[1]
f1_macro_dc_constant_b = macro_dc_constant_b[2]
print(macro_dc_constant_b)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  dc_constant_b_predict, target_names=['bad','good'] ,output_dict=True)
print(pd.DataFrame(class_report).T)

"""Η αξιολόγηση του ταξινομητή γίνεται μέσω του f1_micro και f1_macro"""

print("Dummy constant_b Classifier f1_micro score = {}".format(f1_micro_dc_constant_b))
print("Dummy constant_b Classifier  f1_macro score = {}".format(f1_macro_dc_constant_b))

"""Παρακάτω φαίνεται η οπτικοποίηση όλων των μετρικών που τυπώσαμε προηγουμένως, σε heatmap."""

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""###Dummy Constant g

Dummy Constant g: Αρχικά, "εκπαιδεύουμε" τον ταξινομητή στο σύνολο εκπαίδευσης μέσω της fit και με τη μέθοδο predict παράγουμε προβλέψεις για τα δεδομένα ελέγχου(test set) και τις εκτυπώνουμε.
"""

clf = DummyClassifier(strategy="constant", constant = 'g')
clf.fit(X_train, y_train)
dc_constant_g_predict = clf.predict(X_test)
print("Predictions for Dummy constant_g Classifier:"+"\n"+" {}".format(dc_constant_g_predict))

"""Δημιουργούμε τον πίνακα σύγχησης και καλούμε τη συνάρτηση που δημιουργήσαμε για τη γραφική απεικόνισή του. """

cm = confusion_matrix(y_test, dc_constant_g_predict)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of Dummy constant_g Classifier')

clf.predict(X_test)
cnt=0
for i,val in enumerate(dc_constant_g_predict):
  if (val=='g'):
    cnt+=1
print(cnt)

"""Παρακάτω, εφαρμόζουμε τις μετρικές απόδοσης του ταξινομητή."""

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, dc_constant_g_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_dc_constant_g = list(precision_recall_fscore_support(y_test, dc_constant_g_predict, average='micro'))
micro_dc_constant_g.pop(3) # tou support
precision_micro_dc_constant_g = micro_dc_constant_g[0]
recall_micro_dc_constant_g = micro_dc_constant_g[1]
f1_micro_dc_constant_g = micro_dc_constant_g[2]
print(micro_dc_constant_g)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_dc_constant_g = list(precision_recall_fscore_support(y_test, dc_constant_g_predict, average='macro'))
macro_dc_constant_g.pop(3) # tou support
precision_macro_dc_constant_g = macro_dc_constant_g[0]
recall_macro_dc_constant_g = macro_dc_constant_g[1]
f1_macro_dc_constant_g = macro_dc_constant_g[2]
print(macro_dc_constant_g)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  dc_constant_g_predict, target_names=['bad','good'] ,output_dict=True)
print(pd.DataFrame(class_report).T)

"""Η αξιολόγηση του ταξινομητή γίνεται μέσω του f1_micro και f1_macro"""

print("Dummy constant_g Classifier f1_micro score = {}".format(f1_micro_dc_constant_g))
print("Dummy constant_g Classifier  f1_macro score = {}".format(f1_macro_dc_constant_g))

"""Παρακάτω φαίνεται η οπτικοποίηση όλων των μετρικών που τυπώσαμε προηγουμένως, σε heatmap."""

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""###Dummy Most Frequent

Dummy Most Frequent: Αρχικά, "εκπαιδεύουμε" τον ταξινομητή στο σύνολο εκπαίδευσης μέσω της fit και με τη μέθοδο predict παράγουμε προβλέψεις για τα δεδομένα ελέγχου(test set) και τις εκτυπώνουμε.
"""

clf = DummyClassifier(strategy="most_frequent")
clf.fit(X_train, y_train)
dc_most_frequent_predict = clf.predict(X_test)
print("Predictions for Dummy most_frequent Classifier:"+"\n"+" {}".format(dc_most_frequent_predict))

"""Δημιουργούμε τον πίνακα σύγχησης και καλούμε τη συνάρτηση που δημιουργήσαμε για τη γραφική απεικόνισή του. """

cm = confusion_matrix(y_test, dc_most_frequent_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of Dummy most_frequent Classifier')

clf.predict(X_test)
cnt=0
for i,val in enumerate(dc_most_frequent_predict):
  if (val=='g'):
    cnt+=1
print(cnt)

"""Παρακάτω, εφαρμόζουμε τις μετρικές απόδοσης του ταξινομητή."""

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, dc_most_frequent_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_dc_most_frequent = list(precision_recall_fscore_support(y_test, dc_most_frequent_predict, average='micro'))
micro_dc_most_frequent.pop(3) # tou support
precision_micro_dc_most_frequent = micro_dc_most_frequent[0]
recall_micro_dc_most_frequent = micro_dc_most_frequent[1]
f1_micro_dc_most_frequent = micro_dc_most_frequent[2]
print(micro_dc_most_frequent)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_dc_most_frequent = list(precision_recall_fscore_support(y_test, dc_most_frequent_predict, average='macro'))
macro_dc_most_frequent = list(precision_recall_fscore_support(y_test, dc_most_frequent_predict, average='macro'))
macro_dc_most_frequent.pop(3) # tou support
precision_macro_dc_most_frequent = macro_dc_most_frequent[0]
recall_macro_dc_most_frequent = macro_dc_most_frequent[1]
f1_macro_dc_most_frequent = macro_dc_most_frequent[2]
print(macro_dc_most_frequent)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  dc_most_frequent_predict, target_names=['bad','good'] ,output_dict=True)
print(pd.DataFrame(class_report).T)

"""Η αξιολόγηση του ταξινομητή γίνεται μέσω του f1_micro και f1_macro"""

print("Dummy most_frequent Classifier f1_micro score = {}".format(f1_micro_dc_most_frequent))
print("Dummy most_frequent Classifier  f1_macro score = {}".format(f1_macro_dc_most_frequent))

"""Παρακάτω φαίνεται η οπτικοποίηση όλων των μετρικών που τυπώσαμε προηγουμένως, σε heatmap."""

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""Προφανώς εδώ ο most_frequent ταξινομητής ισοδυναμεί με τον constant_g αφού στο data set η πλειοψηφία των διγμάτων είναι good και όταν κάνουμε split τα δεδομένα, η αναλογία αυτή διατηρείται τόσο σε train, όσο και σε test data

###Dummy Stratified

Τέλος, εξετάζουμε τον dummy stratified. Αρχικά, "εκπαιδεύουμε" τον ταξινομητή στο σύνολο εκπαίδευσης μέσω της fit και με τη μέθοδο predict παράγουμε προβλέψεις για τα δεδομένα ελέγχου(test set) και τις εκτυπώνουμε.
"""

clf = DummyClassifier(strategy="stratified")
clf.fit(X_train, y_train)
dc_stratified_predict = clf.predict(X_test)
print("Predictions for Dummy stratified Classifier:"+"\n"+" {}".format(dc_stratified_predict))

"""Δημιουργούμε τον πίνακα σύγχησης και καλούμε τη συνάρτηση που δημιουργήσαμε για τη γραφική απεικόνισή του. """

cm = confusion_matrix(y_test, dc_stratified_predict)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of Dummy stratified Classifier')

clf.predict(X_test)
cnt=0
for i,val in enumerate(dc_stratified_predict):
  if (val=='g'):
    cnt+=1
print(cnt)

"""Παρακάτω, εφαρμόζουμε τις μετρικές απόδοσης του ταξινομητή."""

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, dc_stratified_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_dc_stratified = list(precision_recall_fscore_support(y_test, dc_stratified_predict, average='micro'))
micro_dc_stratified.pop(3) # tou support
precision_micro_dc_stratified = micro_dc_stratified[0]
recall_micro_dc_stratified = micro_dc_stratified[1]
f1_micro_dc_stratified = micro_dc_stratified[2]
print(micro_dc_stratified)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_dc_stratified = list(precision_recall_fscore_support(y_test, dc_stratified_predict, average='macro'))
macro_dc_stratified.pop(3) # tou support
precision_macro_dc_stratified = macro_dc_stratified[0]
recall_macro_dc_stratified = macro_dc_stratified[1]
f1_macro_dc_stratified = macro_dc_stratified[2]
print(macro_dc_stratified)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  dc_stratified_predict, target_names=['bad','good'] ,output_dict=True)
print(pd.DataFrame(class_report).T)

"""Η αξιολόγηση του ταξινομητή γίνεται μέσω του f1_micro και f1_macro"""

print("Dummy stratified Classifier f1_micro score = {}".format(f1_micro_dc_stratified))
print("Dummy stratified Classifier  f1_macro score = {}".format(f1_macro_dc_stratified))

"""Παρακάτω φαίνεται η οπτικοποίηση όλων των μετρικών που τυπώσαμε προηγουμένως, σε heatmap."""

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""##Gaussian Naive Bayes

>Ο Gaussian Naive Bayes ταξινομητής στηρίζεται στο νόμο του Bayes και στην υπόθεση ότι τα χαρακτηριστικά των δεδομένων είναι ανεξάρτητα μεταξύ τους. Ταξινομεί, κάθε δεδομένο στην κλάση y σύμφωνα με τον εξής τύπο: $$\hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i \mid y)$$ δηλαδή κάθε δείγμα ταξινομείται στην κλάση που μεγιστοποιεί τη σχετική συχνότητα της κλάσης αυτής επί το γινόμενο των πιθανοτήτων όλων των χαρακτηριστικών, δεδομένης της κλάσης αυτής(πιθανοφάνεια).

>Θεωρούμε ότι η κατανομή κάθε χαρακτηριστικού ως προς κάθε κλάση ακολουθεί την κανονική κατανομή:
$$P(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma^2_y}\right)$$

>Πρακτικά, όσο πιο κοντά στη μέση τιμή του (ως προς το σύνολο του train set)είναι ένα χαρακτηριστικό ενός δείγματος, τόσο πιο κοντά στη μοναδα θα είναι η πιθανοφάνεια του χαρακτηριστικού.

>Τον ταξινομητή αυτόν, θα τον αναλύσουμε περισσότερο ως προς τα αριθμητικά αποτελέσματα που παράγει, καθώς λόγω της κατασκευής του και της συμπεριφοράς του, εξάγει τα ίδια αποτελέσματα σε κάθε ταξινόμηση(κάθε run του κώδικα) που πραγματοποιεί πάνω σε συγκεκριμένο test set, δεδομένου ενός συγκεκριμένου training set.

Αρχικά, "εκπαιδεύουμε" τον ταξινομητή στο σύνολο εκπαίδευσης μέσω της fit και με τη μέθοδο predict παράγουμε προβλέψεις για τα δεδομένα ελέγχου(test set) και τις εκτυπώνουμε.
"""

clf = GaussianNB()
clf.fit(X_train, y_train)
GaussianNB_predict = clf.predict(X_test)
print("Predictions for KNN classifier:"+"\n"+" {}".format(GaussianNB_predict))

"""Δημιουργούμε τον πίνακα σύγχησης και καλούμε τη συνάρτηση που δημιουργήσαμε για τη γραφική απεικόνισή του. Από την παρακάτω αναπαράσταση προκύπτει ότι από τα 71 δείγματα του test set, τα 20+44 = 64 ταξινομήθηκαν σωστά (20 ταξινομήθηκαν ως bad και ανήκαν όντως στην κλάση bad, σύμφωνα με το label τους και 44 ταξινομήθηκαν ως good και όντως ανήκαν σε αυτή την κλάση). Τα υπόλοιπα 7 ταξινομήθηκαν λανθασμένα από τον gaussian NB και συγκεκριμένα 2 ανήκαν στην κλάση good και ταξινομήθηκαν ως bad, ενώ 5 ταξινομήθηκαν ως good και ανήκαν στην κλάση bad."""

cm = confusion_matrix(y_test, GaussianNB_predict)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of Gaussian Naive Bayes classifier')

clf.predict(X_test)
cnt=0
for i,val in enumerate(GaussianNB_predict):
  if (val=='g'):
    cnt+=1
print(cnt)

"""Παρακάτω, εφαρμόζουμε τις μετρικές απόδοσης του ταξινομητή."""

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, GaussianNB_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_GNB = list(precision_recall_fscore_support(y_test, GaussianNB_predict, average='micro'))
micro_GNB.pop(3) # tou support
precision_micro_GNB = micro_GNB[0]
recall_micro_GNB = micro_GNB[1]
f1_micro_GNB = micro_GNB[2]
print(micro_GNB)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_GNB = list(precision_recall_fscore_support(y_test, GaussianNB_predict, average='macro'))
macro_GNB.pop(3) # tou support
precision_macro_GNB = macro_GNB[0]
recall_macro_GNB = macro_GNB[1]
f1_macro_GNB = macro_GNB[2]
print(macro_GNB)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,GaussianNB_predict, target_names=['bad','good'] ,output_dict=True)
print(pd.DataFrame(class_report).T)

"""Η αξιολόγηση του Gaussian Naive Bayes (όπως και κάθε άλλου ταξινομητή στην παρούσα άσκηση) γίνεται μέσω των μετρικών f1_micro και f1_macro. Παρατηρούμε ότι δεν είναι ίσα καθώς οι κλάσεις δεν είναι ισορροπημένες.

Αξίζει, για ένα τουλάχιστον παράδειγμα ταξινομητή, να δείξουμε αναλυτικά πώς προκύπτουν οι παρακάτω τιμές.

Από τον παραπάνω πίνακα σύγχυσης έχουμε για κάθε κλάση:


*   Bad: 20 TP και 2 FP και 5 FN
*   Good: 44 TP και 5 FP και 2 FN


οπότε έχουμε 


*   $P_{Bad}=\frac{20}{20+2} = 0.91$
*   $R_{Bad}=\frac{20}{20+5} = 0.80$ 
*   $P_{Good}=\frac{44}{44+5} = 0.90$
*   $R_{Good}=\frac{44}{44+2} = 0.96$.

Οπότε για **macro_average** έχουμε:
$P = \frac{0.91+0.9}{2} = 0.905$  και $R = \frac{0.80+0.96}{2} = 0.88$ άρα το  $f1_{macro} = 2\frac{P \times R}{P+R} = 0.89$

Αντίστοιχα, για το **micro_average** έχουμε: $P = \frac{20+44}{20+44+2+7} = 0.90$ και  $R = \frac{20+44}{20+44+2+7} = 0.90$, άρα και το $f1_{micro} = 2\frac{P \times R}{P+R} = 0.90$.

*Οι αποκλίσεις οφείλονται στις στρογγυλοποιήσεις*

"""

print("Gaussian Naive Bayes classifier f1_micro score = {}".format(f1_micro_GNB))
print("Gaussian Naive Bayes classifier f1_macro score = {}".format(f1_macro_GNB))

"""Παρακάτω φαίνεται η οπτικοποίηση όλων των μετρικών που τυπώσαμε προηγουμένως, σε heatmap."""

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""##KNN

>O kNN είναι ένας μη παραμετρικός ταξινομητής βασισμένος σε παραδείγματα (instance-based). Η αρχή λειτουργίας του είναι πολύ απλή. Για ένα νέο δείγμα προς ταξινόμηση, πρώτα υπολογίζουμε τους k πλησιέστερους γείτονές του (στον ν-διάστατο χώρο των χαρακτηριστικών εισόδου) με βάση κάποια συνάρτηση απόστασης, συνήθως ευκλείδεια
$$d(x, x') = \sqrt{\left(x_1 - x'_1 \right)^2 + \left(x_2 - x'_2 \right)^2 + \dotsc + \left(x_n - x'_n \right)^2}$$
Η κλάση του νέου δείγματος θα είναι η κλάση της πλειοψηφίας των k γειτόνων, είτε απλά υπολογισμένη (άθροισμα) είτε (αντίστροφα) ζυγισμένη με βάση την απόσταση του κάθε γείτονα.

>Ο KNN είναι απαιτητικός σε χρόνο και σε χώρο καθώς για να ταξινομήσουμε ένα νέο δείγμα στη φάση test,  πρέπει να συγκρίνουμε την απόστασή του με κάθε δείγμα του train set, τα οποία πρέπει να είναι αποθηκευμένα.

Αρχικά, "εκπαιδεύουμε" τον ταξινομητή στο σύνολο εκπαίδευσης μέσω της fit και με τη μέθοδο predict παράγουμε προβλέψεις για τα δεδομένα ελέγχου(test set) και τις εκτυπώνουμε. Κάνοντας απλή αρχικοποίηση, παίρνουμε ως τιμή της παραμέτρου του πλήθους των γειτόνων(k) την  default, δηλαδή το 5.
"""

clf = KNeighborsClassifier(n_neighbors=5)
clf.fit(X_train, y_train)
KNN_predict = clf.predict(X_test)
print("Predictions for KNN classifier:"+"\n"+" {}".format(KNN_predict ))

"""Δημιουργούμε τον πίνακα σύγχησης και καλούμε τη συνάρτηση που δημιουργήσαμε για τη γραφική απεικόνισή του. """

cm = confusion_matrix(y_test, KNN_predict)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of KNN classifier')

clf.predict(X_test)
cnt=0
for i,val in enumerate(KNN_predict):
  if (val=='g'):
    cnt+=1
print(cnt)

"""Παρακάτω, εφαρμόζουμε τις μετρικές απόδοσης του ταξινομητή."""

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, KNN_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_KNN = list(precision_recall_fscore_support(y_test, KNN_predict, average='micro'))
micro_KNN.pop(3) # tou support
precision_micro_KNN  = micro_KNN [0]
recall_micro_KNN  = micro_KNN [1]
f1_micro_KNN  = micro_KNN [2]
print(micro_KNN )

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_KNN  = list(precision_recall_fscore_support(y_test, KNN_predict, average='macro'))
macro_KNN .pop(3) # tou support
precision_macro_KNN  = macro_KNN [0]
recall_macro_KNN  = macro_KNN [1]
f1_macro_KNN  = macro_KNN [2]
print(macro_KNN )
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  KNN_predict, target_names=['bad','good'] ,output_dict=True)
print(pd.DataFrame(class_report).T)

"""Η αξιολόγηση του ταξινομητή γίνεται μέσω του f1_micro και f1_macro"""

print("KNN classifier f1_micro score = {}".format(f1_micro_KNN))
print("KNN classifier f1_macro score = {}".format(f1_macro_KNN))

"""Παρακάτω φαίνεται η οπτικοποίηση όλων των μετρικών που τυπώσαμε προηγουμένως, σε heatmap."""

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""## Συγκρίσεις Ταξινομητών

Έχοντας υπολογίσει αναλυτικά στα προηγούμενα βήματα τις μετρικές απόδοσης f1_micro και f1_macro για τους Dummy, Gaussian Naive Bayes και KNN ταξινομητές, παρουσιάζουμε συγκεντρωτικά τα αποτελέσματα με γραφική απαεικόνιση, ώστε να συγκρίνουμε την απόδοσή τους.
"""

fig = plt.figure()
x = ['uniform','constant b', 'constant g','most frequent', 'stratified','Gaussian NB','KNN(5)',]
y=[100*f1_micro_dc_uniform, 100*f1_micro_dc_constant_b, 100*f1_micro_dc_constant_g, 100*f1_micro_dc_most_frequent, 100*f1_micro_dc_stratified,100*f1_micro_GNB,100*f1_micro_KNN]
plt.bar(x,y, align='center', width=0.5)
plt.xticks(x,x)
plt.title('F1 micro')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

fig = plt.figure()
x = ['uniform','constant b', 'constant g','most frequent', 'stratified','Gaussian NB','KNN(5)',]
y=[100*f1_macro_dc_uniform, 100*f1_macro_dc_constant_b, 100*f1_macro_dc_constant_g, 100*f1_macro_dc_most_frequent, 100*f1_macro_dc_stratified,100*f1_macro_GNB,100*f1_macro_KNN]
plt.bar(x,y, align='center', width=0.5)
plt.xticks(x,x)
plt.title('F1 macro')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

""">Αρχικά, βλέπουμε ότι κάθε ταξινομητής έχει διαφορετικές τιμές για κάθε μετρική, αφού το data set είναι imbalanced.

>Παρατηρούμε ότι την καλύτερη απόδοση έχει ο Gaussian NB και ως προς τις δύο μετρικές και αυτό οφείλεται στο ότι ο αλγόριθμος "μαθαίνει" μέσω της εκπαίδευσης του training set. Επίσης οι αποκλίσεις από τη μέση τιμή και την διασπορά είναι πολύ μικρές αφού τα δεδομένα είναι κανονικοποιημένα στο [-1,1] και ένας λόγος αστοχίας του Gaussian NB είναι οι μεγάλες αυτές αποκλίσεις, που εδώ δεν παρατηρούνται.

>Ο KNN που δεν έχει φάση εκπαίδευσης, ναι μεν εξετάζει όλα τα στοιχεία του train set σε κάθε βήμα, αλλά δεν είναι βελτιστοποιημένος ως προς την υπερπαράμετρο k, η οποία είναι ιδιαιτέρως σημαντική για την απόδοση του ταξινομητή. 

>Και στους δύο αυτούς ταξινομητές έχουμε καλές αποδόσεις και οι λανθασμένες ταξινομήσεις οφείλονται εν μέρει στο γεγονός ότι το set είναι imbalanced και οι ταξινομητές, προβλέπουν κυρίως την επικρατέστερη κλάση που είναι η good(Ο Gaussian NB που έχει 7 λανθασμένες, προβλέπει από αυτές 5 στην good και 2 στην bad και ο kNN έχει 10 και τις προβλέπει όλες ως good).

> Οι dummy classifiers, σε κάθε run παρουσιάζουν διαφορετικές αποδόσεςι και κάθε φορά άλλος εμφανίζεται καλύτερος και άλλος χειρότερος. Πάντως αυτούς που μπορούμε να λάβουμε σοβαρότητα υπόψην μας είναι ο stratified και o uniform. 

> Αναφορικά με το precision και recall στους Gaussian Naive Bayes και kNN, παρατηρούμε ότι το precision (θεωρώντας ως positive το label "bad") είναι αρκετά υψηλό και στις δύο, που σημαίνει ότι το ποσοστό των δειγμάτων που ταξινομήσαμε ως bad και ανήκουν όντως σε αυτή την κλάση, είναι αρκετά υψηλό, ενώ το recall είναι πιο χαμηλό, δηλαδή το ποσοστό των δειγμάτων που ανήκουν στην bad και τα ταξινομήσαμε στην κλάση αυτή δεν είναι πολύ υψηλό. Αυτό είναι λογικό, καθώς υπάρχει μια σχέση trade-off μεταξύ των δύο μεγεθών.

#Βημα 3: Βελτιστοποίηση ταξινομητών

Στο βήμα αυτό, βελτιστοποιούμε ως προς την υπερπαράμετρο k του KNN και αξιολογούμε την απόδοσή του συναρτήσει της παραμέτρου με τη βοήθεια δύο βοηθητικών εργαλείων: το Validation Curve και το Learning Curve. Τέλος, θα συγκρίνουμε τον βελτιστοποιημένο ταξινομητή με τους υπόλοιπους.

*Οι dummy και Gaussian NB δεν έχουν υπερπαραμέτρους προς βελτιστοποίηση*

> Προσοχή: Επίσης κάναμε προσπάθεια βελτιστοποίησης και των δεδομένων εκπαιδευσεις ελέγχοντας το κατα πόσον η μείωση διαστατικότητας που κάνει ο pca θα βοηθούσε στην επίδοση. Ωστόσο επειδή ο χώρος είναι μικρής διάστασης, δηλαδή έχει πολλά features δεν παρατηρηθηκε καποια βελτιστοποίηση. Στο Β μέρος της εργασίας μπορείτε να δείτε το πρωτόκολλο με την εφαρμογή gridsearch στην υπερπαρμάμετρο n του pca.

Αρχικά θα κάνουμε μια συνοπτική επισκόπηση των δύο εργαλείων Validation Curve και Learning curve.

**Validation Curve:** Αποτελεί σημαντικό εργαλείο απεικόνισης της απόδοσης ενός μοντέλου ως προς μια μετρική, συναρτήσει κάποιας υπερπαραμέτρου. Απεικονίζει 2 καμπύλες: μία για το training set score και μία για το cross-validation score. 

**Learning Curve:** Η καμπύλη αυτή μας δείχνει το training και το testing score του εκτιμητή ως συνάρτηση του μεγέθους των training data. Οπτικοποιεί το πόσο βοηθά το σύστημα η προσθήκη περισσότερων training δεδομένων και το κατά πόσο ο εκτιμητής πάσχει απόό variance ή bias error.

##Βελτιστοποίηση ΚΝΝ

Για τη βελτιστοποίηση του k κάνουμε grid search, με 10-fold cross validation, oρίζoντας ένα πεδίο ορισμού για το k. Για κάθε τιμή k, θα πρέπει να υπολογιστεί ο μέσος όρος του εκτιμητή σε όλα τα folds του cross-validation με βάση το f1_micro και f1_macro και να επιλεχθεί το καλύτερο k. 

Η συγκεκριμένη στρατηγική αναζήτησης των βέλτιστων υπερπαραμέτρων είναι η εξαντλητική αναζήτηση πλέγματος (exhaustive grid search) και είναι προφανώς πολύ ακριβή υπολογιστικά. Υπάρχουν διάφορες τεχνικές για να περιορίζεται η πολυπλοκότητα του grid search, αλλά δεν το αποφεύγουμε γενικά, γιατί οι υπερπαράμετροι είναι ορίσματα των εκτιμητών και δεν μαθαίνονται από την fit.
"""

# Commented out IPython magic to ensure Python compatibility.
param_grid = {'n_neighbors':[1,2,3,4,5,6,7,8,9,10]} 

grid = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True,  cv = 10, n_jobs = -1,scoring="f1_micro")
# %time grid.fit(X_train, y_train)  # austira to train set!
print("Best parameters regarding f1_micro is = " +str(grid.best_params_))
print()
grid = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True,  cv = 10, n_jobs = -1,scoring="f1_macro")
# %time grid.fit(X_train, y_train)  # austira to train set!
print("Best parameters regarding f1_macro is = " +str(grid.best_params_))

"""To μοντέλο δουλεύει για k = 2, αν και δε συνηθιζεται να θέτουμε παράμετρο του kΝΝ άρτιους αριθμούς, λόγω του τυχαίου τρόπου που λύνονται οι ισοπαλίες. Αλλά από ότι φαίνεται εδώ, αυτή η τυχαιότητα επιβoηθά το μοντέλο και το σύστημα.

Στη συνέχεια απεικονίζουμε γραφικά τις validation curves για το f1_macro και f1_micro, θεωρώντας ως υπερπαράμετρο το πλήθος των γειτόνων(k) του kNN.
"""

param_range = [1,2,3,4,5,6,7,8,9,10]
train_scores, test_scores = validation_curve(
    KNeighborsClassifier(), X_train, y_train,  param_name="n_neighbors", param_range=param_range,
    scoring="f1_micro", n_jobs=-1,cv = 10)

train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

plt.title("Validation Curve with KNN scroning f1_micro")
plt.xlabel("n_neighbors")
plt.ylabel("f1_micro Score")
plt.ylim(0.0, 1.1)
lw = 2
plt.semilogx(param_range, train_scores_mean, label="Training score",
             color="darkorange", lw=lw)
plt.fill_between(param_range, train_scores_mean - train_scores_std,
                 train_scores_mean + train_scores_std, alpha=0.2,
                 color="darkorange", lw=lw)
plt.semilogx(param_range, test_scores_mean, label="Cross-validation score",
             color="navy", lw=lw)
plt.fill_between(param_range, test_scores_mean - test_scores_std,
                 test_scores_mean + test_scores_std, alpha=0.2,
                 color="navy", lw=lw)
plt.legend(loc="best")
plt.show()

param_range = [1,2,3,4,5,6,7,8,9,10]
train_scores, test_scores = validation_curve(
    KNeighborsClassifier(), X_train, y_train,  param_name="n_neighbors", param_range=param_range,
    scoring="f1_macro", n_jobs=-1,cv = 10)

train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

plt.title("Validation Curve with KNN scroning f1_macro")
plt.xlabel("n_neighbors")
plt.ylabel("f1_macro Score")
plt.ylim(0.0, 1.1)
lw = 2
plt.semilogx(param_range, train_scores_mean, label="Training score",
             color="darkorange", lw=lw)
plt.fill_between(param_range, train_scores_mean - train_scores_std,
                 train_scores_mean + train_scores_std, alpha=0.2,
                 color="darkorange", lw=lw)
plt.semilogx(param_range, test_scores_mean, label="Cross-validation score",
             color="navy", lw=lw)
plt.fill_between(param_range, test_scores_mean - test_scores_std,
                 test_scores_mean + test_scores_std, alpha=0.2,
                 color="navy", lw=lw)
plt.legend(loc="best")
plt.show()

"""Από τις δύο παραπάνω καμπύλες παρατηρούμε παρόμοια συμπεριφορά του training και cross-validation score και για τις δύο μετρικές απόδοσης. Γενικά παρατηρούμε ότι οι δύο καμπύλες συγκλίνουν σε μεγάλο βαθμό για όλες τις τιμές της υπερπαραμέτρου, το οποίο μας δείχνει ότι τα δεδομένα είναι fitted αρκετά καλά από τον ταξινομητή. Επίσης, δεν υπάρχουν μεγάλες διακυμάνσεις στις καμπύλες, που σημαίνει ότι το σύστημα είναι ευσταθές ως προς την παράμετρο n_neighbors(robust). Ωστόσο, παρατηρούμε ότι για k = 2, η καμπύλη του validation λαμβάνει τη μέγιστη τιμή, υποδηλώνοντας ότι αυτή είναι η τιμή στην οποία το σύστημα παρουσιάζει την καλύτερη απόδοση, το οποίο συμφωνεί και με το παραπάνω αποτέλεσμα του grid search. Αξίζει να αναφέρουμε ως παρατήρηση ότι υπάρχει μεγαλύτερη σύγκλιση των δύο καμπυλών για μεγαλύτερα k αλλά η απόδοση του cross-validation score, πέφτει.

Παρακάτω απεικονίζουμε γραφικά τις learning curves για το f1_macro και f1_micro, θεωρώντας ότι ο ταξινομητής έχει λειτουργήσει με k=2, δηλαδή με βέλτιστη υπερπαράμετρο.
"""

def plot_learning_curve(train_scores, test_scores, train_sizes,title):
    plt.figure()
    plt.grid()
    plt.title(title)
    plt.xlabel("Training examples")
    plt.ylabel("Score")

    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",label="Testing score")
    plt.legend(loc="best")
    return

train_sizes, train_scores, test_scores = learning_curve(KNeighborsClassifier(n_neighbors=2), 
                                              X_train, y_train, train_sizes = np.linspace(0.1, 1.0, 5), 
                                              n_jobs = -1,scoring="f1_micro")
plot_learning_curve(train_scores, test_scores, train_sizes,"Learning Curve scoring f1_micro")

train_sizes, train_scores, test_scores = learning_curve(KNeighborsClassifier(n_neighbors=2), 
                                              X_train, y_train, train_sizes = np.linspace(0.1, 1.0, 5), 
                                              n_jobs = -1,scoring="f1_macro")
plot_learning_curve(train_scores, test_scores, train_sizes,"Learning Curve scoring f1_macro")

"""Από τα παραπάνω βλέπουμε ότι και για τις δύο μετρικές, οι καμπύλες δεν συγκλίνουν και η καμπύλη του training score είναι πάνω από το testing, που σημαίνει ότι το μοντέλο είναι underfitted. Αυτό είναι λογικό καθώς έχουμε πολύ μικρό πλήθος δεδομένων, ώστε να δημιουργήσουμε ένα ακριβές και λειτουργικό μοντέλο, το οποίο να σκιαγραφεί την κατανομή των δεδομένων.

Έπειτα κάνουμε fit τα train δεδομένα με τη βέλτιστη τιμή υπερπαραμέτρου και predict με τα test δεδομένα. Παράλληλα, τυπώνουμε και τους χρόνους εκτέλεσης της πρόβλεψης, οι οποίοι παρατηρούμε ότι είναι μικροί.
"""

# Commented out IPython magic to ensure Python compatibility.
clf = KNeighborsClassifier(n_neighbors=2)
# %time clf.fit(X_train, y_train)
# %time KNN_predict = clf.predict(X_test)
print("Predictions for KNN optimal classifier:"+"\n"+" {}".format(KNN_predict ))

"""Στη συνέχεια τυπώνουμε κατά τα γνωστά τον πίνακα σύγχησης και παρατηρούμε ότι από τα 71 δεδομένα προβλέπει λάθος μόνο τα 9, που είναι λιγότερα από τις λανθασμένες προβλέψεις του kNN που δεν ήταν βελτιστοποιημένος ως προς την υπερπαράμετρο k και ταξινομούσε 10 δείγματα λάθος."""

cm = confusion_matrix(y_test, KNN_predict)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of KNN optimal classifier')

"""Ο ταξινομητής προβλέπει 53 good ('g') αποτελέσματα"""

clf.predict(X_test)
cnt=0
for i,val in enumerate(KNN_predict):
  if (val=='g'):
    cnt+=1
print(cnt)

"""Παρακάτω, φαίνεται το σύνολο των μετρικών απόδοσης τους ταξινομητή μέσω της μεθόδου classification_report."""

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, KNN_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_KNN_opt = list(precision_recall_fscore_support(y_test, KNN_predict, average='micro'))
micro_KNN_opt.pop(3) # tou support
precision_micro_KNN_opt  = micro_KNN_opt[0]
recall_micro_KNN_opt  = micro_KNN_opt [1]
f1_micro_KNN_opt  = micro_KNN_opt[2]
print(micro_KNN_opt )

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_KNN_opt = list(precision_recall_fscore_support(y_test, KNN_predict, average='macro'))
macro_KNN_opt .pop(3) # tou support
precision_macro_KNN_opt  = macro_KNN_opt[0]
recall_macro_KNN_opt = macro_KNN_opt[1]
f1_macro_KNN_opt = macro_KNN_opt[2]
print(macro_KNN_opt)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  KNN_predict, target_names=['bad','good'] ,output_dict=True)
print(pd.DataFrame(class_report).T)

"""H αξιολόγηση της απόδοσης του ταξινομητή γίνεται με τις μετρικές f1_micro και f1_macro και οι τιμές τους φαίνονται παρακάτω."""

print("KNN classifier f1_micro score = {}".format(f1_micro_KNN_opt))
print("KNN classifier f1_macro score = {}".format(f1_macro_KNN_opt))

"""Παρακάτω φαίνονται τα προηγούμενα αποτελέσματα με heatmap."""

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""Για πληρότητα παρουσιάζουμε και τα αποτελέσματα των dummy stratified και Gaussian Naive Bayes με 10-fold-cross-validation και παρατηρούμε ότι τα αποτελέσματα είναι συγκρίσιμα με τα αντίστοιχα του βήματος 2.

##Dummy stratified
"""

dc_stratified = DummyClassifier(strategy="stratified")
dc_stratified.fit(X_train, y_train)
dc_stratified_predict = cross_val_predict(dc_stratified, np.concatenate((X_train,X_test), axis=0), 
                           np.concatenate((y_train,y_test), axis=0), cv=KFold(n_splits=10))
print("Predictions for Stratified dummy classifier:"+"\n"+" {}".format(dc_stratified_predict))
print()

scores_most_stratified_micro = cross_val_score(dc_stratified , np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), 
                         scoring="f1_micro")
scores_most_stratified_macro = cross_val_score(dc_stratified , np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), 
                         scoring="f1_macro")


print("Stratified dummy classifier CV f1_micro score = %f +-%f" % (100*np.mean(scores_most_stratified_micro), 100*np.std(scores_most_stratified_micro)))
print("Stratified dummy classifier CV f1_macro score = %f +-%f" % (100*np.mean(scores_most_stratified_macro), 100*np.std(scores_most_stratified_macro)))

cm = confusion_matrix(np.concatenate((y_train,y_test), axis=0), dc_stratified_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of Stratified dummy classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0),  dc_stratified_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_dc_stratified= list(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), dc_stratified_predict, average='micro'))
micro_dc_stratified.pop(3) # tou support
precision_micro_dc_stratified  = micro_dc_stratified[0]
recall_micro_dc_stratified  = micro_dc_stratified[1]
f1_micro_dc_stratified = micro_dc_stratified[2]
print(micro_dc_stratified )

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_dc_stratified = list(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), dc_stratified_predict, average='macro'))
macro_dc_stratified .pop(3) # tou support
precision_macro_dc_stratified = macro_dc_stratified[0]
recall_macro_dc_stratified  = macro_dc_stratified[1]
f1_macro_dc_stratified = macro_dc_stratified[2]
print(macro_dc_stratified)
print()
print("All the details shown here:")
print()
print(classification_report(np.concatenate((y_train,y_test), axis=0), dc_stratified_predict, target_names=['bad','good'] ) )

"""##Gaussian Naive Bayes"""

clf = GaussianNB()
clf.fit(X_train, y_train)

#GaussianNB_predict = clf.predict(X_test)
GaussianNB_predict = cross_val_predict(clf, np.concatenate((X_train,X_test), axis=0), 
                           np.concatenate((y_train,y_test), axis=0), cv=KFold(n_splits=10))

print("Predictions for Gaussian Naive Bayes classifier:"+"\n"+" {}".format(GaussianNB_predict))
print()

GaussianNB_micro = cross_val_score(clf, np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), scoring="f1_micro")
GaussianNB_macro = cross_val_score(clf, np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), scoring="f1_macro")

print("Gaussian Naive Bayes classifier CV f1_micro score = %f +-%f" % (100*np.mean(GaussianNB_micro), 100*np.std(GaussianNB_micro)))
print("Gaussian Naive Bayes classifier CV f1_macro score = %f +-%f" % (100*np.mean(GaussianNB_macro), 100*np.std(GaussianNB_macro)))

cm = confusion_matrix(np.concatenate((y_train,y_test), axis=0), GaussianNB_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of Gaussian Naive Bayes classifier')

### me ayth thn synarthsh epibabaiwnnw oti evala swsta ta labels
### vlepw ;oti eipa oti 49 data htan good ek twn opoiwn ta 44 htan ontws good
cnt=0
for i,val in enumerate(GaussianNB_predict):
  if (val=='g'):
    cnt+=1
print(cnt)

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), GaussianNB_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_GNB = list(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), GaussianNB_predict, average='micro'))
micro_GNB.pop(3) # tou support
precision_micro_GNB = micro_GNB[0]
recall_micro_GNB = micro_GNB[1]
f1_micro_GNB = micro_GNB[2]
print(micro_GNB)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_GNB = list(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), GaussianNB_predict, average='macro'))
macro_GNB.pop(3) # tou support
precision_macro_GNB = macro_GNB[0]
recall_macro_GNB = macro_GNB[1]
f1_macro_GNB = macro_GNB[2]
print(macro_GNB)
print()
print("All the details shown here:")
print()
print(classification_report(np.concatenate((y_train,y_test), axis=0), GaussianNB_predict, target_names=['bad','good'] ) )

"""## Σύγκριση Ταξινομητων

Στο σημείο αυτό, παρουσιάζουμε τα συγκεντρωτικά αποτελέσματα της απόδοσης των Dummy Stratified, Gaussian Naive Bayes, kNN και kNN optimal, όπου ο kNN αντιστοιχεί στην default τιμή k = 5 και o kNN_optimal σε αυτό, με βελτιστoποιημένο αριθμό γειτόνων k = 2.

Τη σύγκριση των Dummy, Gaussian Naive Bayes και απλού kNN πραγματοποιήσαμε στο βήμα 3 και εδώ βλέπουμε ότι η απόδοση του νέου ταξινομητή kNN_optimal είναι μεγαλύτερη από τον απλό kNN, όπως αναμέναμε αλλά παραμένει μικρότερη από τον Gaussian Naive Bayes, γεγονός που υποδεικνύει ότι ο ταξινομητής μπορεί να επιδέχεται περαιτέρω βελτίωση που ενδεχομένως να αυξήσει την απόδοσή του, το οποίο και πραγματοποιούμε στο βήμα 4 (Bonus: Extra Βελτιστοποιήσεις). Οι παρατηρήσεις αυτές ισχύουν και για τις δύο μετρικές.

Αναφορικά με τα precision και recall παρατηρούμε τα ίδια φαινόμενα για τον Gaussian και τον kNN, ενώ αξίζει να αναφέρουμε ότι στον Dummy Stratified είναι και τα δύο αρκετά χαμηλά.
"""

fig = plt.figure()
x = ['Dummy_stratified','Gaussian NB','KNN','KNN_optimal']
y=[100*f1_micro_dc_stratified,100*f1_micro_GNB,100*f1_micro_KNN,100*f1_micro_KNN_opt]
plt.bar(x,y, align='center', width=0.4)
plt.xticks(x,x)
plt.title('F1 micro')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

fig = plt.figure()
x = ['Dummy_stratified','Gaussian NB','KNN','KNN_optimal']
y=[100*f1_macro_dc_stratified,100*f1_macro_GNB,100*f1_macro_KNN,100*f1_macro_KNN_opt]
plt.bar(x,y, align='center', width=0.4)
plt.xticks(x,x)
plt.title('F1 macro')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

"""## Συγκριση βελτίωσης επίδοσης KNN

Παρακάτω αντιπαραβάλουμε μόνο την απόδοση των δύο kNN (του απλού και του optimal), ώστε να έχουμε μια πιο συμπυκνωμένη εικόνα σύγκρισης. Η διαφορά αυτή έγκειται στο ότι με τη μέθοδο grid search βρήκαμε τη βέλτιστη παράμετρο k και επομένως έχουμε πιο αποδοτικό μοντέλο.
"""

fig = plt.figure()
x = ['KNN','KNN_optimal']
y=[100*f1_micro_KNN,100*f1_micro_KNN_opt]
plt.bar(x,y, align='center', width=0.4)
plt.xticks(x,x)
plt.title('F1 micro KNN improvement')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
x = ['KNN','KNN_optimal']
y=[100*f1_macro_KNN,100*f1_macro_KNN_opt]
plt.bar(x,y, align='center', width=0.4)
plt.xticks(x,x)
plt.title('F1 macro KNN improvement')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

"""# Βήμα 4 Bonus : Extra Βελτισοποιησεις

Στο βήμα αυτό εφαρμόζουμε στάδια προεπεξεργασίας των δεδομένων ώστε ο ταξινομητής να μας δώσει ακόμα καλύτερα αποτελέσματα.

Ωστόσο εφαρμόζουμε εξ αρχής default προεπεξεργασία ορίζοντας σταθερές τιμές στο threshold του variance χωρίς να κάνουμε gridsearch για καθε ταξινομητή. Αυτό είναι κάτι απλοικό, αλλά το σωστό θα ήταν να γίνει gridsearch για να βρεθεί η καταλληλότερη τιμή του για κάθε ταξινομητή.

## Κόβουμε πιο αυστηρά τα Variance

Αρχικά, εφαρμόζουμε την τεχνική Variance Threshold, έτσι ώστε να μειώσουμε τις διαστάσεις των δεδομένων μας, καθώς η ευκλείδια απόσταση με την οποία ο kNN βρίσκει τους κοντινούς του γείτονες το οποίο δεν έχει ιδιαίτερο νόημα για μεγάλη διαστατικότητα ειδικά στην περίπτωσή μας πού έχουμε μικρό data set. Σε γενικές γραμμές αν η διακύμανση ενός χαρακτηριστικού εισόδου είναι πολύ χαμηλή, δεν μπορεί να προσφέρει σημαντικά στη διαχωριστική ικανότητα του ταξινομητή, οπότε μπορούμε να αφαιρέσουμε αυτά τα χαρακτηριστικά.
Πρέπει επίσης να πάρουμε μια μάσκα (index) των χαρακτηριστικών που επιλέγουμε, ώστε να την εφαρμόσουμε και στα δεδομένα train ώστε να έχουν τις ίδιες διαστάσεις. Αυτό δεν σπάει τον κανόνα ότι δεν χρησιμοποιούμε τα δεδομένα test γιατί μπορούμε να θεωρήσουμε ότι ο ταξινομητής απλώς αγνοεί τις εισόδους που δεν περιλαμβάνονται στη μάσκα.

Εδώ επιλέγουμε τυχαία για threshold την τιμή 0.27. Εναλλακτικά, θα μπορούσαμε να χωρίσουμε το data set σε training, testing και validation set και με το τελευταίο να ρυθμίζουμε κατάλληλα τις υπερπαραμέτρους και στη συγκεκριμένη περίπτωση το Variance Threshold.
"""

from sklearn.feature_selection import VarianceThreshold

# αρχικοποιούμε έναν selector
selector = VarianceThreshold(threshold=0.27)
# όπως κάναμε και με τους ταξινομητές τον κάνουμε fit στα δεδομένα εκπαίδευσης
train_reduced = selector.fit_transform(X_train)
#print(train_reduced)
mask = selector.get_support()
#print(mask)

test_reduced = selector.transform(X_test)
#print(test_reduced)
print(train_reduced .shape)

"""Βλέπουμε πως από τα 34 χαρακτηριστικά που είχαμε, με την εφαρμογή κατωφλίου για το Variance, έχουμε πλέον 17 διαστάσεις για τα δεδομένα μας, το οποίο είναι σημαντική μείωση. 

Αξίζει να σημειώσουμε ότι αφού έχουμε επιτύχει μια καλή μείωση της διαστατικότητας και αφού έχουμε μικρό data set, δεν κρίνεται απαραίτητο να εφαρμόσουμε τη μέθοδο PCA για την οποία κάναμε λόγο παραπάνω που μπορεί, ως extra υπερπαράμετρος να μας οδηγήσει σε overfitting.

## Κάνουμε κανονικοποίηση των δεδομέένων

Στο βήμα αυτό κάνουμε κανονικοποίηση των χαρακτηριστικών. Χαρακτηριστικά με πολύ μεγάλες διαφορές στις απόλυτες τιμές τους μπορούν να προκαλέσουν προβλήματα στην εκπαίδευση και να δώσουν ταξινομητές με μη βέλτιστη απόδοση. Για παράδειγμα, ένα χαρακτηριστικό με πολύ μεγάλες τιμές θα έχει μεγαλύτερη επίδραση στον υπολογισμό της απόστασης στον kNN από ότι ένα με μικρές τιμές, χωρίς αυτό να σημαίνει απαραίτητα ότι είναι περισσότερο καθοριστικό για το διαχωρισμό των κλάσεων. Η κανονικοποίηση μετασχηματίζει τις τιμές των χαρακτηριστικών ώστε να αμβλυνθούν αυτές οι διαφορές.
"""

from sklearn import preprocessing

# standardization των features του training set
X_train_scaled = preprocessing.scale(train_reduced)
#print(X_train_scaled)
# μέση τιμη και απόκλιση των scaled χαρακτηριστικών
#print(X_train_scaled.mean(axis=0))
#print(X_train_scaled.std(axis=0))

# όριζουμε ένα αντικείμενο scaler και το κάνουμε fit στο train set
scaler = preprocessing.StandardScaler().fit(train_reduced)
# εφαρμόζουμε τον scaler στα δεδομένα test. ΠΡΟΣΟΧΗ μέθοδος transform, όχι fit!
X_test_scaled = scaler.transform(test_reduced)
#print(X_test_scaled)

# και τυπώνουμε τη μέση τιμ;h και απόκλιση του test set 
#print(X_test_scaled.mean(axis=0))
#print(X_test_scaled.std(axis=0))

"""##Εξισορρόπηση data set

Όπως είδαμε, σύμφωνα με το κριτήριο της εκφώνησης, το σύνολο δεδομένων μας είναι μη ισορροπημένο. 

Όπως γνωρίζουμε, έχουμε δύο βασικούς τρόπους για να εξισσοροπούμε ένα dataset, την υποδειγματοληψία (undersampling) και την υπερδειγματοληψία (oversampling). Εν ολίγοις, στο undersampling απλά αφαιρούμε τυχαία δείγματα από όλες τις κατηγορίες που έχουν μεγαλύτερο πλήθος από τη μικρότερη, ενώ στο oversampling επιλέγουμε τυχαία ορισμένα παραδείγματα από τις λιγότερο συχνές κατηγορίες και τα επαναλαμβάνουμε. Στην πρώτη δηλαδή αφαιρούμε δεδομένα ενώ στην άλλη προσθέτουμε. 

Εδώ κάνουμε oversampling αφού δεν χάνουμε δεδομένα εκπαίδευσης και είναι ιδιαίτερα σημαντικό για τον ταξινομητή μας καθώς όπως γνωρίζουμε ο kNN ταξινομεί ένα δείγμα στην κλάση που έχουν οι περισσότεροι από τους k περισσότερους γείτονές του. Στην περίπτωσή μας που το data set είναι μη ισορροπημένο και έχουμε περισσότερα good από bad δεδομένα, είναι πιθανό κάποιο δείγμα που είναι πιο κοντά σε δείγμα της κλάσης bad, να έχει περισσότερους good γείτονες εξαιτίας του πλήθους των δειγμάτων της κλάσης αυτής.
"""

ros = RandomOverSampler(random_state=0)

X_train2,y_train2= ros.fit_sample(X_train_scaled,y_train)

num_of_good= len(np.where(y_train2 == 'g')[0])
num_of_bad= len(np.where(y_train2 == 'b')[0])
print("Size of new training dataset: {}".format(y_train2.shape))
print("Num of goods in training set = {}".format(num_of_good))
print("Num of bads in training set = {}".format(num_of_bad))
print("Num_of_bad/Num_of_good = {}".format(num_of_bad/num_of_good))

print("Final size of training dataset:{}".format(X_train2.shape))

"""## Bέλτιστος kNN

Στο σημείο αυτό, εφαρμόζουμε τα γνωστά μας βήματα ταξινόμησης και γραφικής απεικόνισης των αποτελεσμάτων.

Αρχίκά κάνουμε fit και predict.
"""

clf = KNeighborsClassifier(n_neighbors=2)
clf.fit(X_train2, y_train2)
KNN_predict = clf.predict(X_test_scaled)
print("Predictions for KNN classifier:"+"\n"+" {}".format(KNN_predict ))

"""Δημιουργούμε τον πίνακα σύγχυσης και βλέπουμε ότι έχουμε μόνο 6 λαθος ταξινομημένα δείγματα."""

cm = confusion_matrix(y_test, KNN_predict)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of KNN classifier')

"""Παρακάτω, εφαρμόζουμε τις μετρικές απόδοσης του ταξινομητή."""

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, KNN_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_KNN = list(precision_recall_fscore_support(y_test, KNN_predict, average='micro'))
micro_KNN.pop(3) # tou support
precision_micro_KNN  = micro_KNN [0]
recall_micro_KNN  = micro_KNN [1]
f1_micro_KNN_final  = micro_KNN [2]
print(micro_KNN )

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_KNN  = list(precision_recall_fscore_support(y_test, KNN_predict, average='macro'))
macro_KNN .pop(3) # tou support
precision_macro_KNN  = macro_KNN [0]
recall_macro_KNN  = macro_KNN [1]
f1_macro_KNN_final  = macro_KNN [2]
print(macro_KNN )
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  KNN_predict, target_names=['bad','good'] ,output_dict=True)
print(pd.DataFrame(class_report).T)

"""Η αξιολόγηση του ταξιμομητή γίνεται μέσω του f1_micro και f1_macro."""

print("KNN classifier f1_micro score = {}".format(f1_micro_KNN_final))
print("KNN classifier f1_macro score = {}".format(f1_macro_KNN_final))

"""Και τυπώνουμε τον heatmap με τις μετρικές."""

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""## Σύγκριση ταξινομητών

Βλέπουμε, λοιπόν, ότι ο kNN, που εφαρμόζεται σε δεδομένα που έχουν υποστεί επεξεργασία, έχει καλύτερη απόδοση και ως προς τις δύο μετρικές από τον απλό και αυτόν που έχει βελτιωθεί μόνο ως προς την παράμετρο k και έχει εφαρμοστεί στα δεδομένα με την αρχική τους μορφή.
"""

fig = plt.figure()
x = ['KNN','KNN_optimal','KNN_polu_optimal']
y=[100*f1_micro_KNN,100*f1_micro_KNN_opt,100*f1_micro_KNN_final]
plt.bar(x,y, align='center', width=0.4)
plt.xticks(x,x)
plt.title('F1 micro KNN improvement')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
x = ['KNN','KNN_optimal','KNN_polu_optimal']
y=[100*f1_macro_KNN,100*f1_macro_KNN_opt,100*f1_macro_KNN_final]
plt.bar(x,y, align='center', width=0.4)
plt.xticks(x,x)
plt.title('F1 macro KNN improvement')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

"""Παρακάτω, βλέπουμε ότι ο βέλτιστος ταξινομητής κατάφερε να ξεπεράσει όλους τους άλλους και ως προς τις δύο τεχνικές, ακόμα και τον Gaussian NB και να γίνει ο καλύτερος ταξινομητής."""

fig = plt.figure()
x = ['Dummy_stratified','Gaussian NB','KNN','KNN_optimal','KNN_polu_optimal']
y=[100*f1_micro_dc_stratified,100*f1_micro_GNB,100*f1_micro_KNN,100*f1_micro_KNN_opt,100*f1_micro_KNN_final]
plt.bar(x,y, align='center', width=0.4)
plt.xticks(x,x)
plt.title('F1 micro')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

fig = plt.figure()
x = ['Dummy_stratified','Gaussian NB','KNN','KNN_optimal','KNN_polu_optimal']
y=[100*f1_macro_dc_stratified,100*f1_macro_GNB,100*f1_macro_KNN,100*f1_macro_KNN_opt,100*f1_macro_KNN_final]
plt.bar(x,y, align='center', width=0.4)
plt.xticks(x,x)
plt.title('F1 micro')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

"""#Βήμα 2 BONUS CV στο Scoring

Στο βήμα αυτό, επαναλαμβάνουμε τα στάδια του βήματος 2 (της ταξινόμησης δηλαδή), χωρίς να χωρίζουμε το set σε 80-20% αναλογία, αλλά εφαρμόζοντας 10-fold cross validation, βελτιώνοντας έτσι τις αποδόσεις.

##Dummy

Θα δοκιμάσουμε πρώτα κάποιες πολύ απλές τακτικές ταξινόμησης. Η κλάση DummyClassifier δέχεται μια παράμετρο που καθορίζει την τακτική της ταξινόμησης ως εξής:
* “uniform”: προβλέπει τυχαία και ομοιόμορφα.
* “constant”: προβλέπει πάντα μία κατηγορία που τη διαλέγει ο χρήστης.
* “most_frequent”: προβλέπει πάντα την πιο συχνή κατηγορία στο training set.
* “stratified”: κάνει προβλέψεις διατηρώντας την κατανομή των κλάσεων στο training set.

### Dummy unifrom
"""

dc_uniform = DummyClassifier(strategy="uniform")
model_uniform = dc_uniform.fit(X_train, y_train)
dc_uniform_predict = cross_val_predict(model_uniform, np.concatenate((X_train,X_test), axis=0), 
                           np.concatenate((y_train,y_test), axis=0), cv=KFold(n_splits=10))
print("Predictions for Uniform dummy classifier:"+"\n"+" {}".format(dc_uniform_predict))
print()
scores_uniform_micro = cross_val_score(dc_uniform, np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), 
                         scoring="f1_micro")
scores_uniform_macro = cross_val_score(dc_uniform, np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), 
                         scoring="f1_macro")

print("Uniform dummy classifier CV f1_micro score = %f +-%f" % (100*np.mean(scores_uniform_micro), 100*np.std(scores_uniform_micro)))
print("Uniform dummy classifier CV f1_macro score = %f +-%f" % (100*np.mean(scores_uniform_macro), 100*np.std(scores_uniform_macro)))

cm = confusion_matrix(np.concatenate((y_train,y_test), axis=0), dc_uniform_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of Uniform dummy classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), dc_uniform_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_dc_uniform= list(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), dc_uniform_predict, average='micro'))
micro_dc_uniform .pop(3) # tou support
precision_micro_dc_uniform  = micro_dc_uniform [0]
recall_micro_dc_uniform  = micro_dc_uniform[1]
f1_micro_dc_uniform  = micro_dc_uniform [2]
print(micro_dc_uniform )

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_dc_uniform  = list(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), dc_uniform_predict, average='macro'))
macro_dc_uniform .pop(3) # tou support
precision_macro_dc_uniform  = macro_dc_uniform[0]
recall_macro_dc_uniform  = macro_dc_uniform [1]
f1_macro_dc_uniform = macro_dc_uniform[2]
print(macro_dc_uniform )
print()
print("All the details shown here:")
print()
print(classification_report(np.concatenate((y_train,y_test), axis=0),  dc_uniform_predict, target_names=['bad','good'] ) )

"""### Dummy Constant good """

dc_constant_g = DummyClassifier(strategy="constant", constant='g')
model_constant_g = dc_constant_g.fit(X_train, y_train)
dc_constant_g_predict = dc_constant_g.predict(X_test)
print("Predictions for Constant 'g' dummy classifie:"+"\n"+" {}".format(dc_constant_g_predict))
print()

scores_constant_g_micro = cross_val_score(model_constant_g, np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), 
                         scoring="f1_micro")
scores_constant_g_macro = cross_val_score(model_constant_g, np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), 
                         scoring="f1_macro")

print("Constant 'g' dummy classifie CV f1_micro score = %f +-%f" % (100*np.mean(scores_constant_g_micro), 100*np.std(scores_constant_g_micro)))
print("Constant 'g' dummy classifie CV f1_macro score = %f +-%f" % (100*np.mean(scores_constant_g_macro), 100*np.std(scores_constant_g_macro)))

cm = confusion_matrix(y_test, dc_constant_g_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of Uniform dummy classifier')

matrix = classification_report(y_test, dc_constant_g_predict, target_names=['bad','good'])
print(matrix)
matrix_list = matrix.split()
precision_b_dc_constant_g = matrix_list[5]
recall_b_dc_constant_g =matrix_list[6]
f1_b_dc_constant_g = matrix_list[7]
precision_g_dc_constant_g =matrix_list[10]
recall_g_dc_constant_g =matrix_list[11]
f1_g_dc_constant_g = matrix_list[12]

# plot classification report
visualizer = ClassificationReport(dc_constant_g, classes=['bad','good'], support=True)
visualizer.fit(X_train, y_train)  # Fit the visualizer and the model
visualizer.score(X_test, y_test)  # Evaluate the model on the test data
visualizer.poof();

"""Αναμενόμενο οι τιμές για το bad να βγουν μηδέν επειδή ο ταξινομιτής βάζει όλα τα data ως 'good'

### Dummy Constant Bad
"""

dc_constant_b = DummyClassifier(strategy="constant", constant='b')
model_constant_b = dc_constant_b.fit(X_train, y_train)
dc_constant_b_predict = dc_constant_b.predict(X_test)
print("Predictions for Constant 'b' dummy classifie:"+"\n"+" {}".format(dc_constant_b_predict))
print()

scores_constant_b_micro = cross_val_score(model_constant_b, np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), 
                         scoring="f1_micro")
scores_constant_b_macro = cross_val_score(model_constant_b, np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), 
                         scoring="f1_macro")

print("Constant 'b' dummy classifie CV f1_micro score = %f +-%f" % (100*np.mean(scores_constant_b_micro), 100*np.std(scores_constant_b_micro)))
print("Constant 'b' dummy classifie CV f1_macro score = %f +-%f" % (100*np.mean(scores_constant_b_macro), 100*np.std(scores_constant_b_macro)))

cm = confusion_matrix(y_test, dc_constant_b_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of Uniform dummy classifier')

matrix = classification_report(y_test, dc_constant_b_predict, target_names=['bad','good'])
print(matrix)
matrix_list = matrix.split()
precision_b_dc_constant_b = matrix_list[5]
recall_b_dc_constant_b =matrix_list[6]
f1_b_dc_constant_b = matrix_list[7]
precision_g_dc_constant_b =matrix_list[10]
recall_g_dc_constant_b =matrix_list[11]
f1_g_dc_constant_b = matrix_list[12]

# plot classification report
visualizer = ClassificationReport(dc_constant_b, classes=['bad','good'], support=True)
visualizer.fit(X_train, y_train)  # Fit the visualizer and the model
visualizer.score(X_test, y_test)  # Evaluate the model on the test data
visualizer.poof();

"""###Dummy most frequent"""

dc_most_frequent = DummyClassifier(strategy="most_frequent")
model_most_frequent = dc_most_frequent.fit(X_train, y_train)
dc_most_frequent_predict = dc_most_frequent.predict(X_test)
print("Predictions for Most frequent dummy classifier:"+"\n"+" {}".format(dc_most_frequent_predict))
print()

scores_most_frequent_micro = cross_val_score(model_most_frequent, np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), 
                         scoring="f1_micro")
scores_most_frequent_macro = cross_val_score(model_most_frequent, np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), 
                         scoring="f1_macro")
print("Most frequent dummy classifier CV f1_micro score = %f +-%f" % (100*np.mean(scores_most_frequent_micro), 100*np.std(scores_most_frequent_micro)))
print("Most frequent dummy classifier CV f1_macro score = %f +-%f" % (100*np.mean(scores_most_frequent_macro), 100*np.std(scores_most_frequent_macro)))

cm = confusion_matrix(y_test, dc_most_frequent_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of Uniform dummy classifier')

matrix = classification_report(y_test, dc_most_frequent_predict, target_names=['bad','good'])
print(matrix)
matrix_list = matrix.split()
precision_b_dc_most_frequent = matrix_list[5]
recall_b_dc_most_frequent =matrix_list[6]
f1_b_dc_most_frequent = matrix_list[7]
precision_g_dc_most_frequent =matrix_list[10]
recall_g_dc_most_frequent =matrix_list[11]
f1_g_dc_most_frequent = matrix_list[12]

# plot classification report
visualizer = ClassificationReport(dc_most_frequent, classes=['bad','good'], support=True)
visualizer.fit(X_train, y_train)  # Fit the visualizer and the model
visualizer.score(X_test, y_test)  # Evaluate the model on the test data
visualizer.poof();

"""###Dummy stratified"""

dc_stratified = DummyClassifier(strategy="stratified")
model_stratified = dc_stratified.fit(X_train, y_train)
dc_stratified_predict = dc_stratified.predict(X_test)
print("Predictions for Stratified dummy classifier:"+"\n"+" {}".format(dc_stratified_predict))
print()

scores_most_stratified_micro = cross_val_score(model_stratified , np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), 
                         scoring="f1_micro")
scores_most_stratified_macro = cross_val_score(model_stratified , np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), 
                         scoring="f1_macro")


print("Stratified dummy classifier CV f1_micro score = %f +-%f" % (100*np.mean(scores_most_stratified_micro), 100*np.std(scores_most_stratified_micro)))
print("Stratified dummy classifier CV f1_macro score = %f +-%f" % (100*np.mean(scores_most_stratified_macro), 100*np.std(scores_most_stratified_macro)))

cm = confusion_matrix(y_test, dc_stratified_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of Uniform dummy classifier')

matrix = classification_report(y_test, dc_stratified_predict, target_names=['bad','good'])
print(matrix)
matrix_list = matrix.split()
precision_b_dc_stratified = matrix_list[5]
recall_b_dc_stratified =matrix_list[6]
f1_b_dc_stratified = matrix_list[7]
precision_g_dc_stratified =matrix_list[10]
recall_g_dc_stratified =matrix_list[11]
f1_g_dc_stratified = matrix_list[12]

# plot classification report
visualizer = ClassificationReport(dc_stratified, classes=['bad','good'], support=True)
visualizer.fit(X_train, y_train)  # Fit the visualizer and the model
visualizer.score(X_test, y_test)  # Evaluate the model on the test data
visualizer.poof();

"""##Gaussian Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X_train, y_train)

#GaussianNB_predict = clf.predict(X_test)
GaussianNB_predict = cross_val_predict(clf, np.concatenate((X_train,X_test), axis=0), 
                           np.concatenate((y_train,y_test), axis=0), cv=KFold(n_splits=10))

print("Predictions for Gaussian Naive Bayes classifier:"+"\n"+" {}".format(GaussianNB_predict))
print()

GaussianNB_micro = cross_val_score(clf, np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), scoring="f1_micro")
GaussianNB_macro = cross_val_score(clf, np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), scoring="f1_macro")

print("Gaussian Naive Bayes classifier CV f1_micro score = %f +-%f" % (100*np.mean(GaussianNB_micro), 100*np.std(GaussianNB_micro)))
print("Gaussian Naive Bayes classifier CV f1_macro score = %f +-%f" % (100*np.mean(GaussianNB_macro), 100*np.std(GaussianNB_macro)))

cm = confusion_matrix(np.concatenate((y_train,y_test), axis=0), GaussianNB_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of Gaussian Naive Bayes classifier')

### me ayth thn synarthsh epibabaiwnnw oti evala swsta ta labels
### vlepw ;oti eipa oti 49 data htan good ek twn opoiwn ta 44 htan ontws good
cnt=0
for i,val in enumerate(GaussianNB_predict):
  if (val=='g'):
    cnt+=1
print(cnt)

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), GaussianNB_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_GNB = list(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), GaussianNB_predict, average='micro'))
micro_GNB.pop(3) # tou support
precision_micro_GNB = micro_GNB[0]
recall_micro_GNB = micro_GNB[1]
f1_micro_GNB = micro_GNB[2]
print(micro_GNB)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_GNB = list(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), GaussianNB_predict, average='macro'))
macro_GNB.pop(3) # tou support
precision_macro_GNB = macro_GNB[0]
recall_macro_GNB = macro_GNB[1]
f1_macro_GNB = macro_GNB[2]
print(macro_GNB)
print()
print("All the details shown here:")
print()
print(classification_report(np.concatenate((y_train,y_test), axis=0), GaussianNB_predict, target_names=['bad','good'] ) )

"""##kΝΝ"""

clf = KNeighborsClassifier(n_neighbors=1)
clf.fit(X_train, y_train)


#KNN_predict = clf.predict(X_test)
KNN_predict = cross_val_predict(clf, np.concatenate((X_train,X_test), axis=0), 
                           np.concatenate((y_train,y_test), axis=0), cv=KFold(n_splits=10))
print("Predictions for KNN classifier:"+"\n"+" {}".format(KNN_predict ))
print()

KNN_micro = cross_val_score(clf, np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), 
                         scoring="f1_micro")
KNN_macro = cross_val_score(clf, np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), 
                         scoring="f1_macro")

print("KNN classifier CV f1_micro score = %f +-%f" % (100*np.mean(KNN_micro), 100*np.std(KNN_micro)))
print("KNN classifier CV f1_macro score = %f +-%f" % (100*np.mean(KNN_macro), 100*np.std(KNN_macro)))

cm = confusion_matrix(np.concatenate((y_train,y_test), axis=0), KNN_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of KNN classifier')

clf.predict(X_test)
cnt=0
for i,val in enumerate(KNN_predict):
  if (val=='g'):
    cnt+=1
print(cnt)

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), KNN_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_KNN = list(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), KNN_predict, average='micro'))
micro_KNN .pop(3) # tou support
precision_micro_KNN  = micro_KNN [0]
recall_micro_KNN  = micro_KNN [1]
f1_micro_KNN  = micro_KNN [2]
print(micro_KNN )

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_KNN  = list(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), KNN_predict, average='macro'))
macro_KNN .pop(3) # tou support
precision_macro_KNN  = macro_KNN [0]
recall_macro_KNN  = macro_KNN [1]
f1_macro_KNN  = macro_KNN [2]
print(macro_KNN )
print()
print("All the details shown here:")
print()
print(classification_report(np.concatenate((y_train,y_test), axis=0),  KNN_predict, target_names=['bad','good'] ) )

"""##Σύγκριση Ταξινομητών

Αν θεωρήσουμε το good ώς possitive,δηλαδή ως αυτό που θέλουμε να εντωπίσουμε έχουμε:
"""

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
x = ['Dummy_uniform','Gaussian NB','KNN',]
y=[100*f1_micro_dc_uniform,100*f1_micro_GNB,100*f1_micro_KNN]
plt.bar(x,y, align='center', width=0.4)
plt.xticks(x,x)
plt.title('F1 micro')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
x = ['Dummy_uniform','Gaussian NB','KNN',]
y=[100*f1_macro_dc_uniform,100*f1_macro_GNB,100*f1_macro_KNN]
plt.bar(x,y, align='center', width=0.4)
plt.xticks(x,x)
plt.title('F1 macro')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

"""##Βελτιστοποίηση ταξινομητών"""

# Commented out IPython magic to ensure Python compatibility.
param_grid = {'n_neighbors':[1,2,3,4,5,6,7,8,9,10]} 

grid = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True,  cv = 10, n_jobs = -1,scoring="f1_micro")
# %time grid.fit(X_train, y_train)  # austira to train set!
print("Best parameters regarding f1_micro is = " +str(grid.best_params_))
print()
grid = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True,  cv = 10, n_jobs = -1,scoring="f1_macro")
# %time grid.fit(X_train, y_train)  # austira to train set!
print("Best parameters regarding f1_macro is = " +str(grid.best_params_))

param_range = [1,2,3,4,5,6,7,8,9,10]
train_scores, test_scores = validation_curve(
    KNeighborsClassifier(), X_train, y_train,  param_name="n_neighbors", param_range=param_range,
    scoring="f1_micro", n_jobs=-1,cv = 10)

train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

plt.title("Validation Curve with KNN scroning f1_micro")
plt.xlabel("n_neighbors")
plt.ylabel("f1_micro Score")
plt.ylim(0.0, 1.1)
lw = 2
plt.semilogx(param_range, train_scores_mean, label="Training score",
             color="darkorange", lw=lw)
plt.fill_between(param_range, train_scores_mean - train_scores_std,
                 train_scores_mean + train_scores_std, alpha=0.2,
                 color="darkorange", lw=lw)
plt.semilogx(param_range, test_scores_mean, label="Cross-validation score",
             color="navy", lw=lw)
plt.fill_between(param_range, test_scores_mean - test_scores_std,
                 test_scores_mean + test_scores_std, alpha=0.2,
                 color="navy", lw=lw)
plt.legend(loc="best")
plt.show()

param_range = [1,2,3,4,5,6,7,8,9,10]
train_scores, test_scores = validation_curve(
    KNeighborsClassifier(), X_train, y_train,  param_name="n_neighbors", param_range=param_range,
    scoring="f1_macro", n_jobs=-1,cv = 10)

train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

plt.title("Validation Curve with KNN scroning f1_macro")
plt.xlabel("n_neighbors")
plt.ylabel("f1_macro Score")
plt.ylim(0.0, 1.1)
lw = 2
plt.semilogx(param_range, train_scores_mean, label="Training score",
             color="darkorange", lw=lw)
plt.fill_between(param_range, train_scores_mean - train_scores_std,
                 train_scores_mean + train_scores_std, alpha=0.2,
                 color="darkorange", lw=lw)
plt.semilogx(param_range, test_scores_mean, label="Cross-validation score",
             color="navy", lw=lw)
plt.fill_between(param_range, test_scores_mean - test_scores_std,
                 test_scores_mean + test_scores_std, alpha=0.2,
                 color="navy", lw=lw)
plt.legend(loc="best")
plt.show()

def plot_learning_curve(train_scores, test_scores, train_sizes,title):
    plt.figure()
    plt.grid()
    plt.title(title)
    plt.xlabel("Training examples")
    plt.ylabel("Score")

    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",label="Cross-validation score")
    plt.legend(loc="best")
    return

train_sizes, train_scores, test_scores = learning_curve(KNeighborsClassifier(n_neighbors=2), 
                                              X_train, y_train, train_sizes = np.linspace(0.1, 1.0, 5), 
                                              cv = 10,n_jobs = -1,scoring="f1_micro")
plot_learning_curve(train_scores, test_scores, train_sizes,"Learning Curve scoring f1_micro")

train_sizes, train_scores, test_scores = learning_curve(KNeighborsClassifier(n_neighbors=2), 
                                              X_train, y_train, train_sizes = np.linspace(0.1, 1.0, 5), 
                                              cv = 10,n_jobs = -1,scoring="f1_macro")
plot_learning_curve(train_scores, test_scores, train_sizes,"Learning Curve scoring f1_macro")

"""Αρα ειναι underfited μοντέλο, λογικό αφού τα δεδομένα είναι πολύ λίγα.

Άρα ο καλύτερος είναι γαι ν=2
"""

clf_opt = KNeighborsClassifier(n_neighbors=2)
clf_opt .fit(X_train, y_train)


#KNN_predict = clf.predict(X_test)
KNN_predict_opt = cross_val_predict(clf_opt, np.concatenate((X_train,X_test), axis=0), 
                           np.concatenate((y_train,y_test), axis=0), cv=KFold(n_splits=10))
print("Predictions for KNN classifier:"+"\n"+" {}".format(KNN_predict ))
print()

KNN_micro_opt = cross_val_score(clf_opt, np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), 
                         scoring="f1_micro")
KNN_macro_opt = cross_val_score(clf_opt, np.concatenate((X_train,X_test), axis=0), np.concatenate((y_train,y_test), axis=0), 
                         cv=KFold(n_splits=10), 
                         scoring="f1_macro")

print("KNN classifier CV f1_micro score = %f +-%f" % (100*np.mean(KNN_micro_opt), 100*np.std(KNN_micro_opt)))
print("KNN classifier CV f1_macro score = %f +-%f" % (100*np.mean(KNN_macro_opt), 100*np.std(KNN_macro_opt)))

cm = confusion_matrix(np.concatenate((y_train,y_test), axis=0), KNN_predict_opt)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['bad', 'good'],title='Confusion matrix of KNN classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), KNN_predict_opt, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_KNN_opt = list(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), KNN_predict_opt, average='micro'))
micro_KNN_opt.pop(3) # tou support
precision_micro_KNN_opt  = micro_KNN_opt [0]
recall_micro_KNN_opt  = micro_KNN_opt [1]
f1_micro_KNN_opt  = micro_KNN_opt [2]
print(micro_KNN_opt )

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_KNN_opt  = list(precision_recall_fscore_support(np.concatenate((y_train,y_test), axis=0), KNN_predict_opt, average='macro'))
macro_KNN_opt.pop(3) # tou support
precision_macro_KNN_opt = macro_KNN_opt [0]
recall_macro_KNN_opt  = macro_KNN_opt [1]
f1_macro_KNN_opt  = macro_KNN_opt [2]
print(macro_KNN_opt)
print()
print("All the details shown here:")
print()
print(classification_report(np.concatenate((y_train,y_test), axis=0),  KNN_predict_opt, target_names=['bad','good'] ) )

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
x = ['Dummy_uniform','Gaussian NB','KNN','KNN_opt']
y=[100*f1_micro_dc_uniform,100*f1_micro_GNB,100*f1_micro_KNN,100*f1_micro_KNN_opt]
plt.bar(x,y, align='center', width=0.4)
plt.xticks(x,x)
plt.title('F1 micro')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
x = ['Dummy_uniform','Gaussian NB','KNN','KNN_opt']
y=[100*f1_macro_dc_uniform,100*f1_macro_GNB,100*f1_macro_KNN,100*f1_macro_KNN_opt]
plt.bar(x,y, align='center', width=0.4)
plt.xticks(x,x)
plt.title('F1 macro')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

"""#Μέρος 2ο

##Πληροφορίες Epileptic Seizure Recognition DataSet

Το αρχικό Data Set αποτελείται από 5 διαφορετικούς φακέλους, που ο καθένας περιέχει 100 αρχεία, με κάθε αρχείο να περιγράφει ένα άτομο. Κάθε αρχείο περιέχει μια καταγραφή εγκεφαλικής δραστηριότητας, 23.6 δευτερολέπτων. Η αντίστοιχη χρονική ακολουθία έχει δειγματοληπτηθεί σε 4097 σημεία. Κάθε σημείο αντιστοιχεί στην τιμή της ΕΕG καταγραφής (ηλεκτροεγκεφαλογράφημα) σε μία χρονική στιγμή. Οπότε έχουμε συνολικά 500 εξεταζόμενους που στον καθένα αντιστοιχίζονται 4097 σημεία δεδομένων που ελήφθησαν σε χρονική διάρκεια 23.5 sec.

To αρχικό αυτό Data Set χωρίστηκε και ανακατεύθηκε, ώστε κάθε 4097 δεδομένα να χωριστούν σε 23 κομμάτια, όπου κάθε κομμάτι περιέχει 178 σημεία για ένα δευτερόλεπτο και κάθε δεδομένο αντιστοιχεί στην τιμή του EEG μια δεδομένη χρονική στιγμή. Οπότε έχουμε 23*500 = 11500 κομμάτια πληροφορίας(γραμμή) και κάθε μία περιέχει 178 δεδομένα που αφορούν ένα δευτερόλεπτο(στήλες). Η τελευταία στήλη (179) αναπαριστά το label της κατάστασης {1,2,3,4,5} του ατόμου που περιγράφεται από ένα κομμάτι πληροφορίας. Συγκεκριμένα κάθε κλάση αντιστοιχεί στην εξής κατάσταση:

1. Επιληπτική δραστηριότητα.
2. Περιοχή με παρουσία όγκου και καταγραφή EEG από εκείνη την περιοχή
3. Παρουσία όγκου αλλά καταγραφή EEG από υγιή περιοχή.
4. Κλειστά μάτια κατά την καταγραφή.
5. Ανοιχτά μάτια κατά την καταγραφή.

Αν και έχουμε 5 κλάσεις, μόνο η κλάση 1 περιγράφει επιληπτικό φαινόμενο, ενώ οι 2,3,4,5 όχι. Όπως αναφέρεται, οι περισσότεροι ερευνητές πραγματοποιούν binary classification στο Data Set αυτό με κλάση "Epileptic Seizure" την κλάση 1 και "Non Epileptic Seizure" τις κλάσεις 2,3,4,5. Έτσι θα αντιμετωπίσουμε κι εμείς το εν λόγω Data Set οπότε πολλά βήματα της επεξεργασίας του θα γίνουν με ανάλογο τρόπο με το μικρό Data Set, Ionosphere Data.

*Το αρχείο έχει τη μορφή csv*

#Βήμα 1

>Θα εισάγουμε τα δεδομένα σε dataframes απευθείας από τα αρχεία που βρίσκονται αποθηκευμένα στο drive μας.
"""

df1 = pd.read_csv("/content/drive/My Drive/Neural_Networks/lab1/big/data.csv", sep=" ", header=None)

df1

"""Επομένως βλέπουμε ότι τα δεδομένα μας δεν είναι χωρισμένα σε κολόνες, όπως θα περιμέναμε, για να γίνεται πιο κατανοητή η αναπαράσταση του διανύσματος, οπότε θα κάνουμε κατάλληλη μετατροπή των δεδομένων διαχωρίζοντας τα χαρακτηριστικά σε στήλες (**ερώτημα 5**). Επίσης, από την παραπάνω μορφή, παρατηρούμε ότι  υπάρχει τόσο αρίθμηση γραμμών όσο επικεφαλίδες στα δείγματα (**ερώτημα 3**)"""

mylist=[]
for i in range(0,11501):
  mylist.append(df1.values.tolist()[i][0].split(','))
'''
labels = []
for i in range(0,34):
  labels.append("Feature_"+str(i))
labels.append("Label")
'''

labels = []
for i in range(0,179):
  labels.append("X"+str(i))
labels.append("Label")

#df = pd.DataFrame(mylist, columns = labels)
df = pd.DataFrame(mylist, columns = labels)
df

"""Έτσι παρατηρούμε ότι τα features είναι διακριτά αριθμητικά και τα labels των class που ανήκουν τα δεδομένα είναι διατεταγμένα(αν θεωρήσουμε το είδος της κλάσης ως χαρακτηριστικό και λάβουμε υπόψιν ότι η σοβαρότητα της πάθησης αυξάνεται κατά κάποιο τρόπο όσο αυξάνεται το νούμερο που της αντιστοιχεί). Με την προηγούμενη ανάλυση βλέπουμε ότι έχουμε 11500 δείγματα και οι κολώνες 1-178 περιέχουν τα features (178 features)(**ερώτημα 2**) ενώ η τελευταία κολόνα δηλαδή η 180 περιέχει το όνομα τον αριθμό της κλάσης στην οποία ανήκει το κάθε δεδομένο(έχουμε 5 κλάσεις: 1,2,3,4,5)(**ερώτημα 4**).

Η πρώτη γραμμή(0) και η πρώτη κολώνα(0) έχουν πληροφορίες μη χρήσιμες για την ταξινόμηση, τις οποίες και αφαιρούμε παρακάτω.

Στο data set εν υπάρχουν απουσιάζουσες τιμές(**ερώτημα 6**)
"""

df.drop(labels="X0",axis=1,inplace=True)
df.drop(labels=0,axis=0,inplace=True)
df

"""##y

Με την παραπάνω μετατροπή έχουμε πλέον 11500 δείγματα που το καθένα περιγράφεται από 178 χαρακτηριστικά και στην τελευταία κολόνα έχουμε την κλάση.
"""

lala =df.values
print((lala).shape)
int(lala[11499][1])

numpy_arr = df.values 
class_list=[]
for i in range(0,11499):
  class_list.append(int(numpy_arr[i][178]))

y_df = pd.DataFrame(class_list,columns = ["Class"])
y_df

"""Παραπάνω βλέπουμε την στήλη με τις 5 κλάσεις. Τώρα θα κάνουμε το dataset binary όπως οι περισσότεροι ερευνητες (η κλάση 1 παραμένει 1 και αφορά την επιληπτική δραστηριότητα και η κλάσεις 2,3,4,5 αντιστοιχίζονται στην κλάση 0 που αφορά την μη επιληπτική δραστηριότητα)."""

binary_class_list = []
for i in class_list:
  if (class_list[i]!=1):
    binary_class_list.append(0) # an den exei epilipsia
  else:
    binary_class_list.append(1) # an exei

y_df = pd.DataFrame(binary_class_list,columns = ["Class"])
y_df

zeros=0
ones=0
for i in binary_class_list:
  if (binary_class_list[i]==0):
    zeros+=1
  else:
    ones+=1

"""Παρακάτω μετράμε το ποσοστό των δειγμάτων που αφορούν επιληπτική φάση(κλάση 1) και αυτό που αφορά μη επιληπτική(κλάση 0). Παρατηρούμε ότι η αναλογία είναι 20-80% δηλαδή το Data Set είναι αρκετά μη ισορροπημένο(**ερώτημα 7**)."""

zeros=0
ones=0
for i in binary_class_list:
  if (binary_class_list[i]==0):
    zeros+=1
  else:
    ones+=1
print("Percentage of data labeled as Epileptic seizure: {:.3f}".format(ones/11499))
print("Percentage of data labeled as not Epileptic seizure: {:.3f}".format(zeros/11499))

"""Αναπαριστούμε γραφικά την αναλογία αυτή σε πλήθος δειγμάτων και ποσοστό."""

priors = [zeros, ones]

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
x = ['Not Epileptic seizure', 'Epileptic seizure']
plt.bar(x, priors)
plt.grid()
plt.xticks(x,x)
plt.title('Data classified in each group')
plt.xlabel('Class')
plt.ylabel('Number')
plt.grid()
plt.show()

priors = [zeros/11499, ones/11499]

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
x = ['Not Epileptic seizure', 'Epileptic seizure']
plt.bar(x, priors)
plt.grid()
plt.xticks(x,x)
plt.title('Percentage of classified in each group')
plt.xlabel('Class')
plt.ylabel('Percentage')
plt.grid()
plt.show()

"""## X"""

numpy_arr = df.values 
class_list=[]
for i in range(0,11499):
  row_list=[]
  for j in range (0,178):
    row_list.append(int(numpy_arr[i][j]))
  class_list.append(row_list)

feature_names_list = []
for i in range(1,179):
  feature_names_list .append("X"+str(i))

X_df = pd.DataFrame(class_list,columns = feature_names_list )
X_df

"""Παρακάτω κάνουμε στατιστική ανάλυση των features μέσω της describe() μεθόδου, έχοντας πρώτα αποκλείσει την κλάση από την αναπαράστασή τους."""

A = X_df.describe()
A

"""Επειδή το μέγεθος είναι πολύ μεγάλο βλέπουμε αν υπάρχουν feature με μηδενικό std."""

std = A.values[2]
idiot_feature=[]
for i,val in enumerate(std):
  if (val==0):
    idiot_feature.append(i)
print(idiot_feature)

"""Άρα δεν υπάρχουν feature με μηδενικό std

##split

Και στην συνέχεια διαχωρίζουμε training και testing (**ερώτημα 8**) δεδομένα σε ποσοστό 70%-30%, όπως μας ζητείται, άρα έχουμε 8049 δείγματα στο train set και 3450 στο test set.
"""

X = X_df.values
y = y_df.values.ravel()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=17)

print("Training feature size {} with type {} and training labels size {} with type {} ".format(X_train.shape, type(X_train), y_train.shape,type(y_train)))

print("Testing feature size {} with type {} and testing labels size {} with type {} ".format(X_test.shape, type(X_test), y_test.shape,type(y_test)))

"""Οπτικοποιούμε το Data Set στις δύο διαστάσεις με τη βοήθεια του PCA."""

pca = PCA(n_components=2)
pca_test  = pca.fit_transform(X_train)
print("The pca shape is: {}".format(pca_test.shape))
print()
print("The first component contains {:.2f}% of the total information and the second one contains {:.2f}%".format( (100*pca.explained_variance_ratio_[0]),(100*pca.explained_variance_ratio_[1]) ) )
print("In total the 2 components contains {:.2f}% of the total information".format((100*pca.explained_variance_ratio_[0])+(100*pca.explained_variance_ratio_[1])) )
print()

fig = plt.figure(figsize = (5,5))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel("PCA componenct 1")
ax.set_ylabel("PCA componenct 2")
ax.set_title("PCA Analysis")

classes = [0,1]
colors = ['black', 'pink']

for i, color in zip(classes, colors):
  index = np.where(y_train == i)
  first_component = []
  second_component = []
  for i in index[0]:
    first_component.append(pca_test [i][0])
    second_component.append(pca_test [i][1])
  ax.scatter(first_component, second_component, c = color, s = 10,alpha=0.2)
ax.legend(['Not Epileptic seizure', 'Epileptic seizure'])
ax.grid()

"""Μπορούμε να δούμε ότι το PCA μας κρατάει συνολικά περίπου το 11% της συνολικής πληροφορίας και η υπόλοιπη "χανεται" λόγο της μείωσης διαστατικότητας. Έτσι, μπορούμε να παρατηρήσουμε ότι τα δεδομένα που έχουν κατηγοριοποίηση στην κλάση 'Not Epileptic seizure' βρίσκονται κυρίως στη μέση.
Δηλαδή θα μπορούσαμε να κάναμε ένα πολυ απλό ταξινομητή ο οποίος θα έλεγε ότι αν μια μέτρηση είναι μακριά του κέντρου, τότε έχει 'Epileptic seizure', αν είναι όμως στο κέντρο δεν μπορούμε να καταλάβουμε καθώς εκεί βρίσκονταί τόσο Not Epileptic seizure και  Epileptic seizure.

Είναι ισοζυγισμένο;
"""

num_of_Not_Epileptic_seizure= len(np.where(y_train == 0)[0])
num_of_Epileptic_seizure= len(np.where(y_train == 1)[0])

print("Num of Not Epileptic seizure in training set = {}".format(num_of_Not_Epileptic_seizure))
print("Num of Epileptic seizure in training set = {}".format(num_of_Epileptic_seizure))
print("Νum_of_Not_Epileptic_seizure/num_of_Epileptic_seizure = {}".format(num_of_Not_Epileptic_seizure/num_of_Epileptic_seizure))

"""Άρα το training set είναι αρκετά μη ισοροπημένο, όπως προείπαμε για ολοόκληρο το dataset και θα προχωρήσουμε στην εξισορρόπηση του.

Επίσης είναι μη κανονικοποιημένο και επομένως θα το κανικοποιησουμε πρώτα.
"""

# standardization των features του training set
X_train_scaled = preprocessing.scale(X_train)
#print(X_train_scaled)
# μέση τιμη και απόκλιση των scaled χαρακτηριστικών
#print(X_train_scaled.mean(axis=0))
#print(X_train_scaled.std(axis=0))

# όριζουμε ένα αντικείμενο scaler και το κάνουμε fit στο train set
scaler = preprocessing.StandardScaler().fit(X_train)
# εφαρμόζουμε τον scaler στα δεδομένα test. ΠΡΟΣΟΧΗ μέθοδος transform, όχι fit!
X_test = scaler.transform(X_test)
#print(X_test_scaled)

# και τυπώνουμε τη μέση τιμ;h και απόκλιση του test set 
#print(X_test_scaled.mean(axis=0))
#print(X_test_scaled.std(axis=0))
print(X_train_scaled)

ros = RandomOverSampler(random_state=0)

X_train,y_train= ros.fit_sample(X_train_scaled,y_train)
#X_train,y_train= ros.fit_sample(X_train,y_train)

num_of_Not_Epileptic_seizure= len(np.where(y_train == 0)[0])
num_of_Epileptic_seizure= len(np.where(y_train == 1)[0])
print("Size of new training dataset: {}".format(y_train.shape))
print("Num of Not Epileptic seizure in training set = {}".format(num_of_Not_Epileptic_seizure))
print("Num of Epileptic seizure in training set = {}".format(num_of_Epileptic_seizure))
print("num_of_Epileptic_seizure/num_of_Not_Epileptic_seizure = {}".format(num_of_Epileptic_seizure/num_of_Not_Epileptic_seizure))

X_train.shape
x=[]
cnt=0
for i,val1 in enumerate(X_train):
  for j,val2 in enumerate(X_train):
    if (np.array_equal(val1,val2) and i!=j):
      x.append("("+str(i)+","+str(j)+")")
      cnt+=1
  if (cnt>3):break
print(x)

"""Δεν κανουμε  σε αυτό το βήμα PCA και Variance Thresholding γιατί θεωρούνται υπερπαράμετροι.

Έχοντας αναλύσει τα ερωτήματα και παραπάνω, τα συγκεντρώνουμε περιληπτικά εδώ:


1.   Οι πληροφορίες για το data set δίνονται στην παράγραφο **Πληροφορίες Epileptic Seizure DataSet**
2.  Το πλήθος των δειγμάτων είναι 11500 και ο αριθμός των χαρακτηριστικών 178. Είναι διακριτά αριθμητικά χαρακτηριστικά και δεν υπάρχουν μη διατεταγμένα χαρακτηριστικά δεδομένου ότι η κλάση δεν είναι χαρακτηριστικό(αν την λάβουμε υπόψιν μπορούμε να θεωρήσουμε ότι είναι διατεταγμένο γιατί όσο αυξάνεται η αρίθμηση της κλάσης αυξάνεται κατά κάποιο τρόπο και η σοβαρότητα της κατάστασης).
3.  Υπάρχει και αρίθμηση γραμμών και επικεφαλίδες.
4.  Οι ετικέτες των κλάσεων είναι 1,2,3,4,5 και βρίσκονται στην 179 κολόνα(με αρίθμηση από το 0) κολόνα. Μετά από την επεξεργασία έχουμε δύο ετικέτες κλάσεων την 0(κλάσεις 2,3,4,5) και την 1(κλάση 1) ανάλογα με το αν έχουμε επιληπτικό ή όχι, άτομο. Επίσης αφαιρόντας τον κωδικό των γραμμών, οι στήλες γίνονται 179 σε πλήθος και οι ετικέτες βρίσκονται στην στήλη 178(με αρίθμηση από το 0).
5.  Διαβάσαμε το αρχείο, χωρίσαμε τα χαρακτηριστικά σε κολόνες, ονοματοδοτώντας την κάθε κολόνα που αντιστοιχεί σε κάθε χαρακτηριστικό και αφαιρώντας τις επικεφαλίδες και τους αριθμούς.
6.  Από την παραπάνω ανάλυση παρατηρήσαμε ότι δεν υπάρχουν απουσιάζουσες τιμές στο συγκεκριμένο data set
7.  Ο αριμός των κλάσεων είναι 2(μετά την μετατροπή σε binary, αρχικά ήταν 5) και το ποσοτό των 1(επιληπτικών) είναι 20% και των 0(μη επιληπτικών) 80%, θεωρώντας ως μη ισορροπημένο ένα dataset στο οποίο η μια κλάση είναι 1.5 φορά πιο συχνή από την άλλη (δηλαδή 60-40 αναλογία) προκύπτει ότι το εν λόγω data set είναι μη ισορροπημένο.
8.  Στο κομμάτι **split**, χωρίσαμε το data set σε training και testing δεδομένα, με αναλογία 70-30%, δηλαδή 8049 δεδομένα για training και 3450 για testing.

#Βήμα 2: Ταξινόνηση

Στο βήμα αυτό θα εφαρμόσουμε classification στο data set, χρησιμοποιώντας ως ταξινομητές τον Dummy Stratified, τον kNN, τον Multi-Layer Perceptron(MLP) και τον Support Vector Machines(SVM), χρησιμοποιώντας την default αρχικοποίησή τους.
"""

def plot_confusion_matrix(cm, classes,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('Predicted label')
    plt.xlabel('True label')
    plt.tight_layout()  
    plt.show()

"""Ακολουθούμε την συνήθη επεξεργασία στο βήμα αυτό, όπως και στο μικρό Data Set: fit, predict, confusion matrix, classification_report, f1_micro, f1_macro και heatmap.

### Dummy stratified

Από τους Dummy ταξινομητές, τους οποίους υλοποιήσαμε όλους στο μικρό Data Set, επιλέγουμε εδώ τον Dummy Stratified, ο οποίος κάνει προβλέψεις διατηρώντας την κατανομή στο training set και μαζί με τον umiform, κάνει την λιγότερο "dummy" ταξινόμηση από τους dummy ταξινομητές.
"""

clf = DummyClassifier(strategy="stratified")
clf.fit(X_train, y_train)
dc_stratified_predict = clf.predict(X_test)
print("Predictions for Dummy stratified Classifier:"+"\n"+" {}".format(dc_stratified_predict))

cm = confusion_matrix(y_test, dc_stratified_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['Not Epileptic seizure', 'Epileptic seizure'],title='Confusion matrix of Dummy stratified Classifier')

clf.predict(X_test)
cnt=0
for i,val in enumerate(dc_stratified_predict):
  if (val==0):
    cnt+=1
print(cnt)

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, dc_stratified_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_dc_stratified = list(precision_recall_fscore_support(y_test, dc_stratified_predict, average='micro'))
micro_dc_stratified.pop(3) # tou support
precision_micro_dc_stratified = micro_dc_stratified[0]
recall_micro_dc_stratified = micro_dc_stratified[1]
f1_micro_dc_stratified = micro_dc_stratified[2]
print(micro_dc_stratified)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_dc_stratified = list(precision_recall_fscore_support(y_test, dc_stratified_predict, average='macro'))
macro_dc_stratified.pop(3) # tou support
precision_macro_dc_stratified = macro_dc_stratified[0]
recall_macro_dc_stratified = macro_dc_stratified[1]
f1_macro_dc_stratified = macro_dc_stratified[2]
print(macro_dc_stratified)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  dc_stratified_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'] ,output_dict=True)
print(pd.DataFrame(class_report).T)

print("Dummy stratified Classifier f1_micro score = {}".format(f1_micro_dc_stratified))
print("Dummy stratified Classifier  f1_macro score = {}".format(f1_macro_dc_stratified))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""Από τα παραπάνω φαίνεται πως ο Dummy Stratified δεν δίνει καλά αποτελέσματα ταξινόμησης, όπως και περιμέναμε, καθώς είναι dummy αλγόριθμος ταξινόμησης.

###Gaussian NB
"""

clf = GaussianNB()
clf.fit(X_train,y_train)
GaussianNB_predict = clf.predict(X_test)
print("Predictions for KNN classifier:"+"\n"+" {}".format(GaussianNB_predict))

cm = confusion_matrix(y_test,GaussianNB_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['Not Epileptic seizure', 'Epileptic seizure'] ,title='Confusion matrix of Gaussian Naive Bayes classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, GaussianNB_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_GNB = list(precision_recall_fscore_support(y_test, GaussianNB_predict, average='micro'))
micro_GNB.pop(3) # tou support
precision_micro_GNB = micro_GNB[0]
recall_micro_GNB = micro_GNB[1]
f1_micro_GNB = micro_GNB[2]
print(micro_GNB)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_GNB = list(precision_recall_fscore_support(y_test, GaussianNB_predict, average='macro'))
macro_GNB.pop(3) # tou support
precision_macro_GNB = macro_GNB[0]
recall_macro_GNB = macro_GNB[1]
f1_macro_GNB = macro_GNB[2]
print(macro_GNB)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  GaussianNB_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'] ,output_dict=True)
print(pd.DataFrame(class_report).T)

print("Gaussian Naive Bayes classifier f1_micro score = {}".format(f1_micro_GNB))
print("Gaussian Naive Bayes classifier f1_macro score = {}".format(f1_macro_GNB))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""Από τα παραπάνω φαίνεται πως ο Gaussian Naive Bayes έχει πολύ καλή απόδοση στο συγκεκριμένο data set. Τυχούσες αστοχίες του οφείλονται και στο γεγονός ότι ο Gaussian θεωρεί ότι τα χαρακτηριστικά είναι ανεξάρτητα μεταξύ τους, το οποίο δεν μπορούμε να πούμε με ασφάλεια ότι ισχύει σε αυτό το Data Set, καθώς πρόκειται για τιμές καταγραφής ηλεκτροεγκεφαλογραφήματος σε διαφορετικές στιγμές που δεν μπορούμε να πούμε ότι μια τιμή σε μια συγκεκριμένη χρονική στιγμή δεν επηρεάζει καθόλου την επόμενη.

###KNN

Για τον kNN, τον οποίο και έχουμε αναλύσει στο προηγούμενο data set, χρησιμοποιούμε το default πλήθος γειτόνων αλλά καθώς αργότερα θα θεωρήσουμε ως υπερπαραμέτρους και το "weight" και "metric", χρήσιμο είναι να τις περιγράψουμε συνοπτικά:

*   weight: συνάρτηση βάρους που χρησιμοποιείται στο prediction. H default τιμή είναι η uniform, δηλαδή σε κάθε σημείο στη γειτονιά που δημιουργείται σε κάθε φάση, αποδίδεται το ίδιο βάρος.
*   metric: η μετρική αποστάσεων που χρησιμοποιείται. Η default τιμή είναι η minkowski(για p=1 ταυτίζεται με την manhattan και για p=2 με την euclidean).
"""

# Commented out IPython magic to ensure Python compatibility.
clf1 = KNeighborsClassifier(n_neighbors=5)
# %time clf1.fit(X_train, y_train)
# %time KNN_predict = clf1.predict(X_test)
print("Predictions for KNN classifier:"+"\n"+" {}".format(KNN_predict ))

cm = confusion_matrix(y_test, KNN_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['Not Epileptic seizure', 'Epileptic seizure'] ,title='Confusion matrix of KNN classifier')

clf1.predict(X_test)
cnt=0
for i,val in enumerate(KNN_predict):
  if (val==0):
    cnt+=1
print(cnt)

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, KNN_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_KNN = list(precision_recall_fscore_support(y_test, KNN_predict, average='micro'))
micro_KNN.pop(3) # tou support
precision_micro_KNN  = micro_KNN [0]
recall_micro_KNN  = micro_KNN [1]
f1_micro_KNN  = micro_KNN [2]
print(micro_KNN )

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_KNN  = list(precision_recall_fscore_support(y_test, KNN_predict, average='macro'))
macro_KNN .pop(3) # tou support
precision_macro_KNN  = macro_KNN [0]
recall_macro_KNN  = macro_KNN [1]
f1_macro_KNN  = macro_KNN [2]
print(macro_KNN )
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  KNN_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'],output_dict=True)
print(pd.DataFrame(class_report).T)

print("KNN classifier f1_micro score = {}".format(f1_micro_KNN))
print("KNN classifier f1_macro score = {}".format(f1_macro_KNN))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""Παρατηρούμε ότι και εδώ έχουμε πολύ καλή απόδοση και αυτό είναι λογικό, αφού οι default τιμές είναι μελετημένες ώστε να παράγουν καλά αποτελέσματα στη γενική περίπτωση.

###SVM

Κάνουμε μια σύντομη ποιοτική περιγραφή του SVM.

Πρόκειται για έναν μη παραμετρικό, διαχωριστικό ταξινομητή που επιδιώκει να βρει μια ευθεία ή καμπύλη(σε δύο διαστάσεις) ή πολύπτυχο (manifold) σε περισσότερες τα οποία να διαχωρίζουν / διακρίνουν τις κατηγορίες μεταξύ τους. 

Πλεονεκτήματα:

*  Η εξάρτησή τους από σχετικά λίγα διανύσματα υποστήριξης σημαίνει ότι είναι πολύ συμπαγή μοντέλα και καταλαμβάνουν πολύ λίγη μνήμη.
*  Εφόσον εκπαιδευτεί το μοντέλο, η φάση πρόβλεψης είναι πολύ γρήγορη. 
* Επειδή επηρεάζονται μόνο από σημεία κοντά στο περιθώριο, λειτουργούν καλά με δεδομένα υψηλών διαστάσεων - ακόμη και δεδομένα με περισσότερες διαστάσεις από ότι δείγματα, κάτι που αποτελεί πρόκληση για άλλους αλγόριθμους.
* Η ενσωμάτωσή τους με μεθόδους πυρήνα τα καθιστά πολύ ευέλικτα, ικανά να προσαρμοστούν σε πολλούς τύπους δεδομένων.

Μειονεκτήματα:
* Η κλιμάκωση με τον αριθμό των δειγμάτων 𝑁 είναι στη χειρότερη $𝑂[𝑁^3]$ και $𝑂[𝑁^2]$
για αποτελεσματικές υλοποιήσεις. Για μεγάλο αριθμό δειγμάτων εκπαίδευσης, αυτό το υπολογιστικό κόστος μπορεί να είναι απαγορευτικό.
* Τα αποτελέσματα εξαρτώνται σε μεγάλο βαθμό από την κατάλληλη επιλογή για την παράμετρο μαλακώματος 𝐶. Αυτό πρέπει να γίνει μόνο με cross-validation ή οποία μπορεί να είναι ακριβή υπολογιστικά αν έχουμε πολλά δεδομένα.
* Τα αποτελέσματα δεν έχουν άμεση πιθανοτική ερμηνεία (κατά πόσο ένα δείγμα ανήκει σε μία κλάση).Σε κάποιες περιπτώσεις SVM αυτό μπορεί να εκτιμηθεί μέσω μιας εσωτερικής διασταυρούμενης επικύρωσης ( παράμετρος "probability" του "SVC"), αλλά αυτή η επιπλέον εκτίμηση είναι υπολογιστικά δαπανηρή.

Οι υπερπαράμετροι που εξετάζουμε  στο παρόν βήμα(default), είναι:

* kernel: Ο τύπος πυρήνα που χρησιμοποιείται από τον αλγόριθμο (συνάρτηση που ορίζει το εσωτερικό γινόμενο μεταξύ δύο διανυσμάτων  σε έναν άλλο διανυσματικό χώρο). Ο default είναι ο rbf(ακτινική συνάρτηση βάσης).

* C: παράμετρος softening(μεγάλο: σκληρό περιθώριο, μικρό: μαλακό περιθώριο). Το default είναι 1.

* gamma: συντελεστής kernel(default: scale)

* tol: tolerance stopping κριτηρίου (default: 0.001)

Αργότερα που θα κάνουμε tuning υπερπαραμέτρων, θεωρώντας τον τύπο πυρήνα ως υπερπαράμετρο, για linear πυρήνα θα θεωρήσουμε ως υπερπαραμέτρους τα loss, tol, C και για rbf, poly τα C, gamma, tol και το degree μόνο για τον poly.
"""

# Commented out IPython magic to ensure Python compatibility.
clf2  = SVC(C = 1, gamma = 'scale', kernel = 'rbf', tol = 1e-3)
# %time clf2.fit(X_train, y_train)
# %time SVC_predict = clf2.predict(X_test)
print("Predictions for KNN classifier:"+"\n"+" {}".format(SVC_predict ))

cm = confusion_matrix(y_test, SVC_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['Not Epileptic seizure', 'Epileptic seizure'] ,title='Confusion matrix of SVM classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, SVC_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_SVC = list(precision_recall_fscore_support(y_test, SVC_predict, average='micro'))
micro_SVC.pop(3) # tou support
precision_micro_SVC = micro_SVC[0]
recall_micro_SVC  = micro_SVC[1]
f1_micro_SVC  = micro_SVC[2]
print(micro_SVC)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_SVC = list(precision_recall_fscore_support(y_test, SVC_predict, average='macro'))
macro_SVC.pop(3) # tou support
precision_macro_SVC  = macro_SVC[0]
recall_macro_SVC  = macro_SVC [1]
f1_macro_SVC  = macro_SVC [2]
print(macro_SVC )
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  SVC_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'],output_dict=True)
print(pd.DataFrame(class_report).T)

print("SVC classifier f1_micro score = {}".format(f1_micro_SVC))
print("SVC classifier f1_macro score = {}".format(f1_macro_SVC))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""Από τα παραπάνω αποτελέσματα βλέπουμε ότι έχουμε μια πολύ καλή απόδοση.Αυτό οφείλεται στο γεγονός ότι χρησιμοποιούμε τς default υπερπαραμέτρους που είναι μελετημένες ώστε να δίνουν καλά αποτελέσματα στη γενική περίπτωση, στο ότι έχουμε binary classification πρόβλημα για τα οποία ο SVM έχει καλή απόδοση και ότι η υψηλή διαστατικότητα όπως αναφέραμε δεν τον εμποδίζει από το να αποδίδει ικανοποιητικά. Επιπλέον το γεγονός ότι χρησιμοποιήσαμε rbf πυρήνα που εξυπηρετεί ταξινομήσεις μη γραμμικών dataset, όπως φαίνεται να είναι το δικό μας, μας έδωσε ως αποτέμεσμα πολύ καλό αποτέλεσμα ως προς τις μετρικές ενδιαφέροντος.

###MLP

Κάνουμε μια σύντομη ποιοτική περιγραφή του MLP.

Πρόκειται για supervised learning αλγόριθμο που "μαθαίνει" μία συνάρτηση  $f:R^m \rightarrow R^o$ μέσω training στο dataset όπου $m$ είναι το πλήθος των διαστάσεων των δειγμάτων και $ο$ το πλήθος των διαστάσρων της εξόδου. Το input layer αποτελείται από ένα πλήθος νευρώνων που αναπαριστά τα δείγματα εισόδου {$x_{i}|x_{1},x_{2},..,x_{n}$}. Κάθε νευρώνας στο κρυφό layer μετασχηματίζει τις τιμές του προηγούμενου layer με μία έμβαρη γραμμική προσομοίωση $w_{1}x_{1} + w_{2}x_{2}+...+w_{m}x_{m}$, που ακολουθείται από μία γραμμική συνάρτηση ενεργοποίησης $g:R \rightarrow R$. To output layer λαμβάνει τις τιμές από το τελευταίο hidden layer και τις μετασχηματίζει σε output τιμές.

Πλεονεκτήματα:


*   Καλή εκπαίδευση σε μη γραμμικά μοντέλα
*   Καλή εκπαίδευση σε real-time.

Μειονεκτήματα:


*   Το MLP με hidden layers έχει μια μη κυρτή συνάρτηση κόστους με περισσότερα από ένα τοπικά ελάχιστα και απαιτεί καλή αρχικοποίηση βαρών.
*  Έχει πάρα πολλές υπερπαραμέτρους προς βελτιστοποίηση.
*  Είναι ευαίσθητο σε feature scaling.


Οι υπερπαράμετροι που εξετάζουμε στην άσκηση αυτή και στο παρόν βήμα χρησιμοποιούμε τους default, είναι:


* hidden_layer_sizes: πλήθος κρυφών layer(default 100)  
* activation: συνάρτηση ενεργοποίησης(default relu - rectified linear unit function)  
* solver: Optimizer βαρών. Ο default είναι ο adam(stochastic gradient-based optimizer).
*alpha: penalty parameter(default 0.0001)
*learning_rate: βήμα μάθησης(default constant)
*max_iter: πλήθος εποχών (default 200)
"""

# Commented out IPython magic to ensure Python compatibility.
clf3 = MLPClassifier(solver='adam', alpha=0.0001,
                    hidden_layer_sizes=(100,), max_iter=200, learning_rate = 'constant', activation = 'relu')
# %time clf3.fit(X_train, y_train)
# %time MLP_predict = clf3.predict(X_test)
print("Predictions for MLP classifier:"+"\n"+" {}".format(MLP_predict ))

cm = confusion_matrix(y_test, MLP_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['Not Epileptic seizure', 'Epileptic seizure'] ,title='Confusion matrix of MLP classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, MLP_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_MLP = list(precision_recall_fscore_support(y_test, MLP_predict, average='micro'))
micro_MLP.pop(3) # tou support
precision_micro_MLP = micro_MLP[0]
recall_micro_MLP  = micro_MLP[1]
f1_micro_MLP  = micro_MLP[2]
print(micro_MLP)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_MLP = list(precision_recall_fscore_support(y_test, MLP_predict, average='macro'))
macro_MLP.pop(3) # tou support
precision_macro_MLP  = macro_MLP[0]
recall_macro_MLP  = macro_MLP [1]
f1_macro_MLP  = macro_MLP [2]
print(macro_MLP)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  MLP_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'],output_dict=True)
print(pd.DataFrame(class_report).T)

print("MLP classifier f1_micro score = {}".format(f1_micro_MLP))
print("MLP classifier f1_macro score = {}".format(f1_macro_MLP))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""Παρατηρούμε ότι και ο MLP έχει πολύ καλή απόδοση καθώς χρησιμοποιούμε την default αρχικοποίησή του, δηλαδή τον adam ως optimizer, ο οποίος έχει αρκετά καλή απόδοση για μεγάλα data set, και ως πλήθος hidden layers το 100, δηλαδή πολλά κρυφά επίπεδα, τα οποία εξυπηρετούν στην ταξινόμηση μη γραμμικά διαχωρίσιμων data set και στην ύπαρξη πολλών διαστάσεων στα δείγματα, όπως συμβαίνει στη συγκεκριμένη περίπτωση.

### Σύγκριση ταξινομητών

Παρακάτω βλέπουμε την απόδοση των ταξινομητών Dummy Stratified,  Gaussian Naive Bayes, kNN, SVM και MLP default αρχικοποίησης ως προς τη μετρική f1_micro.
"""

fig = plt.figure()
x = ['Dummy stratified','Gaussian NB','KNN','SVM','MLP']
y=[100*f1_micro_dc_uniform,100*f1_micro_GNB,100*f1_micro_KNN,100*f1_micro_SVC,100*f1_micro_MLP]
plt.bar(x,y, align='center', width=0.5)
plt.xticks(x,x)
plt.title('F1 micro')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

"""Παρακάτω βλέπουμε την απόδοση των ταξινομητών Dummy Stratified, Gaussian Naive Bayes, kNN, SVM και MLP default αρχικοποίησης ως προς τη μετρική f1_macro."""

fig = plt.figure()
x = ['Dummy stratified','Gaussian NB','KNN','SVM','MLP']
y=[100*f1_macro_dc_uniform,100*f1_macro_GNB,100*f1_macro_KNN,100*f1_macro_SVC,100*f1_macro_MLP]
plt.bar(x,y, align='center', width=0.5)
plt.xticks(x,x)
plt.title('F1 macro')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

"""Από τα παραπάνω παρατηρούμε ότι πέραν του Dummy stratified που δεν κάνει τόσο σοφές προβλέψεις όπως γνωρίζουμε, οι υπόλοιποι έχουν πολύ καλές αποδόσεις τόσο ως προς τη μία μετρική όσο και ως προς την άλλη. Αυτό οφείλεται στο γεγονός ότι έχουμε εξισορροπήσει το data set το οποίο ήταν αρκετά μη ισορροπημένο.

Πριν περάσουμε σε λεπτομέρειες για κάθε ταξινομητή, αξίζει να αναφέρουμε ότι οι 4 "σοβαροί" ταξινομητές μας (Gaussian NB, KNN, SVM και MLP) έχουν μεγάλη απόδοση και ως προς το precision και recall. Αφού το Data Set μας περιέχει μετρήσεις για την ανίχνευση ασθένειας(νευρολογικής διαταραχής), οι μετρικές αυτές μας είναι ιδιαίτερα σημαντικές για την αξιολόγηση της απόδοσης. Σημειώνουμε βέβαια ότι ακόμα πιο σημαντικές είναι οι ποσότητες specificity και sensitivity, στην περίπτωση ιατρικών διαγνώσεων, αλλά δεν αποτελούν αντικείμενο εξέτασης στα πλαίσια της παρούσας άσκησης.

Παρατηρούμε ότι την καλύτερη απόδοση έχει ο SVM και ακολούθως ο MLP. Παρόλο που και οι δύο έχουν καλή απόδοση σε μεγάλα μη γραμμικά datasets υψηλής διαστατικότητας, η υπεροχή του SVM οφείλεται εν μέρει και στις λιγότερες υπερπαραμέτρους που χαρακτηρίζουν τον ταξινομητή. Επιπλέον , το γεγονός ότι έχουμε binary classification διευκολύνει τον SVM.

Ακολουθεί ο Gaussian NB που έχει και αυτός καλή απόδοση. Οι αποκλίσεις από τη μέση τιμή και την διασπορά είναι πολύ μικρές αφού τα δεδομένα είναι κανονικοποιημένα (τα έχουμε κανονικοποιήσει στο πρώστο βήμα) και ένας λόγος αστοχίας του Gaussian NB είναι οι μεγάλες αυτές αποκλίσεις, που εδώ δεν παρατηρούνται. Μπορούμε να θεωρήσουμε ότι αστοχίες οφείλονται και στο γεγονός ότι τα χαρακτηριστικά λογικά δεν είναι ανεξάρτητα μεταξύ τους.

Τέλος ο kNN, έχει και αυτός καλή απόδοση αλλά ακολουθεί τους άλλους αφού η υπερπαράμετρος k που είναι πολύ σημαντική για τον ταξινομητή δεν είναι βελτιστοποιημένο.

Ο Dummy Stratified, όπως προείπαμε και αναμέναμε δεν έχει ικανοποιητικά αποτελέσματα.

#Βημα 3: Βελτιστοποίηση ταξινομητών

Στο βήμα αυτό θα κάνουμε tuning υπερπαραμέτρων ώστε να βρούμε τους κατάλληλους συνδυασμούς που θα βελτιστοποιήσουν τους ταξινομητές μας. Αυτό γίνεται μέσω του Grid-search με 5-fold-cross validation, το οποίο είναι μια ακριβά υπολογιστική διαδικασία και επίπονη στην περίπτωση μεγάλου συνδυασμού υπερπαραμέτρων. Κάνουμε για κάθε ταξινομητή δύο gridsearch, ένα ως προς τη μετρική f1_micro και ένα ως προς την f1_macro και προφανώς ανάλογα με τη μετρική μπορεί να βγει άλλος συνδυασμός βέλτιστων υπερπαραμέτρων.

Αρχικά τυπώνουμε το variance των δεδομένων και παρατηρούμε ότι οι τιμές της διασποράς είναι πολύ κοντινές. Τo variance thresholding μπορεί να θεωρηθεί ως υπερπαράμετρος, την οποία αρχικά εισάγαμε ως μέρος της αρχιτεκτονικής του pipeline αλλά παρατηρήσαμε πως πάντα επιλεγόταν threshold κάτω από το min των τιμών, δηλαδή τα αποτελέσματα είναι χειρότερα κάνοντας variance thresholding. Καθώς αποτελεί μια μη χρήσιμη υπερπαράμετρο στο συγκεκριμένο πρόβλημα που απλά μας φορτώνει με περιττό υπολογιστικό κόστος, την αφαιρέσαμε.

Γενικά, όπως επισημαίνεται για να αποφύγουμε (σε ένα μικρό βέβαια βαθμό) την ακρίβεια του gridsearch μπορούμε να κάνουμε random sampling στα δεδομένα και να πάρουμε ένα μεγάλο πεδίο ορισμού για τις υπερπαραμέτρους και να κάνουμε gridserach, στενεύοντας μετά το πεδίο ορισμού ανάλογα με τις καλύτερες υπερπαραμέτρους και να επαναλάβουμε τη διαδικασία για όλο το data set. Καθώς αυτό ενέχει τον κίνδυνο να μη λάβουμε υπόψην μας σημαντική πληροφορία του data set, στο πρώτο βήμα προτιμήσαμε την εναλλακτική οδό να επιλέγουμε τις δημοφιλείς υπερπαραμέτρους και τιμές κοντά στις default και όπου παρουσιάζεται ανάγκη να μετακινούμε το πεδίο ορισμού στις περιπτώσεις που οι καλύτερες υπερπαράμετροι είναι στο άκρο του πεδίου.
"""

train_variance = X_train.var(axis=0)
print(train_variance)
print(np.max(train_variance))

"""###kNN

Το PCA είναι μία υπερπαράμετρος που πρέπει να λάβουμε υπόψην μας στο grid search και αποτελεί μέρος της αρχιτεκτονικής του pipeline. Για τον kNN, αφού πήραμε στο πεδίο ορισμού τις τιμές 50,100,150 (έχουμε διάσταση 178), μετακινήσαμε το πεδίο προς τα κάτω όπως μας έδειξε το πρώτο gridsearch, και προέκυψε το ακόλουθο αποτέλεσμα.
"""

# Commented out IPython magic to ensure Python compatibility.
n_components = [30, 40, 50]
weights =  ['uniform','distance'] 
metric = ['minkowski','manhattan','mahalanobis']
k = [1,2,3,4,5] 
clf = KNeighborsClassifier()

pipe = Pipeline(steps=[('pca', pca), ('kNN', clf)], memory = 'tmp')
estimator = GridSearchCV(pipe, dict( pca__n_components=n_components, kNN__n_neighbors = k, kNN__weights = weights, kNN__metric = metric), refit = True, cv=5, scoring='f1_micro', verbose = 10, n_jobs=-1)
# %time estimator.fit(X_train, y_train)
print("Best parameters regarding f1_micro is = " +str(estimator.best_params_))
print()

pipe = Pipeline(steps=[('pca', pca), ('kNN', clf)], memory = 'tmp')
estimator = GridSearchCV(pipe, dict( pca__n_components=n_components, kNN__n_neighbors = k, kNN__weights = weights, kNN__metric = metric), refit = True, cv=5, scoring='f1_macro', verbose = 10, n_jobs=-1)
# %time estimator.fit(X_train, y_train)
print("Best parameters regarding f1_macro is = " +str(estimator.best_params_))

"""Παρατηρούμε ότι παρότι τα fits ήταν αρκετά, ο συνολικός χρόνος δεν ήταν πολύ μεγάλος. Επίσης παρατηρούμε ότι και ως προς τις δύο μετρικές, καλύτερος συνδυασμός υπερπαραμέτρων, βρέθηκε ο ίδιος."""

# Commented out IPython magic to ensure Python compatibility.
# %time KNN_predict = estimator.predict(X_test)
print("Predictions for KNN optimal classifier:"+"\n"+" {}".format(KNN_predict))

cm = confusion_matrix(y_test, KNN_predict)
plot_confusion_matrix(cm, ['Not Epileptic', 'Epileptic'],title='Confusion matrix of KNN optimal classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, KNN_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_KNN_opt = list(precision_recall_fscore_support(y_test, KNN_predict, average='micro'))
micro_KNN_opt.pop(3) # tou support
precision_micro_KNN_opt  = micro_KNN_opt[0]
recall_micro_KNN_opt  = micro_KNN_opt [1]
f1_micro_KNN_opt  = micro_KNN_opt[2]
print(micro_KNN_opt )

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_KNN_opt = list(precision_recall_fscore_support(y_test, KNN_predict, average='macro'))
macro_KNN_opt.pop(3) # tou support
precision_macro_KNN_opt  = macro_KNN_opt[0]
recall_macro_KNN_opt = macro_KNN_opt[1]
f1_macro_KNN_opt = macro_KNN_opt[2]
print(macro_KNN_opt)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  KNN_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'],output_dict=True)
print(pd.DataFrame(class_report).T)

print("KNN classifier f1_micro score = {}".format(f1_micro_KNN_opt))
print("KNN classifier f1_macro score = {}".format(f1_macro_KNN_opt))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""Βλέπουμε επίσης ότι ο kNN με gridesarch έδωσε συγκρίσιμα αποτελέσματα με την default αρχικοποίησή του στο προηγούμενο βήμα και ελάχιστα χειρότερα. (κατά 0.1% περίπου). Πρέπει να λάβουμε σοβαρά υπόψην για το Gridsearch το εξής, το οποίο προφανώς ισχύει και για τους άλλους ταξινομητές που βλέπουμε παρακάτω:

**To grid search, χρησιμοποιεί μόνο το training set για τη βελτιστοιποίηση των υπερπαραμέτρων. Οπότε όταν κάνουμε προβλέψεις για το test set, χρησιμοποιώντας τον συνδυασμό αυτό, μπορεί να μην επιτύχουμε την ψηλότερη απόδοση, αλλά μπορούμε να αναμένουμε ότι θα είμαστε αρκετά κοντά σε αυτή**

###SVM

Στην περίπτωση του SVM, εξετάζουμε ως τύπο kernel και τον Linear. Όμως όπως έχουμε καταλάβει τα δεδομένα μας δεν είναι γραμμικά διαχωρίσιμα οπότε δεν αναμένουμε πολύ καλά αποτελέσματα από τον γραμμικό πυρήνα, όπως και όντως συμβαίνει.

####Linear
"""

# Commented out IPython magic to ensure Python compatibility.
n_components = [50,100,150]
loss = ['hinge','squared_hinge'] 
tol = [0.001, 0.0001]
C = [0.5, 1, 1.5]
clf = LinearSVC()

pipe = Pipeline(steps=[('pca', pca), ('linearsvc', clf)], memory = 'tmp')
estimator1 = GridSearchCV(pipe, dict( pca__n_components=n_components, linearsvc__loss = loss, linearsvc__tol = tol, linearsvc__C = C), refit = True, cv=5, scoring='f1_micro', verbose = 10, n_jobs=-1)
# %time estimator1.fit(X_train, y_train)
print("Best parameters regarding f1_micro is = " +str(estimator1.best_params_))
print()

pipe = Pipeline(steps=[('pca', pca), ('linearsvc', clf)], memory = 'tmp')
estimator2 = GridSearchCV(pipe, dict( pca__n_components=n_components, linearsvc__loss = loss, linearsvc__tol = tol, linearsvc__C = C), refit = True, cv=5, scoring='f1_macro', verbose = 10, n_jobs=-1)
# %time estimator2.fit(X_train, y_train)
print("Best parameters regarding f1_macro is = " +str(estimator2.best_params_))

"""Ο SVM είναι γρήγορος αλγόριθμος και όπως βλέπουμε ο χρόνος του grid search δεν είναι μεγάλος, αλλά δεν έχουμε και τόσους πολλούς συνδυασμούς υπερπαραμέτρων(180 fits)."""

# Commented out IPython magic to ensure Python compatibility.
# %time LinearSVC_predict = estimator1.predict(X_test)
print("Predictions for Linear SVM optimal classifier:"+"\n"+" {}".format(LinearSVC_predict))

cm = confusion_matrix(y_test, LinearSVC_predict)
plot_confusion_matrix(cm, ['Not Epileptic', 'Epileptic'],title='Confusion matrix of LinearSVC classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, LinearSVC_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_LinearSVC_opt = list(precision_recall_fscore_support(y_test, LinearSVC_predict, average='micro'))
micro_LinearSVC_opt.pop(3) # tou support
precision_micro_LinearSVC_opt  = micro_LinearSVC_opt[0]
recall_micro_LinearSVC_opt  = micro_LinearSVC_opt [1]
f1_micro_LinearSVC_opt  = micro_LinearSVC_opt[2]
print(micro_LinearSVC_opt )

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_LinearSVC_opt = list(precision_recall_fscore_support(y_test, LinearSVC_predict, average='macro'))
macro_LinearSVC_opt.pop(3) # tou support
precision_macro_LinearSVC_opt  = macro_LinearSVC_opt[0]
recall_macro_LinearSVC_opt = macro_LinearSVC_opt[1]
f1_macro_LinearSVC_opt = macro_LinearSVC_opt[2]
print(macro_LinearSVC_opt)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test, LinearSVC_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'],output_dict=True)
print(pd.DataFrame(class_report).T)

print("LinearSVC classifier f1_micro score = {}".format(f1_micro_LinearSVC_opt))
print("LinearSVC classifier f1_macro score = {}".format(f1_macro_LinearSVC_opt))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

# Commented out IPython magic to ensure Python compatibility.
# %time LinearSVC_predict = estimator2.predict(X_test)
print("Predictions for Linear SVM optimal classifier:"+"\n"+" {}".format(LinearSVC_predict))

cm = confusion_matrix(y_test, LinearSVC_predict)
plot_confusion_matrix(cm, ['Not Epileptic', 'Epileptic'],title='Confusion matrix of LinearSVC classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, LinearSVC_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_LinearSVC_opt = list(precision_recall_fscore_support(y_test, LinearSVC_predict, average='micro'))
micro_LinearSVC_opt.pop(3) # tou support
precision_micro_LinearSVC_opt  = micro_LinearSVC_opt[0]
recall_micro_LinearSVC_opt  = micro_LinearSVC_opt [1]
f1_micro_LinearSVC_opt  = micro_LinearSVC_opt[2]
print(micro_LinearSVC_opt )

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_LinearSVC_opt = list(precision_recall_fscore_support(y_test, LinearSVC_predict, average='macro'))
macro_LinearSVC_opt.pop(3) # tou support
precision_macro_LinearSVC_opt  = macro_LinearSVC_opt[0]
recall_macro_LinearSVC_opt = macro_LinearSVC_opt[1]
f1_macro_LinearSVC_opt = macro_LinearSVC_opt[2]
print(macro_LinearSVC_opt)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test, LinearSVC_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'],output_dict=True)
print(pd.DataFrame(class_report).T)

print("LinearSVC classifier f1_micro score = {}".format(f1_micro_LinearSVC_opt))
print("LinearSVC classifier f1_macro score = {}".format(f1_macro_LinearSVC_opt))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""####Poly, Rbf

Οι πυρήνες poly και RBF έχουν περισσότερο νόημα σε αυτή την περίπτωση καθώς τα δεδομένα είναι μη γραμμικά. Αρχικά έχουμε βάλει στο pipeline και το PCA, αφιαρώντας το όμως παρατηρήσαμε ότι έχουμε καλύτερη απόδοση, όπως φαίνεται και παρακάτω.
"""

# Commented out IPython magic to ensure Python compatibility.
n_components = [50, 150]
gamma = ['auto','scale']
kernel = ['poly','rbf']
degree = [3, 5] 
tol = [0.001, 0.0001]
C = [0.7, 1.2]
clf = SVC()

pipe = Pipeline(steps=[('pca', pca), ('SVC', clf)], memory = 'tmp')
estimator1 = GridSearchCV(pipe, dict( pca__n_components=n_components, SVC__kernel = kernel, SVC__tol = tol, SVC__C = C, SVC__degree = degree, SVC__gamma = gamma), refit = True, cv=5, scoring='f1_micro', verbose = 10, n_jobs=-1)
# %time estimator1.fit(X_train, y_train)
print("Best parameters regarding f1_micro is = " +str(estimator1.best_params_))
print()

pipe = Pipeline(steps=[('pca', pca), ('SVC', clf)], memory = 'tmp')
estimator2 = GridSearchCV(pipe, dict( pca__n_components=n_components, SVC__kernel = kernel, SVC__tol = tol, SVC__C = C, SVC__degree = degree, SVC__gamma = gamma), refit = True, cv=5, scoring='f1_macro', verbose = 10, n_jobs=-1)
# %time estimator2.fit(X_train, y_train)
print("Best parameters regarding f1_macro is = " +str(estimator2.best_params_))

"""Οι χρόνος είναι μεγαλύτερος από την περίπτωση του linear καθώς έχουμε περισσότερες υπερπαραμέτρους για βελτιστοποίση."""

# Commented out IPython magic to ensure Python compatibility.
# %time SVC_predict = estimator1.predict(X_test)
print("Predictions for Linear SVM optimal classifier:"+"\n"+" {}".format(SVC_predict))

cm = confusion_matrix(y_test, SVC_predict)
plot_confusion_matrix(cm, ['Not Epileptic', 'Epileptic'],title='Confusion matrix of SVC classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, SVC_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_SVC_opt = list(precision_recall_fscore_support(y_test, SVC_predict, average='micro'))
micro_SVC_opt.pop(3) # tou support
precision_micro_SVC_opt  = micro_SVC_opt[0]
recall_micro_SVC_opt  = micro_SVC_opt [1]
f1_micro_SVC1_opt  = micro_SVC_opt[2]
print(micro_SVC_opt )

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_SVC_opt = list(precision_recall_fscore_support(y_test, SVC_predict, average='macro'))
macro_SVC_opt.pop(3) # tou support
precision_macro_SVC_opt  = macro_SVC_opt[0]
recall_macro_SVC_opt = macro_SVC_opt[1]
f1_macro_SVC1_opt = macro_SVC_opt[2]
print(macro_SVC_opt)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test, SVC_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'],output_dict=True)
print(pd.DataFrame(class_report).T)

print("SVC classifier f1_micro score = {}".format(f1_micro_SVC1_opt))
print("SVC classifier f1_macro score = {}".format(f1_macro_SVC1_opt))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

# Commented out IPython magic to ensure Python compatibility.
# %time SVC_predict = estimator2.predict(X_test)
print("Predictions for SVM optimal classifier:"+"\n"+" {}".format(SVC_predict))

cm = confusion_matrix(y_test, SVC_predict)
plot_confusion_matrix(cm, ['Not Epileptic', 'Epileptic'],title='Confusion matrix of SVC classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, SVC_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_SVC_opt = list(precision_recall_fscore_support(y_test,SVC_predict, average='micro'))
micro_SVC_opt.pop(3) # tou support
precision_micro_SVC_opt  = micro_SVC_opt[0]
recall_micro_SVC_opt  = micro_SVC_opt [1]
f1_micro_SVC2_opt  = micro_SVC_opt[2]
print(micro_SVC_opt )

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_SVC_opt = list(precision_recall_fscore_support(y_test, SVC_predict, average='macro'))
macro_SVC_opt.pop(3) # tou support
precision_macro_SVC_opt  = macro_SVC_opt[0]
recall_macro_SVC_opt = macro_SVC_opt[1]
f1_macro_SVC2_opt = macro_SVC_opt[2]
print(macro_SVC_opt)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test, SVC_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'],output_dict=True)
print(pd.DataFrame(class_report).T)

print("SVC classifier f1_micro score = {}".format(f1_micro_SVC2_opt))
print("SVC classifier f1_macro score = {}".format(f1_macro_SVC2_opt))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""####Poly, Rbf χωρίς PCA

Η αφαίρεση του PCA από το pipeline, οδηγεί σε καλύτερα αποτελέσματα, το οποίο σημαίνει ότι η μείωση της διαστατικότητας για το παρόν πρόβλημα και τον παρόντα ταξινομητή, δεν βοηθά.
"""

# Commented out IPython magic to ensure Python compatibility.
gamma = ['auto','scale']
kernel = ['poly','rbf']
degree = [3, 5] 
tol = [0.001, 0.0001]
C = [0.7, 1.2]
clf = SVC()

pipe = Pipeline(steps=[ ('SVC', clf)], memory = 'tmp')
estimator1 = GridSearchCV(pipe, dict(  SVC__kernel = kernel, SVC__tol = tol, SVC__C = C, SVC__degree = degree, SVC__gamma = gamma), refit = True, cv=5, scoring='f1_micro', verbose = 10, n_jobs=-1)
# %time estimator1.fit(X_train, y_train)
print("Best parameters regarding f1_micro is = " +str(estimator1.best_params_))
print()

pipe = Pipeline(steps=[('SVC', clf)], memory = 'tmp')
estimator2 = GridSearchCV(pipe, dict( SVC__kernel = kernel, SVC__tol = tol, SVC__C = C, SVC__degree = degree, SVC__gamma = gamma), refit = True, cv=5, scoring='f1_macro', verbose = 10, n_jobs=-1)
# %time estimator2.fit(X_train, y_train)
print("Best parameters regarding f1_macro is = " +str(estimator2.best_params_))

"""´Εχουμε κι εδώ έναν καλό χρόνο δεδομένου του πλήθους των συνδυασμών. Παρατηρούμε επίσης ότι ο συνδυασμός υπερπαραμέτρων είναι ίδιος και ως προς τις δύο μετρικές."""

# Commented out IPython magic to ensure Python compatibility.
# %time SVC_predict = estimator1.predict(X_test)
print("Predictions for Linear SVM optimal classifier:"+"\n"+" {}".format(SVC_predict))

cm = confusion_matrix(y_test, SVC_predict)
plot_confusion_matrix(cm, ['Not Epileptic', 'Epileptic'],title='Confusion matrix of SVC classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, SVC_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_SVC_opt = list(precision_recall_fscore_support(y_test, SVC_predict, average='micro'))
micro_SVC_opt.pop(3) # tou support
precision_micro_SVC_opt  = micro_SVC_opt[0]
recall_micro_SVC_opt  = micro_SVC_opt [1]
f1_micro_SVC3_opt  = micro_SVC_opt[2]
print(micro_SVC_opt )

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_SVC_opt = list(precision_recall_fscore_support(y_test, SVC_predict, average='macro'))
macro_SVC_opt.pop(3) # tou support
precision_macro_SVC_opt  = macro_SVC_opt[0]
recall_macro_SVC_opt = macro_SVC_opt[1]
f1_macro_SVC3_opt = macro_SVC_opt[2]
print(macro_SVC_opt)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test, SVC_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'],output_dict=True)
print(pd.DataFrame(class_report).T)

print("SVC classifier f1_micro score = {}".format(f1_micro_SVC3_opt))
print("SVC classifier f1_macro score = {}".format(f1_macro_SVC3_opt))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

# Commented out IPython magic to ensure Python compatibility.
# %time SVC_predict = estimator2.predict(X_test)
print("Predictions for SVM optimal classifier:"+"\n"+" {}".format(SVC_predict))

cm = confusion_matrix(y_test, SVC_predict)
plot_confusion_matrix(cm, ['Not Epileptic', 'Epileptic'],title='Confusion matrix of SVC classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, SVC_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_SVC_opt = list(precision_recall_fscore_support(y_test,SVC_predict, average='micro'))
micro_SVC_opt.pop(3) # tou support
precision_micro_SVC_opt  = micro_SVC_opt[0]
recall_micro_SVC_opt  = micro_SVC_opt [1]
f1_micro_SVC4_opt  = micro_SVC_opt[2]
print(micro_SVC_opt )

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_SVC_opt = list(precision_recall_fscore_support(y_test, SVC_predict, average='macro'))
macro_SVC_opt.pop(3) # tou support
precision_macro_SVC_opt  = macro_SVC_opt[0]
recall_macro_SVC_opt = macro_SVC_opt[1]
f1_macro_SVC4_opt = macro_SVC_opt[2]
print(macro_SVC_opt)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test, SVC_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'],output_dict=True)
print(pd.DataFrame(class_report).T)

print("SVC classifier f1_micro score = {}".format(f1_micro_SVC4_opt))
print("SVC classifier f1_macro score = {}".format(f1_macro_SVC4_opt))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""Τα αποτελέσματα συγκριτικά με τη default αρχικοποίηση είναι συγκρίσιμα καθώς επιλέχθησαν είτε οι default τιμές, είτε τιμές κοντά στις default. Είναι όμως ελαφρώς καλύτερα το οποίο σημαίνει ότι το grid search ήταν επιτυχές.

###MLP

Και σε αυτή την περίπτωση παρουσιάζουμε τον ταξινομητή με και χωρίς PCA. Κάνοντας ένα grid search με πλήθος διαστάσεων 50,100 και 150 και για τις δύο μετρικές προέκυψε καλύτερη παράμετρος το 150 οπότε παρουσιάζουμε παρακάτω το grid search με τιμές κοντά στο 150. Επειδή όμως οι τιμές αυτές είναι πολύ κοντά στην αρχική διάσταση του προβλήματος, παρουσιάζουμε το grid search και χωρίς PCA. Παρατηρούμε ότι με PCA με πεδίο ορισμού τις τιμές που προείπαμε, τα αποτελέσματα είναι ελαφρώς καλύτερα, αλλά συγκρίσιμα.

Αξίζει να αναφέρουμε ότι δεν συμπεριλαμβάνουμε τον lbfgs solver στο πεδίο ορισμού, καθώς γνωρίζουμε ότι συγκλίνει για πολύ μεγαλύτερο πλήθος εποχών, σε σχέση με τους άλλους και είναι κατάλληλος για μικρά data sets (και εμείς έχουμε μεγάλο).
"""

# Commented out IPython magic to ensure Python compatibility.
n_components = [150, 160, 170]
hidden_layer_sizes = [(1,)]
solver = ['adam','sgd']
max_iter = [250, 300] 
learning_rate = ['constant','adaptive']
alpha = [0.0001,0.00001]
activation = ['tanh', 'relu']
clf = MLPClassifier()

pipe = Pipeline(steps=[('pca', pca), ('MLP', clf)], memory = 'tmp')
estimator1 = GridSearchCV(pipe, dict(pca__n_components=n_components, MLP__hidden_layer_sizes =  hidden_layer_sizes, MLP__solver = solver, MLP__max_iter = max_iter, MLP__learning_rate = learning_rate, MLP__alpha = alpha, MLP__activation = activation),refit = True, cv=5, scoring='f1_micro', verbose = 10, n_jobs=-1)
# %time estimator1.fit(X_train, y_train)
print("Best parameters regarding f1_micro is = " +str(estimator1.best_params_))
print()

pipe = Pipeline(steps=[('pca', pca), ('MLP', clf)], memory = 'tmp')
estimator2 = GridSearchCV(pipe, dict(pca__n_components=n_components, MLP__hidden_layer_sizes =  hidden_layer_sizes, MLP__solver = solver, MLP__max_iter = max_iter, MLP__learning_rate = learning_rate, MLP__alpha = alpha, MLP__activation = activation),refit = True, cv=5, scoring='f1_macro', verbose = 10, n_jobs=-1)
# %time estimator2.fit(X_train, y_train)
print("Best parameters regarding f1_macro is = " +str(estimator2.best_params_))

"""Ο χρόνος είναι αρκετά μεγαλύτερος σε σύγκριση με τους υπόλοιους, αλλά έχουμε περισσότερους συνδυασμούς υπερπαραμέτρων. Παρατηρούμε επίσης ότι δεν επιλέχθηκε ο ίδιος συνδυασμός βέλτιστων υπερπαραμέτρων ως προς κάθε μετρική."""

# Commented out IPython magic to ensure Python compatibility.
# %time MLP_predict =estimator1.predict(X_test)
print("Predictions for KNN classifier:"+"\n"+" {}".format(MLP_predict))

cm = confusion_matrix(y_test, MLP_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['Not Epileptic seizure', 'Epileptic seizure'] ,title='Confusion matrix of KNN classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, MLP_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_MLP_opt = list(precision_recall_fscore_support(y_test, MLP_predict, average='micro'))
micro_MLP_opt.pop(3) # tou support
precision_micro_MLP_opt = micro_MLP_opt[0]
recall_micro_MLP_opt  = micro_MLP_opt[1]
f1_micro_MLP1_opt  = micro_MLP_opt[2]
print(micro_MLP_opt)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_MLP_opt = list(precision_recall_fscore_support(y_test, MLP_predict, average='macro'))
macro_MLP_opt.pop(3) # tou support
precision_macro_MLP_opt  = macro_MLP_opt[0]
recall_macro_MLP_opt  = macro_MLP_opt [1]
f1_macro_MLP1_opt  = macro_MLP_opt [2]
print(macro_MLP_opt)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  MLP_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'],output_dict=True)
print(pd.DataFrame(class_report).T)

print("MLP classifier f1_micro score = {}".format(f1_micro_MLP1_opt))
print("MLP classifier f1_macro score = {}".format(f1_macro_MLP1_opt))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

# Commented out IPython magic to ensure Python compatibility.
# %time MLP_predict =estimator2.predict(X_test)
print("Predictions for KNN classifier:"+"\n"+" {}".format(MLP_predict))

cm = confusion_matrix(y_test, MLP_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['Not Epileptic seizure', 'Epileptic seizure'] ,title='Confusion matrix of KNN classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, MLP_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_MLP_opt = list(precision_recall_fscore_support(y_test, MLP_predict, average='micro'))
micro_MLP_opt.pop(3) # tou support
precision_micro_MLP_opt = micro_MLP_opt[0]
recall_micro_MLP_opt  = micro_MLP_opt[1]
f1_micro_MLP2_opt  = micro_MLP_opt[2]
print(micro_MLP_opt)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_MLP_opt = list(precision_recall_fscore_support(y_test, MLP_predict, average='macro'))
macro_MLP_opt.pop(3) # tou support
precision_macro_MLP_opt  = macro_MLP_opt[0]
recall_macro_MLP_opt  = macro_MLP_opt [1]
f1_macro_MLP2_opt  = macro_MLP_opt [2]
print(macro_MLP_opt)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  MLP_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'],output_dict=True)
print(pd.DataFrame(class_report).T)

print("MLP classifier f1_micro score = {}".format(f1_micro_MLP2_opt))
print("MLP classifier f1_macro score = {}".format(f1_macro_MLP2_opt))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""Ως καλύτερος ταξινομητής επιλέγεται ο adam. O sgd μπορεί να αποδίδει καλύτερα στις περιπτώσεις που το learning rate ρυθμίζεται κατόπιν πιο εξειδικευμένης μελέτης.Βλέπουμε από τα παραπάνω ότι συγκριτικά με τον MLP της default αρχικοποίησης, έχουμε αρκετά χειρότερα αποτελέσματα. Αυτό οφείλεται στο γεγονός ότι η άσκηση ζητούσε να χρησιμοποιήσουμε μόνο ένα hidden layer για την βελτιστοποίηση και το default είναι 100. Το πλήθος όμως των hidden layers είναι πολύ σημαντικό για την ταξινόμηση μη γραμμικά διαχωρίσιμων δεδομένων.

####MLP χωρίς PCA

>ειδαμε οτι κάποιες φορές δεν συγκλίνει με max interations=200.
"""

# Commented out IPython magic to ensure Python compatibility.
hidden_layer_sizes = [(1,)]
solver = ['adam','sgd']
max_iter = [250, 300] 
learning_rate = ['constant','adaptive']
alpha = [0.0001,0.00001]
activation = ['tanh', 'relu']
clf = MLPClassifier()

pipe = Pipeline(steps=[('MLP', clf)], memory = 'tmp')
estimator1 = GridSearchCV(pipe, dict( MLP__hidden_layer_sizes =  hidden_layer_sizes, MLP__solver = solver, MLP__max_iter = max_iter, MLP__learning_rate = learning_rate, MLP__alpha = alpha, MLP__activation = activation),refit = True, cv=5, scoring='f1_micro', verbose = 10, n_jobs=-1)
# %time estimator1.fit(X_train, y_train)
print("Best parameters regarding f1_micro is = " +str(estimator1.best_params_))
print()

hidden_layer_sizes = [(1,)]
solver = ['adam','sgd']
max_iter = [250, 300] 
learning_rate = ['constant','adaptive']
alpha = [0.0001,0.00001]
activation = ['tanh', 'relu']
clf = MLPClassifier()

pipe = Pipeline(steps=[('MLP', clf)], memory = 'tmp')
estimator2 = GridSearchCV(pipe, dict( MLP__hidden_layer_sizes =  hidden_layer_sizes, MLP__solver = solver, MLP__max_iter = max_iter, MLP__learning_rate = learning_rate, MLP__alpha = alpha, MLP__activation = activation),refit = True, cv=5, scoring='f1_macro', verbose = 10, n_jobs=-1)
# %time estimator2.fit(X_train, y_train)
print("Best parameters regarding f1_macro is = " +str(estimator2.best_params_))

# Commented out IPython magic to ensure Python compatibility.
# %time MLP_predict =estimator1.predict(X_test)
print("Predictions for KNN classifier:"+"\n"+" {}".format(MLP_predict))

cm = confusion_matrix(y_test, MLP_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['Not Epileptic seizure', 'Epileptic seizure'] ,title='Confusion matrix of KNN classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, MLP_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_MLP_opt = list(precision_recall_fscore_support(y_test, MLP_predict, average='micro'))
micro_MLP_opt.pop(3) # tou support
precision_micro_MLP_opt = micro_MLP_opt[0]
recall_micro_MLP_opt  = micro_MLP_opt[1]
f1_micro_MLP3_opt  = micro_MLP_opt[2]
print(micro_MLP_opt)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_MLP_opt = list(precision_recall_fscore_support(y_test, MLP_predict, average='macro'))
macro_MLP_opt.pop(3) # tou support
precision_macro_MLP_opt  = macro_MLP_opt[0]
recall_macro_MLP_opt  = macro_MLP_opt [1]
f1_macro_MLP3_opt  = macro_MLP_opt [2]
print(macro_MLP_opt)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  MLP_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'],output_dict=True)
print(pd.DataFrame(class_report).T)

print("MLP classifier f1_micro score = {}".format(f1_micro_MLP3_opt))
print("MLP classifier f1_macro score = {}".format(f1_macro_MLP3_opt))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

# Commented out IPython magic to ensure Python compatibility.
# %time MLP_predict =estimator2.predict(X_test)
print("Predictions for KNN classifier:"+"\n"+" {}".format(MLP_predict))

cm = confusion_matrix(y_test, MLP_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['Not Epileptic seizure', 'Epileptic seizure'] ,title='Confusion matrix of KNN classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, MLP_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_MLP_opt = list(precision_recall_fscore_support(y_test, MLP_predict, average='micro'))
micro_MLP_opt.pop(3) # tou support
precision_micro_MLP_opt = micro_MLP_opt[0]
recall_micro_MLP_opt  = micro_MLP_opt[1]
f1_micro_MLP4_opt  = micro_MLP_opt[2]
print(micro_MLP_opt)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_MLP_opt = list(precision_recall_fscore_support(y_test, MLP_predict, average='macro'))
macro_MLP_opt.pop(3) # tou support
precision_macro_MLP_opt  = macro_MLP_opt[0]
recall_macro_MLP_opt  = macro_MLP_opt [1]
f1_macro_MLP4_opt  = macro_MLP_opt [2]
print(macro_MLP_opt)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  MLP_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'],output_dict=True)
print(pd.DataFrame(class_report).T)

print("MLP classifier f1_micro score = {}".format(f1_micro_MLP4_opt))
print("MLP classifier f1_macro score = {}".format(f1_macro_MLP4_opt))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""Ως προς τους Dummy και Gaussian μπορούμε να θεωρήσουμε ως μοναδική υπερπαράμετρο το PCA, κάνοντας  5-fold cross validation με grid search. Μετατοπίζοντας το πεδίο ορισμού για το PCA προς τα κάτω, όπως υπέδειξε το πρώτο grid search που κάναμε, βλέπουμε ότι και οι δύο ταξιμονητές προτιμούν λίγες διαστάσεις.
Επίσης παρατηρούμε ότι η μέθοδος αυτή βελτίωσε κατά πολύ την απόδοση του Dummy Stratified, ενώ μείωσε ελάχιστα την απόδοση του Gaussian.

###Dummy stratified
"""

# Commented out IPython magic to ensure Python compatibility.
n_components = [10,20,30,40,50]
clf = DummyClassifier(strategy="stratified")

pipe = Pipeline(steps=[('pca', pca), ('Dummy', clf)], memory = 'tmp')
estimator1 = GridSearchCV(pipe, dict(pca__n_components=n_components),refit = True, cv=5, scoring='f1_micro', verbose = 10, n_jobs=-1)
# %time estimator1.fit(X_train, y_train)
print("Best parameters regarding f1_micro is = " +str(estimator1.best_params_))
print()

pipe = Pipeline(steps=[('pca', pca), ('Dummy', clf)], memory = 'tmp')
estimator2 = GridSearchCV(pipe, dict(pca__n_components=n_components),refit = True, cv=5, scoring='f1_macro', verbose = 10, n_jobs=-1)
# %time estimator2.fit(X_train, y_train)
print("Best parameters regarding f1_macro is = " +str(estimator2.best_params_))

# Commented out IPython magic to ensure Python compatibility.
# %time dc_stratified_predict = estimator.predict(X_test)
print("Predictions for KNN optimal classifier:"+"\n"+" {}".format(dc_stratified_predict))

cm = confusion_matrix(y_test, dc_stratified_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['Not Epileptic seizure', 'Epileptic seizure'],title='Confusion matrix of Dummy stratified Classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, dc_stratified_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_dc_stratified_opt = list(precision_recall_fscore_support(y_test, dc_stratified_predict, average='micro'))
micro_dc_stratified_opt.pop(3) # tou support
precision_micro_dc_stratified_opt = micro_dc_stratified_opt[0]
recall_micro_dc_stratified_opt = micro_dc_stratified_opt[1]
f1_micro_dc_stratified_opt = micro_dc_stratified_opt[2]
print(micro_dc_stratified_opt)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_dc_stratified_opt = list(precision_recall_fscore_support(y_test, dc_stratified_predict, average='macro'))
macro_dc_stratified_opt.pop(3) # tou support
precision_macro_dc_stratified_opt = macro_dc_stratified_opt[0]
recall_macro_dc_stratified_opt = macro_dc_stratified_opt[1]
f1_macro_dc_stratified_opt = macro_dc_stratified_opt[2]
print(macro_dc_stratified_opt)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  dc_stratified_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'] ,output_dict=True)
print(pd.DataFrame(class_report).T)

print("Dummy stratified Classifier f1_micro score = {}".format(f1_micro_dc_stratified_opt))
print("Dummy stratified Classifier f1_macro score = {}".format(f1_macro_dc_stratified_opt))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""###Gaussian NB"""

# Commented out IPython magic to ensure Python compatibility.
n_components = [10,20,30,40,50]
clf = GaussianNB()

pipe = Pipeline(steps=[('pca', pca), ('GNB', clf)], memory = 'tmp')
estimator1 = GridSearchCV(pipe, dict(pca__n_components=n_components),refit = True, cv=5, scoring='f1_micro', verbose = 10, n_jobs=-1)
# %time estimator1.fit(X_train, y_train)
print("Best parameters regarding f1_micro is = " +str(estimator1.best_params_))
print()

pipe = Pipeline(steps=[('pca', pca), ('GNB', clf)], memory = 'tmp')
estimator2 = GridSearchCV(pipe, dict(pca__n_components=n_components),refit = True, cv=5, scoring='f1_macro', verbose = 10, n_jobs=-1)
# %time estimator2.fit(X_train, y_train)
print("Best parameters regarding f1_macro is = " +str(estimator2.best_params_))

# Commented out IPython magic to ensure Python compatibility.
# %time GaussianNB_predict = estimator.predict(X_test)
print("Predictions for KNN optimal classifier:"+"\n"+" {}".format(GaussianNB_predict))

cm = confusion_matrix(y_test,GaussianNB_predict)
tn, fp, fn, tp = cm.ravel()
print(tn, fp, fn, tp)
plot_confusion_matrix(cm, ['Not Epileptic seizure', 'Epileptic seizure'] ,title='Confusion matrix of Gaussian Naive Bayes classifier')

# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση
# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση bad, το δεύτερο η good
print(precision_recall_fscore_support(y_test, GaussianNB_predict, average=None), "\n")

# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).
micro_GNB_opt = list(precision_recall_fscore_support(y_test, GaussianNB_predict, average='micro'))
micro_GNB_opt.pop(3) # tou support
precision_micro_GNB_opt = micro_GNB_opt[0]
recall_micro_GNB_opt = micro_GNB_opt[1]
f1_micro_GNB_opt = micro_GNB_opt[2]
print(micro_GNB_opt)

# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)
macro_GNB_opt = list(precision_recall_fscore_support(y_test, GaussianNB_predict, average='macro'))
macro_GNB_opt.pop(3) # tou support
precision_macro_GNB_opt = macro_GNB_opt[0]
recall_macro_GNB_opt = macro_GNB_opt[1]
f1_macro_GNB_opt = macro_GNB_opt[2]
print(macro_GNB_opt)
print()
print("All the details shown here:")
print()
class_report = classification_report(y_test,  GaussianNB_predict, target_names=['Not Epileptic seizure', 'Epileptic seizure'] ,output_dict=True)
print(pd.DataFrame(class_report).T)

print("Gaussian Naive Bayes classifier f1_micro score = {}".format(f1_micro_GNB_opt))
print("Gaussian Naive Bayes classifier f1_macro score = {}".format(f1_macro_GNB_opt))

# .iloc[:-1, :] to exclude support
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True);
#sns.heatmap(pd.DataFrame(class_report).T, annot=True);

"""###Σύγκριση Ταξινομητών

Παρακάτω βλέπουμε τις γραφικές απεικονίσεις της απόδοσης των 5 ταξινομητών με grid search και 5-fold cross validation και για τις δύο μετρικές. Λεπτομέρειες σύγκρισης ταξινομητών, σχόλια για χρόνους και παρατηρήσεις, με και χωρίς grid search, αναφέραμε αναλυτικά και παραπάνω, ενδιάμεσα του κώδικα αλλά αναφέρουμε ως συγκεντρωτικό συμπέρασμα ότι πάλι ο SVM έχει την καλύτερη απόδοση και ο Gaussian και kNN διατηρούν τις αποδόσεις τους με ελάχιστες αποκλίσες.
Σημαντική αύξηση παρουσιάζει ο Dummy Stratified, ενώ αρκετά χειρότερη απόδοση σε σχέση με την default αρχικοποίηση έχει ο MLP (που είναι και ο χειρότερος από όλους σε αυτό το βήμα), το οποίο όπως αναλύσαμε και παραπάνω οφείλεται στο γεγονός ότι ζητείται να χρησιμοποιήσουμε μόνο ένα hidden layer στη βελτιστοποίηση, γεγονός που δυσκολεύει την ταξινόμηση non-linear δεδομένων.

Κλείνοντας, ως προς τους χρόνους, όπως παρατηρήσαμε και παραπάνω, εξαιρώντας τον Gaussian και τον Stratified που ουσιαστικά δεν έχουν υπερπαραμέτρους (πέραν του PCA)οπότε είναι πολύ γρήγοροι, τον λιγότερο χρόνο έκανε ο kNN, έπειτα ο SVM και τέλος ο MLP. Αυτό, προφανώς εξαρτάται από το πλήθος των υπερπαραμέτρων, το μέγεθος του πεδίου ορισμού τους και της πολυπλοκότητας του κάθε αλγορίθμου.
Το πλήθος των folds προφανώς επηρεάζει το χρόνο σε όλες τις περιπτώσεις.
"""

fig = plt.figure()
x = ['Stratified Optimal','Gaussian NB Optimal','KNN Optimal','SVM Optimal','MLP Optimal']
y=[100*f1_micro_dc_stratified_opt,100*f1_micro_GNB_opt,100*f1_micro_KNN_opt,100*f1_micro_SVC3_opt,100*f1_micro_MLP1_opt]
plt.bar(x,y, align='center', width=0.5)
plt.xticks(x,x)
plt.title('F1 micro')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()

fig = plt.figure()
x = ['Stratified Optimal','Gaussian NB Optimal','KNN Optimal','SVM Optimal','MLP Optimal']
y=[100*f1_macro_dc_stratified_opt,100*f1_macro_GNB_opt,100*f1_macro_KNN_opt,100*f1_macro_SVC4_opt,100*f1_macro_MLP2_opt]
plt.bar(x,y, align='center', width=0.5)
plt.xticks(x,x)
plt.title('F1 macro')
plt.xlabel('Classifier')
plt.ylabel('Score')
plt.show()